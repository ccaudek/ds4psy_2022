# Il teorema di Bayes {#chapter-teo-bayes}

```{r c017, include = FALSE}
source("_common.R")
```

Il teorema di Bayes deriva da due teoremi fondamentali delle probabilità: il _teorema della probabilità composta_, ovvero $\ {\mbox{P}}(A\cap B)={\mbox{P}}(B){\mbox{P}}(A \mid B)={\mbox{P}}(A){\mbox{P}}(B \mid A)$, e il _teorema della probabilità assoluta_ (detto anche _teorema delle partizioni_), ovvero ${\mbox{P}}(B)=\sum _{{i=1}}^{n}{\mbox{P}}(A_{i}\cap B)=\sum _{{i=1}}^{n}{\mbox{P}}(A_{i}){\mbox{P}}(B \mid A_{i})$. Iniziamo con il teorema della probabilità assoluta.

## Il teorema della probabilità assoluta 

Il teorema della probabilità assoluta verrà qui presentato considerando una partizione dello spazio campionario in tre sottoinsiemi, ma è facile estendere tale discussione al caso di una partizione in un qualunque numero di sottoinsiemi.

::: {.theorem}
Sia $\{F_1, F_2, F_3\}$ una partizione dello spazio campionario $\Omega$. Se $E$ è un qualunque altro evento, $E \subset \Omega$, con $p(E) > 0$, allora:

\begin{equation}
{\mbox{P}}(E) = {\mbox{P}}(E \cap F_1) + {\mbox{P}}(E \cap F_2) + {\mbox{P}}(E \cap F_3) \notag
(\#eq:prob-total-1a)
\end{equation}

ovvero

\begin{equation}
{\mbox{P}}(E) = {\mbox{P}}(E \mid F_1) {\mbox{P}}(F_1) + {\mbox{P}}(E \mid F_2) {\mbox{P}}(F_2) + {\mbox{P}}(E \mid F_3) {\mbox{P}}(F_3).
(\#eq:prob-total-1b)
\end{equation}
:::

Il teorema della probabilità assoluta afferma che, se l'evento $E$ è
costituito da tutti gli eventi elementari in $E \cap F_1$, $E \cap F_2$
e $E \cap F_3$, allora la probabilità ${\mbox{P}}(E)$ è data dalla somma delle
probabilità di questi tre eventi (figura \@ref(fig:tikz-prob-tot)). La \@ref(eq:prob-total-1b) costituisce il denominatore del teorema di Bayes.

```{tikz, echo=FALSE, tikz-prob-tot, fig.cap = "Partizione dello spazio campionario $\\Omega$.", fig.ext = 'png', cache=TRUE}
\usetikzlibrary{
  matrix, patterns, calc, fit, shapes, chains, snakes,
  arrows.meta, arrows, backgrounds, trees, positioning,
  lindenmayersystems
}
\begin{tikzpicture}
\tikzset{
    myrectangle/.style={
        draw=black,
        minimum width=4cm,
        minimum height=2cm,
    },
    A/.style={
        draw=gray,
    },
    E/.style={
        draw=gray,
    },
    >=stealth,
    node distance=1cm and 1cm,
}

    \node[myrectangle] (left) {};
    \node[myrectangle] (right) [right=of left] {};
    \path (left.south east) -- coordinate (tmp) (right.south west);
    \node[myrectangle] (bottom) [below=of tmp] {};

    % "contents" of left node
    \path (left.west) -- node[pos=.25] {$F_1$} (left.east);
    \path (left.west) -- node[pos=.5] {$F_2$} (left.east);
    \path (left.west) -- node[pos=.75] {$F_3$} (left.east);

    \draw[A] ($(left.north west) ! .4 ! (left.north east)$) -- ($(left.south west) ! .35 ! (left.south east)$);
    \draw[A] ($(left.north west) ! .6 ! (left.north east)$) -- ($(left.south west) ! .66 ! (left.south east)$);

    % "contents" of right node
    \draw[E] (right.center) ellipse [x radius=1.5cm, y radius=0.75cm] node {$E$};

    % "contents" of bottom node
    \path (bottom.west) -- node[pos=.25] {\scriptsize $E\cap F_1$} (bottom.east);
    \path (bottom.west) -- node[pos=.5] {\scriptsize $E \cap F_2$} (bottom.east);
    \path (bottom.west) -- node[pos=.75] {\scriptsize $E \cap F_3$} (bottom.east);

    \draw[A] ($(bottom.north west) ! .4 ! (bottom.north east)$) -- ($(bottom.south west) ! .35 ! (bottom.south east)$);
    \draw[A] ($(bottom.north west) ! .6 ! (bottom.north east)$) -- ($(bottom.south west) ! .66 ! (bottom.south east)$);

    \draw[E] (bottom.center) ellipse [x radius=1.5cm, y radius=0.75cm];

    % arrows
    \begin{scope}[
        shorten >=.2cm,
        shorten <=.2cm,
    ]
        \draw[->, black] (left) -- (bottom);
        \draw[->, black] (right) -- (bottom);
    \end{scope}

    % labels on top
    \node at (left.north east) [anchor=south east] {$\Omega$};
    \node at (right.north east) [anchor=south east] {$\Omega$};
    \node at (bottom.north east) [anchor=south east] {$\Omega$};
\end{tikzpicture}
```

::: {.exercise}
Si considerino tre urne, ciascuna delle quali contiene 100 palline:

-   Urna 1: 75 palline rosse e 25 palline blu,
-   Urna 2: 60 palline rosse e 40 palline blu,
-   Urna 3: 45 palline rosse e 55 palline blu.

\noindent
Una pallina viene estratta a caso da un'urna anch'essa scelta a caso.
Qual è la probabilità che la pallina estratta sia di colore rosso?

Sia $R$ l'evento "la pallina estratta è rossa" e sia $U_i$ l'evento che
corrisponde alla scelta dell'$i$-esima urna. Sappiamo che

$$
{\mbox{P}}(R \mid U_1) = 0.75, \quad {\mbox{P}}(R \mid U_2) = 0.60, \quad {\mbox{P}}(R \mid U_3) = 0.45.
$$

Gli eventi $U_1$, $U_2$ e $U_3$ costituiscono una partizione dello
spazio campionario in quanto $U_1$, $U_2$ e $U_3$ sono eventi
mutualmente esclusivi ed esaustivi, ${\mbox{P}}(U_1 \cup U_2 \cup U_3) = 1.0$. In
base al teorema della probabilità assoluta, la probabilità di estrarre una
pallina rossa è dunque

$$
\begin{split}
{\mbox{P}}(R) &= {\mbox{P}}(R \mid U_1){\mbox{P}}(U_1)+{\mbox{P}}(R \mid U_2){\mbox{P}}(U_2)+{\mbox{P}}(R \mid U_3){\mbox{P}}(U_3) \\
&= 0.75 \cdot \frac{1}{3}+0.60 \cdot \frac{1}{3}+0.45 \cdot \frac{1}{3} \\
&=0.60.
\end{split}
$$
:::


<!-- ::: {.exercise} -->
<!-- Consideriamo un'urna che contiene 5 palline rosse e 2 palline verdi. Due -->
<!-- palline vengono estratte, una dopo l'altra. Vogliamo sapere la -->
<!-- probabilità dell'evento "la seconda pallina estratta è rossa". -->

<!-- Lo spazio campionario è $\Omega = \{RR, RV, VR, VV\}$. Chiamiamo $R_1$ -->
<!-- l'evento "la prima pallina estratta è rossa", $V_1$ l'evento "la prima -->
<!-- pallina estratta è verde", $R_2$ l'evento "la seconda pallina estratta è -->
<!-- rossa" e $V_2$ l'evento "la seconda pallina estratta è verde". Dobbiamo -->
<!-- trovare $P(R_2)$ e possiamo risolvere il problema usando il teorema -->
<!-- della probabilità -->
<!-- totale \@ref(eq:prob-total-1b): -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- P(R_2) &= P(R_2 \mid R_1) P(R_1) + P(R_2 \mid V_1)P(V_1)\\ -->
<!-- &= \frac{4}{6} \cdot \frac{5}{7} + \frac{5}{6} \cdot \frac{2}{7} \\ -->
<!-- &= \frac{30}{42} = \frac{5}{7}. -->
<!-- \end{split} -->
<!-- $$ -->

<!-- Se la prima estrazione è quella di una pallina rossa, nell'urna restano -->
<!-- 4 palline rosse e due verdi, dunque, la probabilità che la seconda -->
<!-- estrazione produca una pallina rossa è uguale a 4/6. La probabilità di -->
<!-- una pallina rossa nella prima estrazione è 5/7. Se la prima estrazione è -->
<!-- quella di una pallina verde, nell'urna restano 5 palline rosse e una -->
<!-- pallina verde, dunque, la probabilità che la seconda estrazione produca -->
<!-- una pallina rossa è uguale a 5/6. La probabilità di una pallina verde -->
<!-- nella prima estrazione è 2/7. -->
<!-- ::: -->

## Il teorema di Bayes

Introduciamo ora il teorema di Bayes considerando un caso specifico per poi esaminarlo nella sua forma più generale. Sia $\{F_1, F_2\}$ una partizione dello spazio campionario $\Omega$. Consideriamo un terzo evento $E \subset \Omega$ con probabilità non nulla di cui si conoscono le probabilità condizionate rispetto ad $F_1$ e a $F_2$, ovvero ${\mbox{P}}(E \mid F_1)$ e $P(E \mid F_2)$. È chiaro per le ipotesi fatte che se si verifica $E$ deve anche essersi verificato almeno uno degli eventi $F_1$ e $F_2$. Supponendo che si sia verificato l'evento $E$, ci chiediamo: qual è la probabilità che si sia verificato $F_1$ piuttosto che $F_2$?

```{tikz echo=FALSE, fig.cap="", fig.ext='png', fig.width = 2, cache=TRUE, out.width="45%"}
\usetikzlibrary{
  matrix, patterns, calc, fit, shapes, chains, snakes,
  arrows.meta, arrows, backgrounds, trees, positioning,
  lindenmayersystems
}
\begin{tikzpicture}[scale=.8]
  % \draw[thick] (0,0) -- (0,5) -- (8,5) -- (8,0) -- (0,0);
  \draw[thick] (0,0) rectangle (8,5);
  \draw[thick, color=gray!15, fill] (4,2.5) ellipse (2.7cm and 1.7cm);
  \draw[thick] (3,0) .. controls (6,2) and (2,4) .. (4,5);
  \node (n1) at (6,4) {\textcolor{gray}{$E$}};
  \node (n2) at (0.7,2) {$F_1$};
  \node (n2) at (3,2.5) {$E\cap F_1$};
  \node (n2) at (5,2.5) {$E\cap F_2$};
  \node (n3) at (7.5,2) {$F_2$};
\end{tikzpicture}
```

Per rispondere alla domanda precedente scriviamo:

$$
\begin{split}
{\mbox{P}}(F_1 \mid E) &= \frac{{\mbox{P}}(E \cap F_1)}{{\mbox{P}}(E)}\notag\\
              &= \frac{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1)}{{\mbox{P}}(E)}.
\end{split}
$$

Sapendo che $E = (E \cap F_1) \cup (E \cap F_2)$ e che $F_1$ e $F_2$ sono eventi disgiunti, ovvero $F_1 \cap F_2 = \emptyset$, ne segue che possiamo calcolare ${\mbox{P}}(E)$ utilizzando il teorema della probabilità assoluta:

$$
\begin{split}
{\mbox{P}}(E) &= {\mbox{P}}(E \cap F_1) + {\mbox{P}}(E \cap F_2)\notag\\
     &= {\mbox{P}}(E \mid F_1)P(F_1) + {\mbox{P}}(E \mid F_2){\mbox{P}}(F_2).
\end{split}
$$

\noindent
Sostituendo il risultato precedente nella formula della probabilità condizionata $P(F_1 \mid E)$ otteniamo:

\begin{equation}
{\mbox{P}}(F_1 \mid E) = \frac{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1)}{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1) + {\mbox{P}}(E \mid F_2)P(F_2)}.
(\#eq:bayes1)
\end{equation}

\noindent
La \@ref(eq:bayes1) si generalizza facilmente al caso di più di due eventi disgiunti, come indicato di seguito.

::: {.theorem}
Sia $E$ un evento contenuto in $F_1 \cup \dots \cup F_k$, dove gli eventi $F_j, j=1, \dots, k$ sono a due a due incompatibili e necessari. Allora per ognuno dei suddetti eventi $F_j$ vale la seguente formula:

\begin{equation}
{\mbox{P}}(F_j \mid E) = \frac{{\mbox{P}}(E \mid F_j){\mbox{P}}(F_j)}{\sum_{j=1}^{k}{\mbox{P}}(F_j)P(E \mid F_j)}.
(\#eq:bayes2)
\end{equation}
:::

\noindent
La \@ref(eq:bayes2) prende il nome di *teorema di Bayes* e mostra che la conoscenza del verificarsi dell'evento $E$ modifica la probabilità che avevamo attribuito all'evento $F_j$. Nella \@ref(eq:bayes2) la probabilità condizionata ${\mbox{P}}(F_j \mid E)$ prende il nome di probabilità _a posteriori_ dell'evento $F_j$: il termine "a posteriori" sta a significare "dopo che è noto che si è verificato l'evento $E$".

Nel capitolo \@ref(chapter-intro-bayes-inference) estenderemo questa discussione mostrando come la \@ref(eq:bayes2) possa essere formulata nel caso delle variabili casuali continue. 

::: {.exercise}
Un lettore attento si sarà reso conto che, in precedenza, abbiamo già applicato il teorema di Bayes, quando abbiamo risolto l'esercizio riportato nella Sezione \@ref(sec:bayes-cancer). Svolgiamo nuovamente lo stesso esercizio applicando ora la \@ref(eq:bayes2) che, per convenienza, riscriviamo come:

\begin{align}
{\mbox{P}}(M \mid +) &= \frac{{\mbox{P}}(+ \mid M) {\mbox{P}}(M)}{{\mbox{P}}(+ \mid M) {\mbox{P}}(M) + {\mbox{P}}(+ \mid M^\complement) {\mbox{P}}(M^\complement)}\notag\\ 
&= \frac{0.9 \cdot 10/1000}{0.9 \cdot 10/1000 + 99 / 990 \cdot 990 / 1000} \notag\\
&= \frac{9}{108}.\notag
\end{align}
:::

## Commenti e considerazioni finali {-}

Il teorema di Bayes rende esplicito il motivo per cui la probabilità non possa essere pensata come uno stato oggettivo, quanto piuttosto come un'inferenza soggettiva e condizionata. Il denominatore del membro di destra della \@ref(eq:bayes2) è un semplice fattore di normalizzazione. Nel numeratore compaiono invece due quantità: ${\mbox{P}}(F_j$) e ${\mbox{P}}(E \mid F_j)$. La probabilità ${\mbox{P}}(F_j$) è la probabilità _probabilità a priori_ (_prior_) dell'evento $F_j$ e rappresenta l'informazione che l'agente bayesiano possiede a proposito dell'evento $F_j$. Diremo che ${\mbox{P}}(F_j)$ codifica il grado di fiducia che l'agente ripone in $F_j$ prima di avere osservato i dati. Nell'interpretazione bayesiana, ${\mbox{P}}(F_j)$ rappresenta un giudizio personale dell'agente e non esistono criteri esterni che possano determinare se tale giudizio sia coretto o meno. La probabilità condizionata ${\mbox{P}}(E \mid F_j)$ rappresenta invece la verosimiglianza di $F_j$ e ci dice quant'è plausibile che si verifichi l'evento $E$ condizionatemente al fatto che si sia verificato $F_j$. Il teorema di Bayes descrive la regola che l'agente deve seguire per aggiornare il suo grado di fiducia in $F_j$ alla luce di un ulteriore evento $E$. La ${\mbox{P}}(F_j \mid E)$ è chiamata probabilità a posteriori dato che rappresenta la nuova probabilità che l'agente assegna ad $F_j$ affinché rimanga consistente con le nuove informazioni fornitegli da $E$.

La probabilità a posteriori dipende sia da $E$, sia dalla conoscenza a priori dell'agente ${\mbox{P}}(F_j)$. In questo senso è chiaro come non abbia senso parlare di una probabilità oggettiva: per il teorema di Bayes la probabilità è definita condizionatamente alla probabilità a priori, la quale a sua volta, per definizione, è un'assegnazione soggettiva. Ne segue pertanto che ogni probabilità debba essere considerata come una rappresentazione del grado di fiducia soggettiva dell'agente. Dato che ogni assegnazione probabilistica rappresenta uno stato di conoscenza e che ciascun particolare stato di conoscenza è arbitrario, un accordo tra agenti diversi  non è richiesto. Tuttavia, la teoria delle probabilità ci fornisce uno strumento che, alla luce di nuove informazioni, consente l'aggiornamento dello stato di conoscenza in un modo razionale.

Il teorema di Bayes consente di modificare una credenza a priori in maniera dinamica, via via che nuove evidenze vengono raccolte, in modo tale da formulare una credenza a posteriori la quale non è mai definitiva, ma possa sempre essere aggiornata in base alle nuove evidenze disponibili. Questo processo si chiama *aggiornamento bayesiano*. 
<!-- Esercizi uleriori sono proposti nelle Appendici \@ref(appendix:bayes-updating) e \@ref(appendix:exrc-abs-prob). -->

::: {.remark}
Qual è la pronuncia di "Bayesian"? Per saperlo possiamo seguire [questo link](https://bayes-rules.github.io/posts/fun/).
:::

<!-- Il teorema di Bayes costituisce il fondamento dell'approccio più moderno -->
<!-- della statistica, quello appunto detto bayesiano. Chi usa il teorema di -->
<!-- Bayes non è, solo per questo motivo, "bayesiano": ci vuole ben altro. Ci -->
<!-- vuole un modo diverso per intendere il significato della probabilità e -->
<!-- un modo diverso per intendere gli obiettivi dell'inferenza statistica. In anni recenti, una gran parte della comunità scientifica ha riconosciuto all'approccio bayesiano il merito di consentire lo sviluppo di modelli anche molto complessi senza richiedere, d'altra parte, conoscenze matematiche troppo avanzate all'utente. Per questa ragione l'approccio bayesiano sta prendendo sempre più piede, anche in psicologia. -->



