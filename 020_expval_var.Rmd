# Valore atteso e varianza {#exp-val-and-variance-rv}

```{r c020, include = FALSE}
source("_common.R")
```

Spesso risulta utile fornire una rappresentazione sintetica della distribuzione di una variabile casuale attraverso degli indicatori caratteristici piuttosto che fare riferimento ad una sua rappresentazione completa mediante la funzione di ripartizione, o la funzione di massa o di densità di probabilità. Una descrizione più sintetica di una variabile casuale, tramite pochi valori, ci consente di cogliere le caratteristiche essenziali della distribuzione, quali: la posizione, cioè il baricentro della distribuzione di probabilità; la variabilità, cioè la dispersione della distribuzione di probabilità attorno ad un centro; la forma della distribuzione di probabilità, considerando la simmetria e la curtosi (pesantezza delle code). In questo Capitolo introdurremo quegli indici sintetici che descrivono il centro di una distribuzione di probabilità e la sua variabilità.

## Valore atteso

Quando vogliamo conoscere il comportamento tipico di una variabile casuale spesso vogliamo sapere qual è il suo "valore tipico". La nozione di "valore tipico", tuttavia, è ambigua. Infatti, essa può essere definita in almeno tre modi diversi:

- la _media_ (somma dei valori divisa per il numero dei valori),
- la _mediana_ (il valore centrale della distribuzione, quando la variabile è ordinata in senso crescente o decrescente),
- la _moda_ (il valore che ricorre più spesso).

Per esempio, la media di $\{3, 1, 4, 1, 5\}$ è $\frac{3+1+4+1+5}{5} = 2.8$, la mediana è $3$ e la moda è $1$. Tuttavia, la teoria delle probabilità si occupa di variabili casuali piuttosto che di sequenze di numeri. Diventa dunque necessario precisare che cosa intendiamo per "valore tipico" quando facciamo riferimento alle variabili casuali. Giungiamo così alla seguente definizione.
<!-- Supponiamo di ripetere un esperimento casuale molte volte in modo tale che ciascun valore $Y$ si presenti con una frequenza approssimativamente uguale alla sua probabilità. Per esempio, possiamo lanciare una coppia di dadi e osservare i valori di $S$ = "somma dei punti" e di $P$ = "prodotto dei punti". Ci poniamo il problema di definire il "risultato tipico" di questo esperimento in modo tale che esso corrisponda al valore medio dei valori della variabile casuale, quando l'esperimento casuale viene ripetuto tante volte.  -->

::: {.definition}
Sia $Y$ è una variabile casuale discreta che assume i valori $y_1, \dots, y_n$ con distribuzione $p(y)$,
ossia

$$
P(Y = y_i) = p(y_i),
$$

per definizione il *valore atteso* di $Y$, $\E(Y)$, è  

\begin{equation}
\E(Y) = \sum_{i=1}^n y_i \cdot p(y_i).
(\#eq:expval-discr)
\end{equation}
:::

A parole: il valore atteso (o speranza matematica, o aspettazione, o valor medio) di una variabile casuale è definito come la somma di tutti i valori che la variabile casuale può prendere, ciascuno pesato dalla probabilità con cui il valore è preso.

::: {.exercise}
Calcoliamo il valore atteso della variabile casuale $Y$ corrispondente al lancio di una moneta equilibrata (testa: _Y_ = 1; croce: _Y_ = 0). 

$$
\E(Y) = \sum_{i=1}^{2} y_i \cdot P(y_i) = 0 \cdot \frac{1}{5} + 1 \cdot \frac{1}{5} = 0.5.
$$
::: 

::: {.exercise}
Supponiamo ora che _Y_ sia il risultato del lancio di un dado equilibrato. Il valore atteso di _Y_ diventa:

$$
\E(Y) = \sum_{i=1}^{6} y_i \cdot P(y_i) = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \dots + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5.
$$
::: 

### Interpretazione

Che interpretazione può essere assegnata alla nozione di valore atteso? Bruno de Finetti adottò lo stesso termine di _previsione_ (e lo stesso simbolo) tanto per la probabilità che per la speranza matematica. Si può pertanto dire che, dal punto di vista bayesiano, la speranza matematica è l'estensione naturale della nozione di probabilità soggettiva.

### Proprietà del valore atteso

La proprietà più importante del valore atteso è la linearità: il valore atteso di una somma di variabili casuali è uguale alla somma dei lori rispettivi valori attesi:

\begin{equation}
\E(X + Y) = \E(X) + \E(Y).
(\#eq:prop-expval-linearity)
\end{equation}

La \@ref(eq:prop-expval-linearity) sembra ragionevole quando $X$ e $Y$ sono indipendenti, ma è anche vera quando $X$ e $Y$ sono associati. Abbiamo anche che

\begin{equation}
\E(cY) = c \E(Y).
(\#eq:prop-expval-const)
\end{equation}

La \@ref(eq:prop-expval-const) ci dice che possiamo estrarre una costante dall'operatore di valore atteso. Tale proprietà si estende a qualunque numero di variabili casuali. Infine, se due variabili casuali $X$ e $Y$ sono indipendenti, abbiamo che 

\begin{equation}
\E(X Y) = \E(X) \E(Y). 
(\#eq:expval-prod-ind-rv)
\end{equation}

::: {.exercise}
Si considerino le seguenti variabili casuali: $Y$, ovvero il numero che si ottiene dal lancio di un dado equilibrato, e $Y$, il numero di teste prodotto dal lancio di una moneta equilibrata. 
Poniamoci il problema di trovare il valore atteso di $X+Y$.

Per risolvere il problema iniziamo a costruire lo spazio campionario dell'esperimento casuale consistente nel lancio di un dado e di una moneta.

   $x/              y$     1        2        3        4        5        6
  --------------------- -------- -------- -------- -------- -------- --------
              0         (0, 1)   (0, 2)   (0, 3)   (0, 4)   (0, 5)   (0, 6)
              1         (1, 1)   (1, 2)   (1, 3)   (1, 4)   (1, 5)   (1, 6)

\noindent
ovvero

   $x/              y$     1        2        3        4        5        6
  --------------------- -------- -------- -------- -------- -------- --------
              0             1       2        3        4        5        6
              1             2       3        4        5        6        7

\noindent
Il risultato del lancio del dado è indipendente dal risultato del lancio della moneta. Pertanto, ciascun evento elementare dello spazio campionario avrà la stessa probabilità di verificarsi, ovvero $Pr(\omega) = \frac{1}{12}$. Il valore atteso di $X+Y$ è dunque uguale a:

$$
\E(X+Y) = 1 \cdot \frac{1}{12} + 2 \cdot \frac{1}{12} + \dots + 7 \cdot \frac{1}{12} = 4.0.
$$

Lo stesso risultato si ottiene nel modo seguente:

$$
\E(X+Y) = \E(X) + E(Y) = 3.5 + 0.5 = 4.0.
$$
:::

::: {.exercise}
Si considerino le variabili casuali $X$ e $Y$ definite nel caso del lancio di tre monete equilibrate, dove $X$ conta il numero delle teste nei tre lanci e $Y$ conta il numero delle teste al primo lancio. Si calcoli il valore atteso del prodotto delle variabili casuali $X$ e $Y$. 

La distribuzione di probabilità congiunta $P(X, Y)$ è fornita nella tabella seguente. 

   $x/              y$    0     1    $p(Y)$
  --------------------- ----- ----- --------
            0            1/8    0     1/8
            1            2/8   1/8    3/8
            2            1/8   2/8    3/8
            3             0    1/8    1/8
         $p(y)$          4/8   4/8    1.0

\noindent
Il calcolo del valore atteso di $XY$ si riduce a

$$
\E(XY) = 1 \cdot \frac{1}{8} + 2 \cdot \frac{2}{8} + 3 \cdot \frac{1}{8} = 1.0.
$$

Si noti che le variabili casuali $Y$ e $Y$ non sono indipendenti. Dunque non possiamo usare la proprietà \@ref(thm:prodindrv). Infatti, il valore atteso di $X$ è

$$
\E(X) = 1 \cdot \frac{3}{8} + 2 \cdot \frac{3}{8} + 3 \cdot \frac{1}{8} = 1.5
$$

e il valore atteso di $Y$ è

$$
\E(Y) = 0 \cdot \frac{4}{8} + 1 \cdot \frac{4}{8} = 0.5.
$$
Dunque

$$
1.5 \cdot 0.5 \neq 1.0.
$$
:::

### Variabili casuali continue

Nel caso di una variabile casuale continua $Y$ il valore atteso diventa:

\begin{equation}
\E(Y) = \int_{-\infty}^{+\infty} y p(y) \,\operatorname {d}\!y
(\#eq:def-ev-rv-cont)
\end{equation}

Anche in questo caso il valore atteso è una media ponderata della $y$, nella quale ciascun possibile valore $y$ è ponderato per il corrispondente valore della densità $p(y)$. Possiamo leggere l'integrale pensando che $y$ rappresenti l'ampiezza delle barre infinitamente strette di un istogramma, con la densità $p(y)$ che corrisponde all'altezza di tali barre e la notazione $\int_{-\infty}^{+\infty}$ che corrisponde ad una somma.

Un'altra misura di tendenza centrale delle variabili casuali continue è la moda. La moda della $Y$ individua il valore $y$ più plausibile, ovvero il valore $y$ che massimizza la funzione di densità $p(y)$:

\begin{equation}
\Mo(Y) = \argmax_y p(y).
(\#eq:def-mode)
\end{equation}

## Varianza 

La seconda più importante proprietà di una variabile casuale, dopo che conosciamo il suo valore atteso, è la _varianza_.

::: {.definition}
Se $Y$ è una variabile casuale discreta con distribuzione $p(y)$, per definizione la varianza di $Y$, $\mathbb{V}(Y)$, è  

\begin{equation}
\mathbb{V}(Y) = \E\Big[\big(Y - \E(Y)\big)^2\Big].
(\#eq:def-var-rv)
\end{equation}
:::

A parole: la varianza è la deviazione media quadratica della variabile dalla sua media.^[Data una variabile casuale $Y$ con valore atteso $\E(Y)$, le "distanze" tra i valori di $Y$ e il valore atteso $\E(Y)$ definiscono la variabile casuale $Y - \E(Y)$ chiamata _scarto_, oppure _deviazione_ oppure _variabile casuale centrata_. La variabile $Y - \E(Y)$ equivale ad una traslazione di sistema di riferimento che porta il valore atteso nell'origine degli assi. Si può dimostrare facilmente che il valore atteso della variabile scarto $Y - \E(Y)$ vale zero, dunque la media di tale variabile non può essere usata per quantificare la "dispersione" dei valori di $Y$ relativamente al suo valore medio. Occorre rendere sempre positivi i valori di $Y - \E(Y)$ e tale risultato viene ottenuto considerando la variabile casuale $\left(Y - \E(Y)\right)^2$.] Se denotiamo $\E(Y) = \mu$, la varianza $\mathbb{V}(Y)$ diventa il valore atteso di $(Y - \mu)^2$.  

::: {.exercise}
Posta $S$ uguale alla somma dei punti ottenuti nel lancio di due dadi equilibrati, poniamoci il problema di calcolare la varianza di $S$.

La variabile casuale $S$ ha la seguente distribuzione di probabilità:

   $s$            2                   3             4               5                   6               7                8                9                10             11              12 
-------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ---------------- ----------------
 $P(S = s)$     $\frac{1}{36}$   $\frac{2}{36}$   $\frac{3}{36}$   $\frac{4}{36}$   $\frac{5}{36}$   $\frac{6}{36}$   $\frac{5}{36}$   $\frac{4}{36}$   $\frac{3}{36}$   $\frac{2}{36}$   $\frac{1}{36}$
 
\noindent
Essendo $\E(S) = 7$, la varianza diventa

\begin{align}
\mathbb{V}(S) &= \sum \left(S- \mathbb{E}(S)\right)^2 \cdot P(S) \notag\\
&= (2 - 7)^2 \cdot 0.0278 + (3-7)^2 \cdot 0.0556 + \dots + (12 - 7)^2 \cdot 0.0278 \notag\\
&= 5.8333.\notag
\end{align}
:::

### Formula alternativa per la varianza

C'è un modo più semplice per calcolare la varianza:

\begin{align}
\E\Big[\big(X - \E(Y)\big)^2\Big] &= \E\big(X^2 - 2X\E(Y) + \E(Y)^2\big)\notag\\
&= \E(Y^2) - 2\E(Y)\E(Y) + \E(Y)^2,\notag
\end{align}

dato che $\E(Y)$ è una costante; pertanto

\begin{equation}
\mathbb{V}(Y) = \E(Y^2) - \big(\E(Y) \big)^2.
(\#eq:def-alt-var-rv)
\end{equation}

A parole: la varianza è la media dei quadrati meno il quadrato della media.

::: {.exercise}
Consideriamo la variabile casuale $Y$ che corrisponde al numero di teste che si osservano nel lancio di una moneta truccata con probabilità di testa uguale a 0.8.
Il valore atteso di $Y$ è

$$
\E(Y) = 0 \cdot 0.2 + 1 \cdot 0.8 = 0.8.
$$
Usando la formula tradizionale della varianza otteniamo:

$$
\mathbb{V}(Y) = (0 - 0.8)^2 \cdot 0.2 + (1 - 0.8)^2 \cdot 0.8 = 0.16.
$$
Lo stesso risultato si trova con la formula alternativa della varianza. Il valore atteso di $Y^2$ è

$$
\E(Y^2) = 0^2 \cdot 0.2 + 1^2 * 0.8 = 0.8.
$$
e la varianza diventa

$$
\mathbb{V}(Y) = \E(Y^2) - \big(\E(Y) \big)^2 = 0.8 - 0.8^2 = 0.16.
$$
::: 

### Variabili casuali continue

Nel caso di una variabile casuale continua $Y$, la varianza diventa:

\begin{equation}
\mathbb{V}(Y) = \int_{-\infty}^{+\infty} \large[y - \E(Y)\large]^2 p(y) \,\operatorname {d}\!y
(\#eq:def-var-rv-cont)
\end{equation}

Come nel caso discreto, la varianza di una v.c. continua $y$ misura approssimativamente la distanza al quadrato tipica o prevista dei possibili valori $y$ dalla loro media.

## Deviazione standard

Quando lavoriamo con le varianze, i termini sono innalzati al quadrato e quindi i numeri possono diventare molto grandi (o molto piccoli). Per trasformare nuovamente i valori nell'unità di misura della scala originaria si prende la radice quadrata.  Il valore risultante viene chiamato _deviazione standard_ e solitamente è denotato dalla lettera greca $\sigma$.

::: {.definition}
Si definisce scarto quadratico medio (o deviazione standard o scarto tipo) la radice quadrata della varianza: 

\begin{equation}
\sigma_Y = \sqrt{\mathbb{V}(Y)}.
(\#eq:def-sd)
\end{equation}
::: 

Interpretiamo la deviazione standard di una variabile casuale come nella statistica descrittiva: misura approssimativamente la distanza tipica o prevista dei possibili valori $y$ dalla loro media.

::: {.exercise}
Per i dadi equilibrati dell'esempio precedente, la deviazione standard della variabile casuale $S$ è uguale a $\sqrt{5.833} = 2.415$. 
:::

## Standardizzazione

:::{.definition}
Data una variabile casuale $Y$, si dice variabile standardizzata di $Y$ l'espressione

\begin{equation}
Z = \frac{Y - \E(Y)}{\sigma_Y}.
(\#eq:standardization)
\end{equation}
:::

Solitamente, una variabile standardizzata viene denotata con la lettera $Z$.

## Momenti di variabili casuali

:::{.definition}
Si chiama *momento* di ordine $q$ di una v.c. $X$, dotata di densità $p(x)$, la
quantità

\begin{equation}
\E(X^q) = \int_{-\infty}^{+\infty} x^q p(x) \; dx.
\end{equation}

Se $X$ è una v.c. discreta, i suoi momenti valgono:

\begin{equation}
\E(X^q) = \sum_i x_i^q p(x_i).
\end{equation}
:::

I momenti sono importanti parametri indicatori di certe proprietà di $X$. I più
noti sono senza dubbio quelli per $q = 1$ e $q = 2$. Il momento del primo ordine corrisponde al valore atteso di $X$. Spesso i momenti di ordine superiore al primo vengono calcolati rispetto al valor medio di $X$, operando una traslazione $x_0 = x − \E(X)$ che individua lo scarto dalla media. Ne deriva che il momento centrale di ordine 2 corrisponde alla varianza.

## Covarianza

La covarianza quantifica la tendenza delle variabili aleatorie $X$ e $Y$ a ``variare assieme''. Per esempio, l'altezza e il peso delle giraffe producono una covarianza positiva perché all'aumentare di una di queste due quantità tende ad aumentare anche l'altra. La covarianza misura la forza e la direzione del legame lineare tra due variabili aleatorie $X$ ed $Y$. Si utilizza la notazione $\mbox{Cov}(X,Y)=\sigma_{xy}$.

::: {.definition}
Date due variabili aleatorie $X$, $Y$, chiamiamo covarianza tra $X$ ed $Y$ il numero

\begin{equation}
\mbox{Cov}(X,Y) = \mathbb{E}\Bigl(\bigl(X - \mathbb{E}(X)\bigr) \bigl(Y - \mathbb{E}(Y)\bigr)\Bigr),
\end{equation}

dove $\mathbb{E}(X)$ e $\mathbb{E}(Y)$ sono i valori attesi di $X$ ed $Y$.
:::

In maniera esplicita,

\begin{equation}
\mbox{Cov}(X,Y) = \sum_{(x,y) \in \Omega} (x - \mu_X) (y - \mu_Y) f(x, y).
\label{eq:cov_def}
\end{equation}

La definizione è  analoga, algebricamente, a quella di varianza e risulta infatti

\begin{equation}
\mathbb{V}(x) = cov(X, X)
\end{equation}

e

\begin{equation}
\mbox{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X).
\label{eq:cov_vc_alt}
\end{equation}

:::{.proof}
La proprietà precedente si dimostra nel modo seguente:

\begin{align}
\mbox{Cov}(X,Y) &= \mathbb{E}\Bigl(\bigl(X-\mathbb{E}(X)\bigr) \bigl(Y-\mathbb{E}(Y)\bigr)\Bigr)\notag\\
          %&= \mathbb{E}(XY) - \mathbb{E}(Y)X -\mathbb{E}(X)Y + \mathbb{E}(X)\mathbb{E}(Y) )\notag\\
          &= \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X) - \mathbb{E}(X)\mathbb{E}(Y) + \mathbb{E}(X)\mathbb{E}(Y)\notag\\
          &= \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X)\notag.
\end{align}
:::

:::{.exercise}
Consideriamo le variabili casuali definite nell'Esercizio 2.4. Si calcoli la covarianza di $X$ e $Y$.

Abbiamo che $\mu_X = 1.5$ e $\mu_Y = 0.5$. Ne segue che la covarianza di $X$ e $Y$ è:

\begin{equation}
\begin{split}
\mbox{Cov}(X,Y) &= \sum_{(x,y) \in\ \Omega} (x - \mu_X) (y - \mu_Y) f(x, y)\\
&= (0-1.5)(0-0.5)\cdot \frac{1}{8} + (0-1.5)(1-0.5) \cdot 0 \\
   &\hskip0.05\textwidth\relax + (1-1.5)(0-0.5)\cdot \frac{2}{8} + (1-1.5)(1-0.5) \cdot \frac{1}{8} \\
    &\hskip0.05\textwidth\relax + (2-1.5)(0-0.5) \cdot \frac{1}{8} + (2-1.5)(1-0.5) \cdot \frac{2}{8} \\
   &\hskip0.05\textwidth\relax + (3-1.5)(0-0.5) \cdot 0 +  (3-1.5)(1-0.5)\cdot\frac{1}{8} \\
   &= \frac{1}{4}. \notag
 \end{split}
\end{equation}

Lo stesso risultato può essere trovato nel modo seguente. Iniziamo a calcolare il valore atteso del prodotto $XY$:

$$
\mathbb{E}(XY) = 0 \cdot\frac{4}{8} + 1 \cdot\frac{1}{8} + 2 \cdot\frac{2}{8} + 3 \cdot\frac{1}{8} = 1.0.
$$

Dunque, la covarianza tra $X$ e $Y$ diventa

\begin{align}
\mbox{Cov}(X,Y) &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)\notag\\
 &= 1 -  1.5\cdot 0.5 \notag\\
 &= 0.25.\notag
\end{align}
:::

## Correlazione

La covarianza dipende dall'unità di misura delle due variabili e quindi non consente di stabilire l'intensità della relazione.
Una misura standardizzata della relazione che intercorre fra due variabili è invece rappresentata  dalla correlazione. La correlazione si ottiene dividendo la covarianza per le deviazioni standard delle due variabili aleatorie.

:::{.defn}
Il coefficiente di correlazione tra $X$ ed $Y$ è il numero definito da

\begin{equation}
\rho(X,Y) =\frac{\mbox{Cov}(X,Y)}{\sqrt{\mathcal{V}(X)\mathcal{V}(Y)}}.
\end{equation}
:::

Si può anche scrivere $\rho_{X,Y}$ al posto di $\rho(X,Y)$.

Il coefficiente di correlazione $\rho_{xy}$ è un numero puro, cioè non
dipende dall'unità di misura delle variabili, e assume valori compresi tra -1 e +1.

## Proprietà

- La covarianza tra una variabile aleatoria $X$ e una costante $c$ è nulla:

\begin{equation}
\mbox{Cov}(c,X) = 0,
\end{equation}

- la covarianza è simmetrica:

\begin{equation}
\mbox{Cov}(X,Y) = \mbox{Cov}(Y,X),
\end{equation}

- vale

\begin{equation}
 -1 \leq \rho(X,Y) \leq 1,
\end{equation}

- la correlazione non dipende dall'unità di misura:

\begin{equation}
 \rho(aX, bY) = \rho(X,Y), \qquad \forall a, b > 0,
\end{equation}

- se $Y = a + bX$ è una funzione lineare di $X$ con costanti $a$ e $b$, allora $\rho(X,Y) = \pm 1$, a seconda del segno di $b$,
- la covarianza tra $X$ e $Y$, ciascuna moltiplicata per una costante, è uguale al prodotto delle costanti per la covarianza tra $X$ e $Y$:

\begin{equation}
\mbox{Cov}(aX,bY) = ab \;\mbox{Cov}(X,Y), \qquad \forall a,b \in \Real,
\end{equation}

- vale

\begin{equation}
\mathbb{V}(X \pm Y) = \mathbb{V}(X) + \mathbb{V}(Y) \pm 2 \cdot \mbox{Cov}(X,Y),
\end{equation}

- vale

\begin{equation}
\mbox{Cov}(X + Y, Z) = \mbox{Cov}(X,Z) + \mbox{Cov}(Y,Z),
\end{equation}

- per una sequenza di variabili aleatorie $X_1, \dots, X_n$, si ha

 \begin{equation}
 \mathbb{V}\left( \sum_{i=1}^n X_i\right) = \sum_{i=1}^n
 \mathbb{V}(X_i) + 2\sum_{i,j: i<j}cov(X_i, X_j),
\end{equation}

- vale

\begin{equation}
\mbox{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_jY_j\right) = \sum_{i=1}^n \sum_{j=1}^m a_j b_j\mbox{Cov}(X_j, Y_j),
\end{equation}

- se $X_1, X_2, \dots, X_n$ sono indipendenti, allora

\begin{equation}
\mbox{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^n b_jX_j\right) = \sum_{i=1}^n a_i b_i \mathbb{V}(X_i).
\end{equation}


### Incorrelazione

Si dice che $X$ ed $Y$ sono incorrelate, o linermente indipendenti, se la loro covarianza è nulla,

\begin{equation}
\sigma_{XY} = \mathbb{E} \big[(X - \mu_X) (y-\mu_u) \big] = 0,
\end{equation}

che si può anche scrivere come

\begin{equation}
\rho_{XY} = 0, \quad \mathbb{E}(XY) = \mathbb{E}(X) \mathbb{E}(Y).
\end{equation}

Si introduce così un secondo tipo di indipendenza, più debole, dopo quello di indipendenza stocastica. Viceversa, però, se $\mbox{Cov}(X, Y) = 0$, non è detto che $X$ ed $Y$ siano indipendenti.


:::{.exercise}
Siano $X$ e $Y$ due variabili aleatorie discrete avente una distribuzione di massa di probabilità congiunta pari a

$$
f_{XY}(x,y) = \frac{1}{4} \quad (x,y) \in \{(0,0), (1,1), (1, -1), (2,0) \}
$$

e zero altrimenti. Si calcoli la covarianza $\rho_{XY}$. Le due variabili aleatorie $X$ e $Y$ sono mutuamente indipendenti?

La distribuzione marginale della $X$ è

$$
\begin{cases}
X = 0, \quad  P_X = 1/4, \\
X = 1, \quad P_X = 2/4, \\
X = 2, \quad P_X = 1/4.
\end{cases}
$$

$$
\mathbb{E}(X) = 0 \frac{1}{4} + 1 \frac{2}{4} + 2 \frac{1}{4} = 1.
$$

$$
\mathbb{E}(X^2) = 0^2 \frac{1}{4} + 1^2 \frac{2}{4} + 2^2 \frac{1}{4} = \frac{3}{2}.
$$

$$
\mathbb{V}(X) = \frac{3}{2} - 1^2 = \frac{1}{2}.
$$

La distribuzione marginale della $Y$ è

$$
\begin{cases}
Y = -1, \quad  P_Y = 1/4, \\
Y = 0, \quad P_Y = 2/4, \\
Y = 1, \quad P_Y = 1/4.
\end{cases}
$$

$$
\mathbb{E}(Y) = 0 \frac{2}{4} + 1 \frac{1}{4} + (-1) \frac{1}{4} = 0.
$$

$$
\mathbb{E}(Y^2) = 0^2 \frac{2}{4} + 1^2 \frac{1}{4} + (-1)^2 \frac{1}{4} = \frac{1}{2}.
$$

$$
\mathbb{V}(X) = \frac{1}{2} - 0^2 = \frac{1}{2}.
$$

Calcoliamo ora la covarianza tra $X$ e $Y$:

$$
\mathbb{E}(XY) = \sum_x\sum_y xy f_{XY} (x,y) =
(0\cdot 0)\frac{1}{4} +
(1\cdot 1)\frac{1}{4} +
(1\cdot -1)\frac{1}{4} +
(2\cdot 0)\frac{1}{4} = 0.
$$

$$
\mbox{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = 0 - 1\cdot0 = 0.
$$

Quindi le due variabili aleatorie hanno covarianza pari a zero. Tuttavia, esse non sono indipendenti, in quanto non è vero che

$$
f_{XY} (x,y) = f_X(x) f_Y(y)
$$

per tutti gli $x$ e $y$. In conclusione, anche se condizione di indipendenza implica una covarianza nulla, questo esempio mostra come l'inverso non sia necessariamente vero. La covarianza può essere zero anche quando le due variabili aleatorie non sono indipendenti.
:::

## Conclusioni {-}

La densità di probabilità congiunta bivariata tiene simultaneamente conto del comportamento di due variabili aleatorie $X$ e $Y$ e di come esse si influenzino. Se $X$ e $Y$ sono legate linearmente, allora il coefficiente di correlazione

\begin{equation}
\rho = \frac{\mbox{Cov}(X, Y)}{\sigma_X \sigma_Y}\notag
\end{equation}

fornisce l'indice maggiormente utilizzato per descrivere l'intensità e il segno dell'associazione lineare. Nel caso di un'associazione lineare perfetta, $Y = a + bX$, avremo $\rho = 1$ con $b$ positivo ed $\rho = -1$ con $b$ negativo. Se il coefficiente di correlazione è pari a 0 le variabili si dicono incorrelate. Condizione sufficiente (ma non necessaria) affinché $\rho = 0$ è che le due variabili siano tra loro indipendenti.







