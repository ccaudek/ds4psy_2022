<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Data Science per psicologi</title>
  <meta name="description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Data Science per psicologi" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="github-repo" content="ccaudek/ds4psy" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science per psicologi" />
  
  <meta name="twitter:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  

<meta name="author" content="Corrado Caudek" />


<meta name="date" content="2022-03-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Data Science per psicologi</h1>
<p class="author"><em>Corrado Caudek</em></p>
<p class="date"><em>2022-03-14</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#prefazione">Prefazione<span></span></a>
<ul>
<li><a href="#la-psicologia-e-la-data-science">La psicologia e la Data science<span></span></a></li>
<li><a href="#come-studiare">Come studiare<span></span></a></li>
<li><a href="#sviluppare-un-metodo-di-studio-efficace">Sviluppare un metodo di studio efficace<span></span></a></li>
</ul></li>
<li><a href="#part-nozioni-preliminari">(PART) Nozioni preliminari<span></span></a></li>
<li><a href="#concetti-chiave"><span class="toc-section-number">1</span> Concetti chiave<span></span></a>
<ul>
<li><a href="#popolazioni-e-campioni"><span class="toc-section-number">1.1</span> Popolazioni e campioni<span></span></a></li>
<li><a href="#variabili-e-costanti"><span class="toc-section-number">1.2</span> Variabili e costanti<span></span></a>
<ul>
<li><a href="#variabili-casuali"><span class="toc-section-number">1.2.1</span> Variabili casuali<span></span></a></li>
<li><a href="#variabili-indipendenti-e-variabili-dipendenti"><span class="toc-section-number">1.2.2</span> Variabili indipendenti e variabili dipendenti<span></span></a></li>
<li><a href="#la-matrice-dei-dati"><span class="toc-section-number">1.2.3</span> La matrice dei dati<span></span></a></li>
</ul></li>
<li><a href="#parametri-e-modelli"><span class="toc-section-number">1.3</span> Parametri e modelli<span></span></a></li>
<li><a href="#effetto"><span class="toc-section-number">1.4</span> Effetto<span></span></a></li>
<li><a href="#stima-e-inferenza"><span class="toc-section-number">1.5</span> Stima e inferenza<span></span></a></li>
<li><a href="#metodi-e-procedure-della-psicologia"><span class="toc-section-number">1.6</span> Metodi e procedure della psicologia<span></span></a></li>
</ul></li>
<li><a href="#chapter-misurazione"><span class="toc-section-number">2</span> La misurazione in psicologia<span></span></a>
<ul>
<li><a href="#le-scale-di-misura"><span class="toc-section-number">2.1</span> Le scale di misura<span></span></a>
<ul>
<li><a href="#scala-nominale"><span class="toc-section-number">2.1.1</span> Scala nominale<span></span></a></li>
<li><a href="#scala-ordinale"><span class="toc-section-number">2.1.2</span> Scala ordinale<span></span></a></li>
<li><a href="#scala-ad-intervalli"><span class="toc-section-number">2.1.3</span> Scala ad intervalli<span></span></a></li>
<li><a href="#scala-di-rapporti"><span class="toc-section-number">2.1.4</span> Scala di rapporti<span></span></a></li>
</ul></li>
<li><a href="#gerarchia-dei-livelli-di-scala-di-misura"><span class="toc-section-number">2.2</span> Gerarchia dei livelli di scala di misura<span></span></a></li>
<li><a href="#variabili-discrete-o-continue"><span class="toc-section-number">2.3</span> Variabili discrete o continue<span></span></a></li>
<li><a href="#alcune-misure-sono-migliori-di-altre"><span class="toc-section-number">2.4</span> Alcune misure sono migliori di altre<span></span></a>
<ul>
<li><a href="#tipologie-di-errori"><span class="toc-section-number">2.4.1</span> Tipologie di errori<span></span></a></li>
</ul></li>
<li><a href="#commenti-e-considerazioni-finali">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#part-analisi-esplorativa-dei-dati">(PART) Analisi esplorativa dei dati<span></span></a></li>
<li><a href="#ch:freq-distr"><span class="toc-section-number">3</span> Variabili e distribuzioni di frequenza<span></span></a>
<ul>
<li><a href="#chapter-descript"><span class="toc-section-number">3.1</span> Introduzione all’esplorazione dei dati<span></span></a></li>
<li><a href="#un-excursus-storico"><span class="toc-section-number">3.2</span> Un excursus storico<span></span></a></li>
<li><a href="#riassumere-i-dati"><span class="toc-section-number">3.3</span> Riassumere i dati<span></span></a></li>
<li><a href="#i-dati-grezzi"><span class="toc-section-number">3.4</span> I dati grezzi<span></span></a></li>
<li><a href="#distribuzioni-di-frequenze"><span class="toc-section-number">3.5</span> Distribuzioni di frequenze<span></span></a></li>
<li><a href="#istogramma"><span class="toc-section-number">3.6</span> Istogramma<span></span></a></li>
<li><a href="#kernel-density-plot"><span class="toc-section-number">3.7</span> Kernel density plot<span></span></a></li>
<li><a href="#forma-di-una-distribuzione"><span class="toc-section-number">3.8</span> Forma di una distribuzione<span></span></a></li>
<li><a href="#indici-di-posizione"><span class="toc-section-number">3.9</span> Indici di posizione<span></span></a>
<ul>
<li><a href="#quantili"><span class="toc-section-number">3.9.1</span> Quantili<span></span></a></li>
<li><a href="#diagramma-a-scatola"><span class="toc-section-number">3.9.2</span> Diagramma a scatola<span></span></a></li>
<li><a href="#sina-plot"><span class="toc-section-number">3.9.3</span> Sina plot<span></span></a></li>
<li><a href="#leccellenza-grafica"><span class="toc-section-number">3.9.4</span> L’eccellenza grafica<span></span></a></li>
</ul></li>
<li><a href="#commenti-e-considerazioni-finali-1">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#ch:loc-scale"><span class="toc-section-number">4</span> Indici di posizione e di scala<span></span></a>
<ul>
<li><a href="#indici-di-tendenza-centrale"><span class="toc-section-number">4.1</span> Indici di tendenza centrale<span></span></a>
<ul>
<li><a href="#media"><span class="toc-section-number">4.1.1</span> Media<span></span></a></li>
<li><a href="#media-spuntata"><span class="toc-section-number">4.1.2</span> Media spuntata<span></span></a></li>
<li><a href="#moda-e-mediana"><span class="toc-section-number">4.1.3</span> Moda e mediana<span></span></a></li>
</ul></li>
<li><a href="#indici-di-dispersione"><span class="toc-section-number">4.2</span> Indici di dispersione<span></span></a>
<ul>
<li><a href="#indici-basati-sullordinamento-dei-dati"><span class="toc-section-number">4.2.1</span> Indici basati sull’ordinamento dei dati<span></span></a></li>
<li><a href="#varianza"><span class="toc-section-number">4.2.2</span> Varianza<span></span></a></li>
<li><a href="#precisione"><span class="toc-section-number">4.2.3</span> Precisione<span></span></a></li>
<li><a href="#scarto-tipo"><span class="toc-section-number">4.2.4</span> Scarto tipo<span></span></a></li>
<li><a href="#deviazione-mediana-assoluta"><span class="toc-section-number">4.2.5</span> Deviazione mediana assoluta<span></span></a></li>
<li><a href="#indici-di-variabilità-relativi"><span class="toc-section-number">4.2.6</span> Indici di variabilità relativi<span></span></a></li>
</ul></li>
<li><a href="#commenti-e-considerazioni-finali-2">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#ch:correlation"><span class="toc-section-number">5</span> Le relazioni tra variabili<span></span></a>
<ul>
<li><a href="#diagramma-a-dispersione"><span class="toc-section-number">5.0.1</span> Diagramma a dispersione<span></span></a></li>
<li><a href="#covarianza"><span class="toc-section-number">5.0.2</span> Covarianza<span></span></a></li>
<li><a href="#correlazione"><span class="toc-section-number">5.0.3</span> Correlazione<span></span></a></li>
<li><a href="#correlazione-e-causazione"><span class="toc-section-number">5.1</span> Correlazione e causazione<span></span></a>
<ul>
<li><a href="#usi-della-correlazione"><span class="toc-section-number">5.1.1</span> Usi della correlazione<span></span></a></li>
<li><a href="#correlazione-di-spearman"><span class="toc-section-number">5.1.2</span> Correlazione di Spearman<span></span></a></li>
<li><a href="#correlazione-nulla"><span class="toc-section-number">5.1.3</span> Correlazione nulla<span></span></a></li>
</ul></li>
<li><a href="#commenti-e-considerazioni-finali-3">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#part-il-calcolo-delle-probabilità">(PART) Il calcolo delle probabilità<span></span></a></li>
<li><a href="#intro-prob-1"><span class="toc-section-number">6</span> La logica dell’incerto<span></span></a>
<ul>
<li><a href="#che-cosè-la-probabilità"><span class="toc-section-number">6.1</span> Che cos’è la probabilità?<span></span></a></li>
<li><a href="#variabili-casuali-e-probabilità-di-un-evento"><span class="toc-section-number">6.2</span> Variabili casuali e probabilità di un evento<span></span></a>
<ul>
<li><a href="#eventi-e-probabilità"><span class="toc-section-number">6.2.1</span> Eventi e probabilità<span></span></a></li>
<li><a href="#spazio-campione-e-risultati-possibili"><span class="toc-section-number">6.2.2</span> Spazio campione e risultati possibili<span></span></a></li>
</ul></li>
<li><a href="#variabili-casuali-1"><span class="toc-section-number">6.3</span> Variabili casuali<span></span></a></li>
<li><a href="#usare-la-simulazione-per-stimare-le-probabilità"><span class="toc-section-number">6.4</span> Usare la simulazione per stimare le probabilità<span></span></a></li>
<li><a href="#la-legge-dei-grandi-numeri"><span class="toc-section-number">6.5</span> La legge dei grandi numeri<span></span></a></li>
<li><a href="#variabili-casuali-multiple"><span class="toc-section-number">6.6</span> Variabili casuali multiple<span></span></a></li>
<li><a href="#sec:fun-mass-prob"><span class="toc-section-number">6.7</span> Funzione di massa di probabilità<span></span></a>
<ul>
<li><a href="#funzione-di-ripartizione"><span class="toc-section-number">6.7.1</span> Funzione di ripartizione<span></span></a></li>
</ul></li>
<li><a href="#commenti-e-considerazioni-finali-4">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#chapter-prob-cond"><span class="toc-section-number">7</span> Probabilità condizionata<span></span></a>
<ul>
<li><a href="#sec:bayes-cancer"><span class="toc-section-number">7.1</span> Probabilità condizionata su altri eventi<span></span></a></li>
<li><a href="#la-regola-moltiplicativa"><span class="toc-section-number">7.2</span> La regola moltiplicativa<span></span></a></li>
<li><a href="#lindipendendenza-stocastica"><span class="toc-section-number">7.3</span> L’indipendendenza stocastica<span></span></a></li>
<li><a href="#il-teorema-della-probabilità-totale"><span class="toc-section-number">7.4</span> Il teorema della probabilità totale<span></span></a></li>
<li><a href="#il-teorema-della-probabilità-assoluta"><span class="toc-section-number">7.5</span> Il teorema della probabilità assoluta<span></span></a></li>
<li><a href="#indipendenza-condizionale"><span class="toc-section-number">7.6</span> Indipendenza condizionale<span></span></a></li>
<li><a href="#commenti-e-considerazioni-finali-5">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#chapter-teo-bayes"><span class="toc-section-number">8</span> Il teorema di Bayes<span></span></a>
<ul>
<li><a href="#il-teorema-di-bayes"><span class="toc-section-number">8.1</span> Il teorema di Bayes<span></span></a></li>
<li><a href="#commenti-e-considerazioni-finali-6">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#chapter-prob-congiunta"><span class="toc-section-number">9</span> Probabilità congiunta<span></span></a>
<ul>
<li><a href="#funzione-di-probabilità-congiunta"><span class="toc-section-number">9.1</span> Funzione di probabilità congiunta<span></span></a>
<ul>
<li><a href="#proprietà"><span class="toc-section-number">9.1.1</span> Proprietà<span></span></a></li>
<li><a href="#eventi"><span class="toc-section-number">9.1.2</span> Eventi<span></span></a></li>
<li><a href="#sec:marg-distr-discr"><span class="toc-section-number">9.1.3</span> Funzioni di probabilità marginali<span></span></a></li>
</ul></li>
<li><a href="#sec:margin-vc-cont"><span class="toc-section-number">9.2</span> Marginalizzazione di variabili casuali continue<span></span></a></li>
<li><a href="#commenti-e-considerazioni-finali-7">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#chapter-intro-density-function"><span class="toc-section-number">10</span> La densità di probabilità<span></span></a>
<ul>
<li><a href="#spinner-e-variabili-casuali-continue-uniformi"><span class="toc-section-number">10.1</span> Spinner e variabili casuali continue uniformi<span></span></a>
<ul>
<li><a href="#il-paradosso-delle-variabili-casuali-continue"><span class="toc-section-number">10.1.1</span> Il paradosso delle variabili casuali continue<span></span></a></li>
</ul></li>
<li><a href="#la-funzione-di-ripartizione-per-una-variabile-casuale-continua"><span class="toc-section-number">10.2</span> La funzione di ripartizione per una variabile casuale continua<span></span></a></li>
<li><a href="#la-distribuzione-uniforme"><span class="toc-section-number">10.3</span> La distribuzione uniforme<span></span></a></li>
<li><a href="#dagli-istogrammi-alle-densità"><span class="toc-section-number">10.4</span> Dagli istogrammi alle densità<span></span></a></li>
<li><a href="#funzione-di-densità-di-probabilità"><span class="toc-section-number">10.5</span> Funzione di densità di probabilità<span></span></a></li>
<li><a href="#la-funzione-di-ripartizione"><span class="toc-section-number">10.6</span> La funzione di ripartizione<span></span></a></li>
</ul></li>
<li><a href="#exp-val-and-variance-rv"><span class="toc-section-number">11</span> Valore atteso e varianza<span></span></a>
<ul>
<li><a href="#valore-atteso"><span class="toc-section-number">11.1</span> Valore atteso<span></span></a>
<ul>
<li><a href="#interpretazione"><span class="toc-section-number">11.1.1</span> Interpretazione<span></span></a></li>
<li><a href="#proprietà-del-valore-atteso"><span class="toc-section-number">11.1.2</span> Proprietà del valore atteso<span></span></a></li>
<li><a href="#variabili-casuali-continue"><span class="toc-section-number">11.1.3</span> Variabili casuali continue<span></span></a></li>
</ul></li>
<li><a href="#varianza-1"><span class="toc-section-number">11.2</span> Varianza<span></span></a>
<ul>
<li><a href="#formula-alternativa-per-la-varianza"><span class="toc-section-number">11.2.1</span> Formula alternativa per la varianza<span></span></a></li>
<li><a href="#variabili-casuali-continue-1"><span class="toc-section-number">11.2.2</span> Variabili casuali continue<span></span></a></li>
</ul></li>
<li><a href="#deviazione-standard"><span class="toc-section-number">11.3</span> Deviazione standard<span></span></a></li>
<li><a href="#standardizzazione"><span class="toc-section-number">11.4</span> Standardizzazione<span></span></a></li>
<li><a href="#momenti-di-variabili-casuali"><span class="toc-section-number">11.5</span> Momenti di variabili casuali<span></span></a></li>
<li><a href="#covarianza-1"><span class="toc-section-number">11.6</span> Covarianza<span></span></a></li>
<li><a href="#correlazione-1"><span class="toc-section-number">11.7</span> Correlazione<span></span></a></li>
<li><a href="#proprietà-1"><span class="toc-section-number">11.8</span> Proprietà<span></span></a>
<ul>
<li><a href="#incorrelazione"><span class="toc-section-number">11.8.1</span> Incorrelazione<span></span></a></li>
</ul></li>
<li><a href="#conclusioni">Conclusioni<span></span></a></li>
</ul></li>
<li><a href="#part-distribuzioni-teoriche-di-probabilità">(PART) Distribuzioni teoriche di probabilità<span></span></a></li>
<li><a href="#distr-rv-discr"><span class="toc-section-number">12</span> Distribuzioni di v.c. discrete<span></span></a>
<ul>
<li><a href="#una-prova-bernoulliana"><span class="toc-section-number">12.1</span> Una prova Bernoulliana<span></span></a></li>
<li><a href="#una-sequenza-di-prove-bernoulliane"><span class="toc-section-number">12.2</span> Una sequenza di prove Bernoulliane<span></span></a>
<ul>
<li><a href="#valore-atteso-e-deviazione-standard"><span class="toc-section-number">12.2.1</span> Valore atteso e deviazione standard<span></span></a></li>
</ul></li>
<li><a href="#distribuzione-di-poisson"><span class="toc-section-number">12.3</span> Distribuzione di Poisson<span></span></a>
<ul>
<li><a href="#alcune-proprietà-della-variabile-di-poisson"><span class="toc-section-number">12.3.1</span> Alcune proprietà della variabile di Poisson<span></span></a></li>
</ul></li>
<li><a href="#distribuzione-discreta-uniforme"><span class="toc-section-number">12.4</span> Distribuzione discreta uniforme<span></span></a>
<ul>
<li><a href="#usiamo-textsfr"><span class="toc-section-number">12.4.1</span> Usiamo <span class="math inline">\(\textsf{R}\)</span><span></span></a></li>
</ul></li>
<li><a href="#distribuzione-beta-binomiale"><span class="toc-section-number">12.5</span> Distribuzione beta-binomiale<span></span></a></li>
<li><a href="#commenti-e-considerazioni-finali-8">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
<li><a href="#distr-rv-cont"><span class="toc-section-number">13</span> Distribuzioni di v.c. continue<span></span></a>
<ul>
<li><a href="#distribuzione-normale"><span class="toc-section-number">13.1</span> Distribuzione Normale<span></span></a>
<ul>
<li><a href="#limite-delle-distribuzioni-binomiali"><span class="toc-section-number">13.1.1</span> Limite delle distribuzioni binomiali<span></span></a></li>
</ul></li>
<li><a href="#normal-random-walk"><span class="toc-section-number">13.2</span> La Normale prodotta con una simulazione<span></span></a>
<ul>
<li><a href="#concentrazione"><span class="toc-section-number">13.2.1</span> Concentrazione<span></span></a></li>
<li><a href="#funzione-di-ripartizione-1"><span class="toc-section-number">13.2.2</span> Funzione di ripartizione<span></span></a></li>
<li><a href="#distribuzione-normale-standard"><span class="toc-section-number">13.2.3</span> Distribuzione Normale standard<span></span></a></li>
</ul></li>
<li><a href="#teorema-del-limite-centrale"><span class="toc-section-number">13.3</span> Teorema del limite centrale<span></span></a></li>
<li><a href="#distribuzione-chi-quadrato"><span class="toc-section-number">13.4</span> Distribuzione Chi-quadrato<span></span></a>
<ul>
<li><a href="#proprietà-2"><span class="toc-section-number">13.4.1</span> Proprietà<span></span></a></li>
</ul></li>
<li><a href="#distribuzione-t-di-student"><span class="toc-section-number">13.5</span> Distribuzione <span class="math inline">\(t\)</span> di Student<span></span></a>
<ul>
<li><a href="#proprietà-3"><span class="toc-section-number">13.5.1</span> Proprietà<span></span></a></li>
</ul></li>
<li><a href="#funzione-beta"><span class="toc-section-number">13.6</span> Funzione beta<span></span></a></li>
<li><a href="#distribuzione-beta"><span class="toc-section-number">13.7</span> Distribuzione Beta<span></span></a></li>
<li><a href="#distribuzione-di-cauchy"><span class="toc-section-number">13.8</span> Distribuzione di Cauchy<span></span></a></li>
<li><a href="#distribuzione-log-normale"><span class="toc-section-number">13.9</span> Distribuzione log-normale<span></span></a></li>
<li><a href="#distribuzione-di-pareto"><span class="toc-section-number">13.10</span> Distribuzione di Pareto<span></span></a></li>
<li><a href="#commenti-e-considerazioni-finali-9">Commenti e considerazioni finali<span></span></a></li>
</ul></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science per psicologi</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<!-- https://github.com/rstudio/rmarkdown-book -->
<div id="prefazione" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Prefazione<a href="#prefazione" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Data Science per psicologi</em> contiene il materiale delle lezioni dell’insegnamento di <em>Psicometria B000286</em> (A.A. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. <em>Psicometria</em> si propone di fornire agli studenti un’introduzione all’analisi dei dati in psicologia. Le conoscenze/competenze che verranno sviluppate in questo insegnamento sono quelle della Data science, ovvero un insieme di conoscenze/competenze che si pongono all’intersezione tra statistica (ovvero, richiedono la capacità di comprendere teoremi statistici) e informatica (ovvero, richiedono la capacità di sapere utilizzare un software).</p>
<div id="la-psicologia-e-la-data-science" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">La psicologia e la Data science<a href="#la-psicologia-e-la-data-science" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sembra sensato spendere due parole su un tema che è importante per gli studenti: quello indicato dal titolo di questo Capitolo. È ovvio che agli studenti di psicologia la statistica non piace. Se piacesse, forse studierebbero Data science e non psicologia; ma non lo fanno. Di conseguenza, gli studenti di psicologia si chiedono: “perché dobbiamo perdere tanto tempo a studiare queste cose quando in realtà quello che ci interessa è tutt’altro?” Questa è una bella domanda.</p>
<p>C’è una ragione molto semplice che dovrebbe farci capire perché la Data science è così importante per la psicologia. Infatti, a ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia <em>gli individui</em> ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, in certi casi, predire. In questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili a quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che i problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data science in psicologia: perché la Data science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.</p>
<p>Sono sicuro che, leggendo queste righe, a molti studenti sarà venuta in mente la seguente domanda: perché non chiediamo a qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data science? La risposta a questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data science. Le tematiche della Data science non possono essere ignorate né dai ricercatori in psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche i professionisti al di fuori dall’università non possono fare a meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data science! Basta aprire a caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano i risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.</p>
<p>Le considerazioni precedenti cercano di chiarire il seguente punto: la Data science non è qualcosa da studiare a malincuore, in un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data science in tantissimi ambiti della loro attività professionale: in particolare quando costruiscono, somministrano e interpretano i test psicometrici. È dunque chiaro che possedere delle solide basi di Data science è un tassello imprescindibile del bagaglio professionale dello psicologo. In questo insegnamento verrano trattati i temi base della Data science e verrà adottato un punto di vista bayesiano, che corrisponde all’approccio più recente e sempre più diffuso in psicologia.</p>
</div>
<div id="come-studiare" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Come studiare<a href="#come-studiare" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Il giusto metodo di studio per prepararsi all’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare i concetti via via che essi vengono presentati e verificare in autonomia le procedure presentate a lezione. Incoraggio gli studenti a farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti a utilizzare i forum attivi su Moodle e, soprattutto, a svolgere gli esercizi proposti su Moodle. I problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino a quel punto sono sufficienti rispetto alle richieste dell’esame.</p>
<p>La prima fase dello studio, che è sicuramente individuale, è quella in cui è necessario acquisire le conoscenze teoriche relative ai problemi che saranno presentati all’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (<span class="math inline">\(\textsf{R}\)</span>) per applicare i concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso ci aiuta a capire meglio.</p>
</div>
<div id="sviluppare-un-metodo-di-studio-efficace" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Sviluppare un metodo di studio efficace<a href="#sviluppare-un-metodo-di-studio-efficace" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Avendo insegnato molte volte in passato un corso introduttivo di analisi dei dati ho notato nel corso degli anni che gli studenti con l’atteggiamento mentale che descriverò qui sotto generalmente ottengono ottimi risultati. Alcuni studenti sviluppano naturalmente questo approccio allo studio, ma altri hanno bisogno di fare uno sforzo per maturarlo. Fornisco qui sotto una breve descrizione del “metodo di studio” che, nella mia esperienza, è il più efficace per affrontare le richieste di questo insegnamento.</p>
<ul>
<li>Dedicate un tempo sufficiente al materiale di base, apparentemente facile; assicuratevi di averlo capito bene. Cercate le lacune nella vostra comprensione. Leggere presentazioni diverse dello stesso materiale (in libri o articoli diversi) può fornire nuove intuizioni.</li>
<li>Gli errori che facciamo sono i nostri migliori maestri. Istintivamente cerchiamo di dimenticare subito i nostri errori. Ma il miglior modo di imparare è apprendere dagli errori che commettiamo. In questo senso, una soluzione corretta è meno utile di una soluzione sbagliata. Quando commettiamo un errore questo ci fornisce un’informazione importante: ci fa capire qual è il materiale di studio sul quale dobbiamo ritornare e che dobbiamo capire meglio.</li>
<li>C’è ovviamente un aspetto “psicologico” nello studio. Quando un esercizio o problema ci sembra incomprensibile, la cosa migliore da fare è dire: “mi arrendo”, “non ho idea di cosa fare!”. Questo ci rilassa: ci siamo già arresi, quindi non abbiamo niente da perdere, non dobbiamo più preoccuparci. Ma non dobbiamo fermarci qui. Le cose “migliori” che faccio (se ci sono) le faccio quando non ho voglia di lavorare. Alle volte, quando c’è qualcosa che non so fare e non ho idea di come affontare, mi dico: “oggi non ho proprio voglia di fare fatica”, non ho voglia di mettermi nello stato mentale per cui “in 10 minuti devo risolvere il problema perché dopo devo fare altre cose”. Però ho voglia di <em>divertirmi</em> con quel problema e allora mi dedico a qualche aspetto “marginale” del problema, che so come affrontare, oppure considero l’aspetto più difficile del problema, quello che non so come risolvere, ma invece di cercare di risolverlo, guardo come altre persone hanno affrontato problemi simili, opppure lo stesso problema in un altro contesto. Non mi pongo l’obiettivo “risolvi il problema in 10 minuti”, ma invece quello di farmi un’idea “generale” del problema, o quello di capire un caso più specifico e più semplice del problema. Senza nessuna pressione. Infatti, in quel momento ho deciso di non lavorare (ovvero, di non fare fatica). Va benissimo se “parto per la tangente”, ovvero se mi metto a leggere del materiale che sembra avere poco a che fare con il problema centrale (le nostre intuizioni e la nostra curiosità solitamente ci indirizzano sulla strada giusta). Quando faccio così, molto spesso trovo la soluzione del problema che mi ero posto e, paradossalmente, la trovo in un tempo minore di quello che, in precedenza, avevo dedicato a “lavorare” al problema. Allora perché non faccio sempre così? C’è ovviamente l’aspetto dei “10 minuti” che non è sempre facile da dimenticare. Sotto pressione, possiamo solo agire in maniera automatica, ovvero possiamo solo applicare qualcosa che già sappiamo fare. Ma se dobbiamo imparare qualcosa di nuovo, la pressione è un impedimento.</li>
<li>È utile farsi da soli delle domande sugli argomenti trattati, senza limitarsi a cercare di risolvere gli esercizi che vengono assegnati. Quando studio qualcosa mi viene in mente: “se questo è vero, allora deve succedere quest’altra cosa”. Allora verifico se questo è vero, di solito con una simulazione. Se i risultati della simulazione sono quelli che mi aspetto, allora vuol dire che ho capito. Se i risultati sono diversi da quelli che mi aspettavo, allora mi rendo conto di non avere capito e ritorno indietro a studiare con più attenzione la teoria che pensavo di avere capito – e ovviamente mi rendo conto che c’era un aspetto che avevo frainteso. Questo tipo di verifica è qualcosa che dobbiamo fare da soli, in prima persona: nessun altro può fare questo al posto nostro.</li>
<li>Non aspettatevi di capire tutto la prima volta che incontrate un argomento nuovo.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> È utile farsi una nota mentalmente delle lacune nella vostra comprensione e tornare su di esse in seguito per carcare di colmarle. L’atteggiamento naturale, quando non capiamo i dettagli di qualcosa, è quello di pensare: “non importa, ho capito in maniera approssimativa questo punto, non devo preoccuparmi del resto”. Ma in realtà non è vero: se la nostra comprensione è superficiale, quando il problema verrà presentato in una nuova forma, non riusciremo a risolverlo. Per cui i dubbi che ci vengono quando studiamo qualcosa sono il nostro alleato più prezioso: ci dicono esattamente quali sono gli aspetti che dobbiamo approfondire per potere migliorare la nostra preparazione.</li>
<li>È utile sviluppare una visione d’insieme degli argomenti trattati, capire l’obiettivo generale che si vuole raggiungere e avere chiaro il contributo che i vari pezzi di informazione forniscono al raggiungimento di tale obiettivo. Questa organizzazione mentale del materiale di studio facilita la comprensione. È estremamente utile creare degli schemi di ciò che si sta studiando. Non aspettate che sia io a fornirvi un riepilogo di ciò che dovete imparare: sviluppate da soli tali schemi e tali riassunti.</li>
<li>Tutti noi dobbiamo imparare l’arte di trovare le informazioni, non solo nel caso di questo insegnamento. Quando vi trovate di fronte a qualcosa che non capite, o ottenete un oscuro messaggio di errore da un software, ricordatevi: “Google is your friend”!</li>
</ul>

<p class="flushright">
Corrado Caudek<br />
Marzo 2022
</p>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="part-nozioni-preliminari" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Nozioni preliminari<a href="#part-nozioni-preliminari" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="concetti-chiave" class="section level1 hasAnchor" number="1">
<h1 class="hasAnchor"><span class="header-section-number">1</span> Concetti chiave<a href="#concetti-chiave" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>La <em>data science</em> si pone all’intersezione tra statistica e informatica. La statistica è un insieme di metodi ugilizzati per estrarre informazioni dai dati; l’informatica implementa tali procedure in un software. In questo Capitolo vengono introdotti i concetti fondamentali.</p>
<div id="popolazioni-e-campioni" class="section level2 hasAnchor" number="1.1">
<h2 class="hasAnchor"><span class="header-section-number">1.1</span> Popolazioni e campioni<a href="#popolazioni-e-campioni" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Popolazione.</em> L’analisi dei dati inizia con l’individuazione delle unità portatrici di informazioni circa il fenomeno di interesse. Si dice popolazione (o universo) l’insieme <span class="math inline">\(\Omega\)</span> delle entità capaci di fornire informazioni sul fenomeno oggetto dell’indagine statistica. Possiamo scrivere <span class="math inline">\(\Omega = \{\omega_i\}_{i=1, \dots, n}= \{\omega_1, \omega_2, \dots, \omega_n\}\)</span>, oppure <span class="math inline">\(\Omega = \{\omega_1, \omega_2, \dots \}\)</span> nel caso di popolazioni finite o infinite, rispettivamente.</p>
<p>L’obiettivo principale della ricerca psicologica è conoscere gli esiti psicologici e i loro fattori trainanti nella popolazione. Questo è l’obiettivo delle sperimentazioni psicologiche e della maggior parte degli studi osservazionali in psicologia. È quindi necessario essere molto chiari sulla popolazione a cui si applicano i risultati della ricerca. La popolazione può essere ben definita, ad esempio, tutte le persone che si trovavano nella città di Hiroshima al momento dei bombardamenti atomici e sono sopravvissute al primo anno, o può essere ipotetica, ad esempio, tutte le persone depresse che hanno subito o saranno sottoporsi ad un intervento di psicoterapia. Il ricercatore deve sempre essere in grado di determinare se un soggetto appartiene alla popolazione oggetto di interesse.</p>
<p>Una <em>sottopopolazione</em> è una popolazione in sé e per sé che soddisfa proprietà ben definite. Negli esempi precedenti, potremmo essere interessati alla sottopopolazione di uomini di età inferiore ai 20 anni o di pazienti depressi sottoposti ad uno specifico intervento psicologico. Molte questioni scientifiche riguardano le differenze tra sottopopolazioni; ad esempio, confrontando i gruppi con o senza psicoterapia per determinare se il trattamento è vantaggioso. I modelli di regressione, introdotti nel Capitolo @ref(regr-models-intro) riguardano le sottopopolazioni, in quanto stimano il risultato medio per diversi gruppi (sottopopolazioni) definiti dalle covariate.</p>
<p><em>Campione.</em> Gli elementi <span class="math inline">\(\omega_i\)</span> dell’insieme <span class="math inline">\(\Omega\)</span> sono detti <em>unità statistiche</em>. Un sottoinsieme della popolazione, ovvero un insieme di elementi <span class="math inline">\(\omega_i\)</span>, viene chiamato <em>campione</em>. Ciascuna unità statistica <span class="math inline">\(\omega_i\)</span> (abbreviata con u.s.) è portatrice dell’informazione che verrà rilevata mediante un’operazione di misurazione.</p>
<p>Un campione è dunque un sottoinsieme della popolazione utilizzato per conoscere tale popolazione. A differenza di una sottopopolazione definita in base a chiari criteri, un campione viene generalmente selezionato tramite un procedura casuale. Il <em>campionamento casuale</em> consente allo scienziato di trarre conclusioni sulla popolazione e, soprattutto, di quantificare l’incertezza sui risultati. I campioni di un sondaggio sono esempi di campioni casuali, ma molti studi osservazionali non sono campionati casualmente. Possono essere <em>campioni di convenienza</em>, come coorti di studenti in un unico istituto, che consistono di tutti gli studenti sottoposti ad un certo intervento psicologico in quell’istituto. Indipendentemente da come vengono ottenuti i campioni, il loro uso al fine di conoscere una popolazione target significa che i problemi di rappresentatività sono inevitabili e devono essere affrontati.</p>
</div>
<div id="variabili-e-costanti" class="section level2 hasAnchor" number="1.2">
<h2 class="hasAnchor"><span class="header-section-number">1.2</span> Variabili e costanti<a href="#variabili-e-costanti" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una <em>variabile</em> è qualsiasi proprietà o descrittore che può assumere più valori (numerici o categoriali). Una variabile può essere pensata come una domanda a cui il valore è la risposta. Ad esempio, “Quanti anni ha questo partecipante?” “38 anni”. Qui, “età” è la variabile e “38” è il suo valore. La probabilità che la variabile <span class="math inline">\(X\)</span> assuma valore <span class="math inline">\(x\)</span> si scrive <span class="math inline">\(P(X = x)\)</span>. Questo è spesso abbreviato in <span class="math inline">\(P(x)\)</span>. Possiamo anche esaminare la probabilità di più valori contemporaneamente; per esempio, la probabilità che <span class="math inline">\(X = x\)</span> e <span class="math inline">\(Y = y\)</span> è scritta <span class="math inline">\(P(X = x, Y = y)\)</span> o <span class="math inline">\(P(x, y)\)</span>. Si noti che <span class="math inline">\(P(X = 38)\)</span> è interpretato come la probabilità che un individuo selezionato casualmente dalla popolazione abbia 38 anni. Il termine “variabile” si contrappone al termine “costante” che descrive una proprietà invariante di tutte le unità statistiche.</p>
<p>Si dice <em>modalità</em> ciascuna delle varianti con cui una variabile statistica può presentarsi. Definiamo <em>insieme delle modalità</em> di una variabile statistica l’insieme <span class="math inline">\(M\)</span> di tutte le possibili espressioni con cui la variabile può manifestarsi. Le modalità osservate e facenti parte del campione si chiamano <em>dati</em> (si veda la Tabella <a href="#tab:term_st_desc" reference-type="ref" reference="tab:term_st_desc">1.1</a>).</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>(#exm:unlabeled-div-1) </strong></span>Supponiamo che il fenomeno studiato sia l’intelligenza. In uno studio, la popolazione potrebbe corrispondere all’insieme di tutti gli italiani adulti. La variabile considerata potrebbe essere il punteggio del test standardizzato WAIS-IV. Le modalità di tale variabile potrebbero essere <span class="math inline">\(112, 92, 121, \dots\)</span>. Tale variabile è di tipo quantitativo discreto.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>(#exm:unlabeled-div-2) </strong></span>Supponiamo che il fenomeno studiato sia il compito Stroop. La popolazione potrebbe corrispondere all’insieme dei bambini dai 6 agli 8 anni. La variabile considerata potrebbe essere il reciproco dei tempi di reazione in secondi. Le modalità di tale variabile potrebbero essere <span class="math inline">\(1 / 2.35, 1/ 1.49, 1/2.93, \dots\)</span>. La variabile è di tipo quantitativo continuo.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>(#exm:unlabeled-div-3) </strong></span>Supponiamo che il fenomeno studiato sia il disturbo di personalità. La popolazione potrebbe corrispondere all’insieme dei detenuti nelle carceri italiane. La variabile considerata potrebbe essere l’assessment del disturbo di personalità tramite interviste cliniche strutturate. Le modalità di tale variabile potrebbero essere i Cluster A, Cluster B, Cluster C descritti dal DSM-V. Tale variabile è di tipo qualitativo.</p>
</div>
<div id="variabili-casuali" class="section level3 hasAnchor" number="1.2.1">
<h3 class="hasAnchor"><span class="header-section-number">1.2.1</span> Variabili casuali<a href="#variabili-casuali" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il termine <em>variabile</em> usato nella statistica è equivalente al termine <em>variabile casuale</em> usato nella teoria delle probabilità. Lo studio dei risultati degli interventi psicologici è lo studio delle variabili casuali che misurano questi risultati. Una variabile casuale cattura una caratteristica specifica degli individui nella popolazione e i suoi valori variano tipicamente tra gli individui. Ogni variabile casuale può assumere in teoria una gamma di valori sebbene, in pratica, osserviamo un valore specifico per ogni individuo. Quando faremo riferiremo alle variabili casuali considerate in termini generali useremo lettere maiuscole come <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>; quando faremo riferimento ai valori che una variabile casuale assume in determinate circostanze useremo lettere minuscole come <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
</div>
<div id="variabili-indipendenti-e-variabili-dipendenti" class="section level3 hasAnchor" number="1.2.2">
<h3 class="hasAnchor"><span class="header-section-number">1.2.2</span> Variabili indipendenti e variabili dipendenti<a href="#variabili-indipendenti-e-variabili-dipendenti" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un primo compito fondamentale in qualsiasi analisi dei dati è l’identificazione delle variabili dipendenti (<span class="math inline">\(Y\)</span>) e delle variabili indipendenti (<span class="math inline">\(X\)</span>). Le variabili dipendenti sono anche chiamate variabili di esito o di risposta e le variabili indipendenti sono anche chiamate predittori o covariate. Ad esempio, nell’analisi di regressione, che esamineremo in seguito, la domanda centrale è quella di capire come <span class="math inline">\(Y\)</span> cambia al variare di <span class="math inline">\(X\)</span>. Più precisamente, la domanda che viene posta è: se il valore della variabile indipendente <span class="math inline">\(X\)</span> cambia, qual è la conseguenza per la variabile dipendente <span class="math inline">\(Y\)</span>? In parole povere, le variabili indipendenti e dipendenti sono analoghe a “cause” ed “effetti”, laddove le virgolette usate qui sottolineano che questa è solo un’analogia e che la determinazione delle cause può avvenire soltanto mediante l’utilizzo di un appropriato disegno sperimentale e di un’adeguata analisi statistica.</p>
<p>Se una variabile è una variabile indipendente o dipendente dipende dalla domanda di ricerca. A volte può essere difficile decidere quale variabile è dipendente e quale è indipendente, in particolare quando siamo specificamente interessati ai rapporti di causa/effetto. Ad esempio, supponiamo di indagare l’associazione tra esercizio fisico e insonnia. Vi sono evidenze che l’esercizio fisico (fatto al momento giusto della giornata) può ridurre l’insonnia. Ma l’insonnia può anche ridurre la capacità di una persona di fare esercizio fisico. In questo caso, dunque, non è facile capire quale sia la causa e quale l’effetto, quale sia la variabile dipendente e quale la variabile indipendente. La possibilità di identificare il ruolo delle variabili (dipendente/indipendente) dipende dalla nostra comprensione del fenomeno in esame.</p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>(#exm:unlabeled-div-4) </strong></span>Uno psicologo convoca 120 studenti universitari per un test di memoria.
Prima di iniziare l’esperimento, a metà dei soggetti viene detto che si
tratta di un compito particolarmente difficile; agli altri soggetti non
viene data alcuna indicazione. Lo psicologo misura il punteggio nella
prova di memoria di ciascun soggetto.</p>
<p>In questo esperimento, la variabile indipendente è l’informazione sulla difficoltà della prova. La variabile indipendente viene manipolata dallo sperimentatore assegnando i soggetti (di solito in maniera causale) o alla condizione (modalità) “informazione assegnata” o “informazione non data”. La
variabile dipendente è ciò che viene misurato nell’esperimento, ovvero
il punteggio nella prova di memoria di ciascun soggetto.</p>
</div>
</div>
<div id="la-matrice-dei-dati" class="section level3 hasAnchor" number="1.2.3">
<h3 class="hasAnchor"><span class="header-section-number">1.2.3</span> La matrice dei dati<a href="#la-matrice-dei-dati" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Le realizzazioni delle variabili esaminate in una rilevazione statistica
vengono organizzate in una <em>matrice dei dati</em>. Le colonne della matrice
dei dati contengono gli insiemi dei dati individuali di ciascuna
variabile statistica considerata. Ogni riga della matrice contiene tutte
le informazioni relative alla stessa unità statistica. Una generica
matrice dei dati ha l’aspetto seguente:</p>
<p><span class="math display">\[
D_{m,n} =
\begin{pmatrix}
  \omega_1 &amp; a_{1}   &amp; b_{1}   &amp; \cdots &amp; x_{1} &amp; y_{1}\\
  \omega_2 &amp; a_{2}   &amp; b_{2}   &amp; \cdots &amp; x_{2} &amp; y_{2}\\
  \vdots   &amp; \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots &amp; \vdots  \\
\omega_n  &amp; a_{n}   &amp; b_{n}   &amp; \cdots &amp; x_{n} &amp; y_{n}
\end{pmatrix}
\]</span></p>
<p>dove, nel caso presente, la prima colonna contiene il
nome delle unità statistiche, la seconda e la terza colonna si
riferiscono a due mutabili statistiche (variabili categoriali; <span class="math inline">\(A\)</span> e
<span class="math inline">\(B\)</span>) e ne presentano le modalità osservate nel campione mentre le ultime
due colonne si riferiscono a due variabili statistiche (<span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>) e ne
presentano le modalità osservate nel campione. Generalmente, tra le
unità statistiche <span class="math inline">\(\omega_i\)</span> non esiste un ordine progressivo; l’indice
attribuito alle unità statistiche nella matrice dei dati si riferisce
semplicemente alla riga che esse occupano.</p>
</div>
</div>
<div id="parametri-e-modelli" class="section level2 hasAnchor" number="1.3">
<h2 class="hasAnchor"><span class="header-section-number">1.3</span> Parametri e modelli<a href="#parametri-e-modelli" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ogni variabile casuale ha una <em>distribuzione</em> che descrive la probabilità che la variabile assuma qualsiasi valore in un dato intervallo.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> Senza ulteriori specificazioni, una distribuzione può fare riferimento a un’intera famiglia di distribuzioni. I parametri, tipicamente indicati con lettere greche come <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\alpha\)</span>, ci permettono di specificare di quale membro della famiglia stiamo parlando. Quindi, si può parlare di una variabile casuale con una distribuzione Normale, ma se viene specificata la media <span class="math inline">\(\mu\)</span> = 100 e la varianza <span class="math inline">\(\sigma^2\)</span> = 15, viene individuata una specifica distribuzione Normale – nell’esempio, la distribuzione del quoziente di intelligenza.</p>
<p>I metodi statistici parametrici specificano la famiglia delle distribuzioni e quindi utilizzano i dati per individuare, stimando i parametri, una specifica distribuzione all’interno della famiglia di distribuzioni ipotizzata. Se <span class="math inline">\(f\)</span> è la PDF di una variabile casuale <span class="math inline">\(Y\)</span>, l’interesse può concentrarsi sulla sua media e varianza. Nell’analisi di regressione, ad esempio, cerchiamo di spiegare come i parametri di <span class="math inline">\(f\)</span> dipendano dalle covariate <span class="math inline">\(X\)</span>. Nella regressione lineare classica, assumiamo che <span class="math inline">\(Y\)</span> abbia una distribuzione normale con media <span class="math inline">\(\mu = \E(Y)\)</span>, e stimiamo come <span class="math inline">\(\E(Y)\)</span> dipenda da <span class="math inline">\(X\)</span>. Poiché molti esiti psicologici non seguono una distribuzione normale, verranno introdotte distribuzioni più appropriate per questi risultati. I metodi non parametrici, invece, non specificano una famiglia di distribuzioni per <span class="math inline">\(f\)</span>. In queste dispense faremo riferimento a metodi non parametrici quando discuteremo della statistica descrittiva.</p>
<p>Il termine <em>modello</em> è onnipresente in statistica e nella <em>data science</em>. Il modello statistico include le ipotesi e le specifiche matematiche relative alla distribuzione della variabile casuale di interesse. Il modello dipende dai dati e dalla domanda di ricerca, ma raramente è unico; nella maggior parte dei casi, esiste più di un modello che potrebbe ragionevolmente usato per affrontare la stessa domanda di ricerca e avendo a disposizione i dati osservati. Nella previsione delle aspettative future dei pazienti depressi che discuteremo in seguito <span class="citation">(<a href="#ref-zetschefuture2019" role="doc-biblioref">Zetsche, Bürkner, and Renneberg 2019</a>)</span>, ad esempio, la specifica del modello include l’insieme delle covariate candidate, l’espressione matematica che collega i predittori con le aspettative future e qualsiasi ipotesi sulla distribuzione della variabile dipendente. La domanda di cosa costituisca un buon modello è una domanda su cui torneremo ripetutamente in questo insegnamento.</p>
</div>
<div id="effetto" class="section level2 hasAnchor" number="1.4">
<h2 class="hasAnchor"><span class="header-section-number">1.4</span> Effetto<a href="#effetto" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>L’<em>effetto</em> è una qualche misura dei dati. Dipende dal tipo di dati e dal tipo di test statistico che si vuole utilizzare. Ad esempio, se viene lanciata una moneta 100 volte e esce testa 66 volte, l’effetto sarà 66/100. Diventa poi possibile confrontare l’effetto ottenuto con l’effetto nullo che ci si aspetterebbe da una moneta bilanciata (50/100), o con qualsiasi altro effetto che può essere scelto. La <em>dimensione dell’effetto</em> si riferisce alla differenza tra l’effetto misurato nei dati e l’effetto nullo (di solito un valore che ci si aspetta di ottenere in base al caso soltanto).</p>
</div>
<div id="stima-e-inferenza" class="section level2 hasAnchor" number="1.5">
<h2 class="hasAnchor"><span class="header-section-number">1.5</span> Stima e inferenza<a href="#stima-e-inferenza" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La stima è il processo mediante il quale il campione viene utilizzato per conoscere le proprietà di interesse della popolazione. La media campionaria è una stima naturale della media della popolazione e la mediana campionaria è una stima naturale della mediana della popolazione. Quando parliamo di stimare una proprietà della popolazione (a volte indicata come parametro della popolazione) o di stimare la distribuzione di una variabile casuale, stiamo parlando dell’utilizzo dei dati osservati per conoscere le proprietà di interesse della popolazione. L’inferenza statistica è il processo mediante il quale le stime campionarie vengono utilizzate per rispondere a domande di ricerca e per valutare specifiche ipotesi relative alla popolazione. Discuteremo le procedure bayesiane dell’inferenza nell’ultima parte di queste dispense.</p>
</div>
<div id="metodi-e-procedure-della-psicologia" class="section level2 hasAnchor" number="1.6">
<h2 class="hasAnchor"><span class="header-section-number">1.6</span> Metodi e procedure della psicologia<a href="#metodi-e-procedure-della-psicologia" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Un modello psicologico di un qualche aspetto del comportamento umano o della mente ha le seguenti proprietà:</p>
<ol style="list-style-type: decimal">
<li>descrive le caratteristiche del comportamento in questione,</li>
<li>formula predizioni sulle caratteristiche future del comportamento,</li>
<li>è sostenuto da evidenze empiriche,</li>
<li>deve essere falsificabile (ovvero, in linea di principio, deve
potere fare delle predizioni su aspetti del fenomeno considerato che
non sono ancora noti e che, se venissero indagati, potrebbero
portare a rigettare il modello, se si dimostrassero incompatibili con
esso).</li>
</ol>
<p>L’analisi dei dati valuta un modello psicologico utilizzando strumenti statistici.</p>
<p>Questa dispensa è strutturata in maniera tale da rispecchiare la suddivisione tra i temi della misurazione, dell’analisi descrittiva e dell’inferenza. Nel prossimo Capitolo sarà affrontato il tema della misurazione e, nell’ultima parte della dispensa verrà discusso l’argomento più difficile, quello dell’inferenza. Prima di affrontare il secondo tema, l’analisi descrittiva dei dati, sarà necessario introdurre il linguaggio di programmazione statistica R (un’introduzione a R è fornita in Appendice). Inoltre, prima di potere discutere l’inferenza, dovranno essere introdotti i concetti di base della teoria delle probabilità, in quanto l’inferenza non è che l’applicazione della teoria delle probabilità all’analisi dei dati.</p>
<!--chapter:end:001_key_notions.Rmd-->
</div>
</div>
<div id="chapter-misurazione" class="section level1 hasAnchor" number="2">
<h1 class="hasAnchor"><span class="header-section-number">2</span> La misurazione in psicologia<a href="#chapter-misurazione" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Introduco il problema della misurazione in psicologia parlando dell’intelligenza. In quanto psicologi, siamo abituati a pensare alla misurazione dell’intelligenza, ma anche le persone che non sono psicologi sono ben familiari con la misurazione dell’intelligenza: tra le misurazioni delle caratteristiche psicologiche, infatti, la misurazione dell’intelligenza è forse la più conosciuta.</p>
<p>I test di intelligenza consistono in una serie di problemi di carattere verbale, numerico o simbolico. Come ci si può aspettare, alcune persone riescono a risolvere correttamente un numero maggiore di problemi di altre. Possiamo contare il numero di risposte corrette e osservare le differenze individuali nei punteggi calcolati. Scopriamo in questo modo che le differenze individuali nell’abilità di risolvere tali problemi risultano sorprendentemente stabili nell’età adulta. Inoltre, diversi test di intelligenza tendono ad essere correlati positivamente: le persone che risolvono un maggior numero di problemi verbali, in media, tenderanno anche a risolvere correttamente un numero più grande di numerici e simbolici. Esiste quindi una notevole coerenza delle differenze osservate tra le persone, sia nel tempo sia considerando diverse procedure di test e valutazione.</p>
<p>Avendo stabilito che ci sono differenze individuali tra le persone, è possibile esaminare le associazioni tra i punteggi dei test di intelligenza e altre variabili. Possiamo indagare se le persone con punteggi più alti nei test di intelligenza, rispetto a persone che ottengono punteggi più bassi, hanno più successo sul lavoro; se guadagnano di più; se votano in modo diverso; o se hanno un’aspettativa di vita più alta. Possiamo esaminare le differenze nei punteggi dei test di intelligenza in funzione di variabili come il genere, il gruppo etnico-razziale o lo stato socio-economico. Possiamo fare ricerche sull’associazione tra i punteggi dei test di intelligenza e l’efficienza dell’elaborazione neuronale, i tempi di reazione o la quantità di materia grigia all’interno della scatola cranica. Tutte queste ricerche sono state condotte e gli psicologi hanno scoperto una vasta gamma di associazioni tra le misure dell’intelligenza e altre variabili. Alcune di queste associazioni sono grandi e stabili, altre sono piccole e difficili da replicare. In riferimento all’intelligenza, dunque, gli psicologi hanno condotto un enorme numero di ricerche ponendosi domande diverse. In quali condizioni si verificano determinati effetti? Quali variabili mediano o moderano le relazioni tra i punteggi dei test di intelligenza e altre variabili? Queste relazioni si mantengono stabili in diversi gruppi di persone? Le ricerche sull’intelligenza umana sono un campo in continuo sviluppo.</p>
<p>Tuttavia, tuttavia una domanda sorge spontanea: i test di intelligenza misurano davvero qualcosa e, in caso affermativo, che cos’è questo qualcosa? Infatti, dopo un secolo di teoria e ricerca sui punteggi dei test di intelligenza e, in generale, sui test psicologici, non sappiamo ancora con precisione cosa effettivamente questi test misurano.
Queste considerazioni relative ai test di intelligenza ci conducono dunque alla domanda che ha motivato le precedenti considerazioni: cosa significa misurare un attributo psicologico? Questa è una domanda a cui è difficile rispondere, una domanda a cui è dedicata un’intera area di ricerca, quella della teoria della misurazione psicologica.</p>
<p>Non possiamo qui entrare nel merito delle complessità formali della teoria della misurazione psicologica – questo argomento verrà approfondito nei successivi insegnamenti sulla testistica psicologica. Ci limiteremo invece a presentare alcune nozioni di base su un tema centrale della teoria della misurazione psicologica: il tema delle scale delle misure psicologiche.</p>
<div id="le-scale-di-misura" class="section level2 hasAnchor" number="2.1">
<h2 class="hasAnchor"><span class="header-section-number">2.1</span> Le scale di misura<a href="#le-scale-di-misura" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In generale possiamo dire che la teoria della misurazione si occupa dello studio delle relazioni esistenti tra due domini: il “mondo fisico” e il “mondo psicologico”. Secondo la teoria della misurazione, la misurazione è un’attività rappresentativa, cioè è un processo di assegnazione di numeri in modo tale da preservare, all’interno del dominio numerico, le relazioni qualitative che sono state osservate nel mondo empirico. La teoria della misurazione ha lo scopo di specificare le condizioni necessarie per la costruzione di una rappresentazione adeguata delle relazioni empiriche all’interno di un sistema numerico. Da una prospettiva formale, le operazioni descritte dalla teoria della misurazione possono essere concettualizzate in termini di mappatura tra le relazioni esistenti all’interno di due insiemi (quello empirico e quello numerico). Il risultato di questa attività è chiamato “scala di misurazione”.</p>
<p>Una famosa teoria delle scale di misura è stata proposta da <span class="citation">Stevens (<a href="#ref-stevens46" role="doc-biblioref">1946</a>)</span>. Stevens ci fa notare che, in linea di principio, le variabili psicologiche sono in grado di rappresentare (preservare) con diversi gradi di accuratezza le relazioni qualitative che sono state osservate nei fenomeni psicologici. Secondo la teoria di Stevens, possiamo distinguere tra quattro scale di misura: le scale nominali (<em>nominal scales</em>), ordinali (<em>ordinal scales</em>), a intervalli (<em>interval scales</em>), di rapporti (<em>ratio scales</em>). Tali scale di misura consentono operazioni aritmetiche diverse, come indicato nella tabella successiva, in quanto ciasuna di esse è in grado di “catturare” soltanto alcune delle proprietà dei fenomeni psicologici che intende misurare.</p>
<p><img src="images/misurazione_2.png" style="width:80.0%" /></p>
<div id="scala-nominale" class="section level3 hasAnchor" number="2.1.1">
<h3 class="hasAnchor"><span class="header-section-number">2.1.1</span> Scala nominale<a href="#scala-nominale" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il livello di misurazione più semplice è quello della scala nominale. Questa scala di misurazione corrisponde ad una tassonomia. I simoboli o numeri che costituiscono questa scala non sono altro che i nomi delle categorie che utilizziamo per classificare i fenomeni psicologici. In base alle misure fornite da una scala nominale, l’unica cosa che siamo in grado di dire a proposito di una caratteristica psicologica è se essa è uguale o no ad un’altra caratteristica psicologica.</p>
<p>La scala nominale raggruppa dunque i dati in categorie qualitative <em>mutuamente esclusive</em> (cioè nessun dato si può collocare in più di una categoria).
Esiste la sola relazione di equivalenza tra le misure delle u.s., cioè
nella scala nominale gli elementi del campione appartenenti a classi
diverse sono differenti, mentre tutti quelli della stessa classe sono
tra loro equivalenti: <span class="math inline">\(x_i = x_j\)</span> oppure <span class="math inline">\(x_i \neq x_j\)</span>.</p>
<p>L’unica operazione algebrica che possiamo compiere sulle modalità della scala nominale è quella di contare le u.s. che appartengono ad ogni modalità e contare il numero delle modalità (classi di equivalenza). Dunque la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.</p>
<p>A partire da una scala nominale è possibile costruire altre scale nominali che sono equivalenti alla prima trasformando i valori della scala di partenza in modo tale
da cambiare i nomi delle modalità, ma lasciando però inalterata la suddivisione u.s. nelle medesime classi di equivalenza. Questo significa che prendendo una variabile misurata su scala nominale e cambiando i nomi delle sue categorie otteniamo una nuova variabile esattamente corrispondente alla prima.</p>
</div>
<div id="scala-ordinale" class="section level3 hasAnchor" number="2.1.2">
<h3 class="hasAnchor"><span class="header-section-number">2.1.2</span> Scala ordinale<a href="#scala-ordinale" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La scala ordinale conserva la proprietà della scala nominale di classificare ciascuna u.s. all’interno di una e una sola categoria, ma alla relazione di equivalenza tra elementi di una stessa classe aggiunge la relazione di ordinamento tra le classi di equivalenza. Essendo basata su una relazione d’ordine, una scala ordinale descrive soltanto l’ordine di rango tra le modalità, ma non ci dà alcuna informazione su quanto una modalità sia più grande di un’altra. Non ci dice, per esempio, se la distanza tra le modalità <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> sia uguale, maggiore o minore della distanza tra le modalità <span class="math inline">\(b\)</span> e <span class="math inline">\(c\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>(#exm:unlabeled-div-5) </strong></span>Un esempio classico di scala ordinale è quello della scala Mohs per la
determinazione della durezza dei minerali. Per stabilire la durezza dei
minerali si usa il criterio empirico della scalfittura. Vengono
stabiliti livelli di durezza crescente da 1 a 10 con riferimento a dieci
minerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo,
topazio, corindone e diamante. Un minerale appartenente ad uno di questi
livelli se scalfisce quello di livello inferiore ed è scalfito da quello
di livello superiore.</p>
</div>
</div>
<div id="scala-ad-intervalli" class="section level3 hasAnchor" number="2.1.3">
<h3 class="hasAnchor"><span class="header-section-number">2.1.3</span> Scala ad intervalli<a href="#scala-ad-intervalli" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La scala ad intervalli include le proprietà di quella nominale e di
quella ordinale, e in più consente di misurare le distanze tra le coppie
di u.s. nei termini di un intervallo costante, chiamato <em>unità di
misura</em>, a cui viene attribuito il valore “1”. La posizione dell’origine
della scala, cioè il punto zero, è scelta arbitrariamente, nel senso che
non indica l’assenza della quantità che si sta misurando. Avendo uno
zero arbitrario, questa scala di misura consente valori negativi. Lo
zero, infatti, <em>non</em> viene attribuito all’u.s. in cui la proprietà
misurata risulta assente.</p>
<p>La scala a intervalli equivalenti ci consente di effettuare operazioni
algebriche basate sulla differenza tra i numeri associati ai diversi
punti della scala, operazioni algebriche non era possibile eseguire nel
caso di misure a livello di scala ordinale o nominale. Il limite della
scala ad intervalli è quello di non consentire il calcolo del rapporto
tra coppie di misure. Possiamo dire, per esempio, che la distanza tra
<span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> è la metà della distanza tra <span class="math inline">\(c\)</span> e <span class="math inline">\(d\)</span>. Oppure che la distanza
tra <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> è uguale alla distanza tra <span class="math inline">\(c\)</span> e <span class="math inline">\(d\)</span>. Non possiamo dire,
però, che <span class="math inline">\(a\)</span> possiede la proprietà misurata in quantità doppia rispetto
<span class="math inline">\(b\)</span>. Non possiamo cioè stabilire dei rapporti diretti tra le misure
ottenute. Solo per le <em>differenze</em> tra le modalità sono dunque permesse
tutte le operazioni aritmetiche: le differenze possono essere tra loro
sommate, elevate a potenza oppure divise, determinando così le quantità
che stanno alla base della statistica inferenziale.</p>
<p>Nelle scale ad intervalli equivalenti, l’unità di misura è arbitraria,
ovvero può essere cambiata attraverso una dilatazione, operazione che
consiste nel moltiplicare tutti i valori della scala per una costante
positiva. Poiché l’aggiunta di una costante non altera le differenze tra
i valori della scala, è anche ammessa la traslazione, operazione che
consiste nel sommare una costante a tutti i valori della scala. Essendo
la scala invariate rispetto alla traslazione e alla dilatazione, le
trasformazioni ammissibili sono le <em>trasformazioni lineari</em>:</p>
<p><span class="math display">\[
y&#39; = a + by, \quad b &gt; 0.
\]</span>
L’aspetto che rimane invariante a seguito di una trasformazione lineare
è l’uguaglianza dei rapporti fra intervalli.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>(#exm:unlabeled-div-6) </strong></span>Esempio di scala ad intervalli è la temperatura misurata in gradi
Celsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è
possibile stabilire se due modalità sono uguali o diverse: 30<span class="math inline">\(^\circ\)</span>C
<span class="math inline">\(\neq\)</span> 20<span class="math inline">\(^\circ\)</span>C. Come per la scala ordinale è possibile mettere due
modalità in una relazione d’ordine: 30<span class="math inline">\(^\circ\)</span>C <span class="math inline">\(&gt;\)</span> 20<span class="math inline">\(^\circ\)</span>C. In
aggiunta ai casi precedenti, però, è possibile definire una unità di
misura per cui è possibile dire che tra 30<span class="math inline">\(^\circ\)</span>C e 20<span class="math inline">\(^\circ\)</span>C c’è
una differenza di 30<span class="math inline">\(^\circ\)</span> - 20<span class="math inline">\(^\circ\)</span> = 10<span class="math inline">\(^\circ\)</span>C. I valori di
temperatura, oltre a poter essere ordinati secondo l’intensità del
fenomeno, godono della proprietà che le differenze tra loro sono
direttamente confrontabili e quantificabili.</p>
<p>Il limite della scala ad intervalli è quello di non consentire il
calcolo del rapporto tra coppie di misure. Ad esempio, una temperatura
di 80<span class="math inline">\(^\circ\)</span>C non è il doppio di una di 40<span class="math inline">\(^\circ\)</span>C. Se infatti
esprimiamo le stesse temperature nei termini della scala Fahrenheit,
allora i due valori non saranno in rapporto di 1 a 2 tra loro. Infatti,
20<span class="math inline">\(^\circ\)</span>C = 68<span class="math inline">\(^\circ\)</span>F e 40<span class="math inline">\(^\circ\)</span>C = 104<span class="math inline">\(^\circ\)</span>F. Questo significa
che la relazione “il doppio di” che avevamo individuato in precedenza si
applicava ai numeri della scala centigrada, ma non alla proprietà
misurata (cioè la temperatura). La decisione di che scala usare
(Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non
deve influenzare le inferenze che traiamo dai dati. Queste inferenze,
infatti, devono dirci qualcosa a proposito della realtà empirica e non
possono in nessun modo essere condizionate dalle nostre scelte
arbitrarie che ci portano a scegliere la scala Centigrada piuttosto che
quella Fahrenheit.</p>
<p>Consideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo in esame, ad esempio, tre temperature:
<span class="math inline">\(20^\circ C = 68^\circ F\)</span>,
<span class="math inline">\(15^\circ C = 59^\circ F\)</span>,
<span class="math inline">\(10^\circ C = 50 ^\circ F\)</span>.</p>
<p>È facile rendersi conto del fatto che i rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:</p>
<p><span class="math display">\[
  \frac{20^\circ C - 10^\circ C}{20^\circ C - 15^\circ C} =
  \frac{68^\circ F - 50^\circ F}{68^\circ F-59^\circ F} = 2.
\]</span></p>
</div>
</div>
<div id="scala-di-rapporti" class="section level3 hasAnchor" number="2.1.4">
<h3 class="hasAnchor"><span class="header-section-number">2.1.4</span> Scala di rapporti<a href="#scala-di-rapporti" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nella scala a rapporti equivalenti la posizione dello zero non è
arbitraria, ma corrisponde all’elemento dotato di intensità nulla
rispetto alla proprietà misurata. Una scala a rapporti equivalenti si
costruisce associando il numero 0 all’elemento con intensità nulla;
viene poi scelta un’unità di misura <span class="math inline">\(u\)</span> e, ad ogni elemento, si assegna
un numero <span class="math inline">\(a\)</span> definito come: <span class="math display">\[a = \frac{d}{u}\]</span> dove <span class="math inline">\(d\)</span> rappresenta la
distanza dall’origine. Alle u.s. vengono dunque assegnati dei numeri
tali per cui le differenze e i rapporti tra i numeri riflettono le
differenze e i rapporti tra le intensità della proprietà misurata.</p>
<p>Operazioni aritmetiche sono possibili non solo sulle differenze tra i
valori della scala (come per la scala a intervalli equivalenti), ma
anche sui valori stessi della scala. L’unica arbitrarietà riguarda
l’unità di misura che si utilizza. L’unità di misura può cambiare, ma
qualsiasi unità di misura si scelga, lo zero deve sempre indicare
l’intensità nulla della proprietà considerata.</p>
<p>Le trasformazioni ammissibili a questo livello di scala sono dette
trasformazioni di similarità: <span class="math display">\[y&#39; = by, \quad b &gt; 0.\]</span> A questo livello
di scala, a seguito delle trasformazioni ammissibili, rimangono
invariati anche i rapporti: <span class="math display">\[\frac{y_i}{y_j} = \frac{y&#39;_i}{y&#39;_j}.\]</span></p>
</div>
</div>
<div id="gerarchia-dei-livelli-di-scala-di-misura" class="section level2 hasAnchor" number="2.2">
<h2 class="hasAnchor"><span class="header-section-number">2.2</span> Gerarchia dei livelli di scala di misura<a href="#gerarchia-dei-livelli-di-scala-di-misura" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">Stevens (<a href="#ref-stevens46" role="doc-biblioref">1946</a>)</span> parla di <em>livelli di scala</em> poiché i quattro tipi di scala di
misura stanno in una precisa gerarchia: la scala nominale rappresenta il
livello più basso della misurazione, la scala a rapporti equivalenti è
invece il livello più alto.</p>
<table>
<colgroup>
<col width="25%" />
<col width="74%" />
</colgroup>
<thead>
<tr class="header">
<th>Scale di modalità</th>
<th>Operazioni aritmetiche</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>nominali</td>
<td>enumerare le classi di equivalenza e/o</td>
</tr>
<tr class="even">
<td></td>
<td>le frequenze per ciascuna classe di equivalenza</td>
</tr>
<tr class="odd">
<td>ordinali</td>
<td>enumerare le classi di equivalenza e/o</td>
</tr>
<tr class="even">
<td></td>
<td>le frequenze per ciascuna classe di equivalenza</td>
</tr>
<tr class="odd">
<td>intervallari</td>
<td>differenze (rapporti tra differenze)</td>
</tr>
<tr class="even">
<td>di rapporti</td>
<td>rapporti diretti tra le misure</td>
</tr>
</tbody>
</table>
<p>Passando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.</p>
<p><img src="images/misurazione_1.png" style="width:80.0%" /></p>
<p>Per ciò che riguarda le trasformazioni ammissibili, più il livello di
scala è basso, più le funzioni sono generali (sono minori cioè i vincoli
per passare da una rappresentazione numerica ad un’altra equivalente).
Salendo la gerarchia, la natura delle funzioni di trasformazione si fa
più restrittiva.</p>
</div>
<div id="variabili-discrete-o-continue" class="section level2 hasAnchor" number="2.3">
<h2 class="hasAnchor"><span class="header-section-number">2.3</span> Variabili discrete o continue<a href="#variabili-discrete-o-continue" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le variabili a livello di intervalli e di rapporti possono essere
discrete o continue. Le variabili discrete possono assumere alcuni
valori ma non altri. Una volta che l’elenco di valori accettabili è
stato specificato, non ci sono casi che cadono tra questi valori.
Le variabili discrete di solito assumono valori interi.</p>
<p>Quando una variabile può assumere qualsiasi valore entro un intervallo
specificato, allora si dice che la variabile è continua. In teoria, ciò
significa che frazioni e decimali possono essere utilizzati per
raggiungere un livello di precisione qualsiasi. In pratica, a un certo
punto dobbiamo arrotondare i numeri, rendendo tecnicamente la variabile
discreta. In variabili veramente discrete, tuttavia, non è possibile
aumentare a piacimento il livello di precisione della misurazione.</p>
<p><img src="images/misurazione_3.png" style="width:80.0%" /></p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>(#exm:unlabeled-div-7) </strong></span>Il numero di biciclette possedute da una persona è una variabile discreta poiché tale variabile può assumere come modalità solo i numeri interi non negativi. Frazioni di bicicletta non hanno senso.</p>
</div>
</div>
<div id="alcune-misure-sono-migliori-di-altre" class="section level2 hasAnchor" number="2.4">
<h2 class="hasAnchor"><span class="header-section-number">2.4</span> Alcune misure sono migliori di altre<a href="#alcune-misure-sono-migliori-di-altre" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In psicologia, ciò che vogliamo misurare non è una caratteristica fisica, ma invece è un concetto teorico inosservabile, ovvero un costrutto. <em>Un costrutto rappresenta il risultato di una fondata riflessione scientifica, non è per definizione accessibile all’osservazione diretta, ma viene inferito dall’osservazione di opportuni indicatori</em> (Sartori, 2005). Ad esempio, supponiamo che un docente voglia valutare quanto bene uno studente comprenda la distinzione tra le quattro diverse scale di misura che sono state descritte sopra. Il docente potrebbe predisporre un test costituito da un insieme di domande e potrebbe contare a quante domande lo studente risponde correttamente. Questo
test, però, può o può non essere una buona misura del costrutto relativo
alla conoscenza effettiva delle quattro scale di misura. Per esempio, se
il docente scrive le domande del test in modo ambiguo o se usa una
linguaggio troppo tecnico che lo studente non conosce, allora i
risultati del test potrebbero suggerire che lo studente non conosce la
materia in questione anche se in realtà questo non è vero. D’altra
parte, se il docente prepara un test a scelta multipla con risposte
errate molto ovvie, allora lo studente può ottenere dei buoni risultati
al test anche senza essere in grado di comprendere adeguatamente le
proprietà delle quattro scale di misura. In generale non è possibile misurare un costrutto senza una certa quantità di errore. Poniamoci dunque il problema di determinare in che modo una misurazione possa dirsi adeguata.</p>
<div id="tipologie-di-errori" class="section level3 hasAnchor" number="2.4.1">
<h3 class="hasAnchor"><span class="header-section-number">2.4.1</span> Tipologie di errori<a href="#tipologie-di-errori" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>L’errore è, per definizione, la differenza tra il valore vero e il
valore misurato della grandezza in esame. Gli errori sono classificati
come sistematici (o determinati) e casuali (o indeterminati). Gli errori
casuali sono fluttuazioni, in eccesso o in difetto rispetto al valore
reale, delle singole determinazioni e sono dovuti alle molte variabili
incontrollabili che influenzano ogni misura psicologica. Gli errori
sistematici, invece, influiscono sulla misurazione sempre nello stesso
senso e, solitamente, per una stessa quantità (possono essere additivi o
proporzionali).</p>
<p>Le differenze tra le due tipologie di errori, sistematici e casuali,
introducono i concetti di accuratezza e di precisione della misura. Una
misura viene definita:</p>
<ul>
<li><em>accurata</em>, quando vi è un accordo tra la misura effettuata ed il
valore reale;</li>
<li><em>precisa</em> quando, ripetendo più volte la misura, i risultati
ottenuti sono concordanti, cioè differiscono in maniera irrilevante
tra loro.</li>
</ul>
<p>La metafora del tiro a bersaglio illustra la relazione tra precisione e accuratezza.</p>
<div class="figure" style="text-align: center">
<img src="images/misurazione_4.png" alt="Metafora del tiro al bersaglio." width="1181" />
<p class="caption">
(#fig:fig-tiro-bersaglio)Metafora del tiro al bersaglio.
</p>
</div>
<p>Per tenere sotto controllo l’incidenza degli errori, sono stati
introdotti in psicologia i concetti di attendibilità e validità.</p>
<p>Uno strumento si dice <em>attendibile</em> quando valuta in modo coerente e
stabile la stessa variabile: i risultati ottenuti si mantengono costanti
dopo ripetute somministrazione ed in assenza di variazioni psicologiche
e fisiche dei soggetti sottoposti al test o cambiamenti dell’ambiente in
cui ha luogo la somministrazione.</p>
<p>L’attendibilità di uno strumento, però, non è sufficiente: in primo luogo uno
strumento di misura deve essere <em>valido</em>, laddove la validità rappresenta
il grado in cui uno strumento misura effettivamente ciò che dovrebbe
misurare. In genere, si fa riferimento ad almeno quattro tipi di
validità.</p>
<ul>
<li>La <em>validità di costrutto</em> riguarda il grado in cui un test misura
ciò per cui è stato costruito. Essa si suddivide in: validità
convergente e validità divergente. La validità convergente fa
riferimento alla concordanza tra uno strumento e un altro che misura
lo stesso costrutto. La validità divergente, al contrario, valuta il
grado di discriminazione tra strumenti che misurano costrutti
differenti. Senza validità di costrutto le altre forme di validità
non hanno senso.</li>
<li>In base alla <em>validità di contenuto</em>, un test fornisce una misura
valida di un attributo psicologico se il dominio dell’attributo è
rappresentato in maniera adeguata dagli item del test. Un requisito
di base della validità di contenuto è la rilevanza e la
rappresentatività del contenuto degli item in riferimento
all’attributo che il test intende misurare.</li>
<li>La <em>validità di criterio</em> valuta il grado di concordanza tra i
risultati dello strumento considerato e i risultati ottenuti da
altri strumenti che misurano lo stesso costrutto, o tra i risultati
dello strumento considerato e un criterio esterno. Nella validità
concorrente, costrutto e criterio vengono misurati contestualmente,
consentendo un confronto immediato. Nella validità predittiva, il
costrutto viene misurato prima e il criterio in un momento
successivo, consentendo la valutazione della capacità dello
strumento di predire un evento futuro.</li>
<li>Infine, la <em>validità di facciata</em> fa riferimento al grado in cui il
test appare valido ai soggetti a cui esso è diretto. La validità di
facciata è importante in ambiti particolari, quali ad esempio la
selezione del personale per una determinata occupazione. In questo
caso è ovviamente importante che chi si sottopone al test ritenga
che il test vada a misurare quegli aspetti che sono importanti per
le mansioni lavorative che dovranno essere svolte, piuttosto che
altre cose. In generale, la validità di facciata non è utile, tranne
in casi particolari.</li>
</ul>
</div>
</div>
<div id="commenti-e-considerazioni-finali" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una domanda che uno psicologo spesso si pone è: “sulla base delle
evidenze osservate, possiamo concludere dicendo che l’intervento
psicologico è efficace nel trattamento e nella cura del disturbo?” Le
considerazioni svolte in questo capitolo dovrebbero farci capire che,
prima di cercare di rispondere a questa domanda con l’analisi statistica
dei dati, devono essere affrontati i problemi della validità e
dell’attendibilità delle misure (oltre a stabilire l’appropriato livello
di scala di misura delle osservazioni). L’attendibilità è un
prerequisito della validità. Se gli errori di misurazione sono troppo
grandi, i dati sono inutili. Inoltre, uno strumento di misurazione può
essere preciso ma non valido. La validità e l’attendibilità delle
misurazioni sono dunque entrambe necessarie.</p>
<p>In generale, l’attendibilità e la validità delle misure devono essere
valutate per capire se i dati raccolti da un ricercatore siano adeguati
(1) per fornire una risposta alla domanda della ricerca, e (2) per
giungere alla conclusione proposta dal ricercatore alla luce dei
risultati dell’analisi statistica che è stata eseguita. È chiaro che le
informazioni fornite in questo capitolo si limitano a scalfire la
superficie di questi problemi. I concetti qui introdotti, però, devono
sempre essere tenuti a mente e costituiscono il fondamento di quanto
verrà esposto nei capitoli successivi.</p>
<!--chapter:end:005_measurement.Rmd-->
</div>
</div>
<div id="part-analisi-esplorativa-dei-dati" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Analisi esplorativa dei dati<a href="#part-analisi-esplorativa-dei-dati" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="ch:freq-distr" class="section level1 hasAnchor" number="3">
<h1 class="hasAnchor"><span class="header-section-number">3</span> Variabili e distribuzioni di frequenza<a href="#ch:freq-distr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Le analisi esplorative dei dati e la statistica descrittiva costituiscono la prima fase dell’analisi dei dati psicologici. Consentono di capire come i dati sono distribuiti, ci aiutano ad individuare le osservazioni anomale e gli errori di tabulazione. Consentono di riassumere le distribuzioni dei dati mediante indici sintetici. Consentono di visualizzare e di studiare le relazioni tra le variabili. In questo Capitolo, dopo avere presentato gli obiettivi dell’analisi esplorative dei dati, discuteremo il problema della descrizione numerica e della rappresentazione grafica delle distribuzioni di frequenza.</p>
<div id="chapter-descript" class="section level2 hasAnchor" number="3.1">
<h2 class="hasAnchor"><span class="header-section-number">3.1</span> Introduzione all’esplorazione dei dati<a href="#chapter-descript" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le analisi esplorative dei dati sono indispensabili per condurre in modo corretto una qualsiasi analisi statistica, dal livello base a quello avanzato. Si parla di analisi descrittiva se l’obiettivo è quello di descrivere le caratteristiche di un campione. Si parla di analisi esplorativa dei dati (<em>Exploratory Data Analysis</em> o EDA) se l’obiettivo è quello di esplorare i dati alla ricerca di nuove informazioni e relazioni tra variabili. Questa distinzione, seppur importante a livello teorico, nella pratica è più fumosa perché spesso entrambe le situazioni si verificano contemporaneamente nella stessa indagine statistica e le metodologie di analisi che si utilizzano sono molto simili.</p>
<p>Né il calcolo delle statistiche descrittive né l’analisi esplorativa dei dati possono essere condotte senza utilizzare un software. Le descrizioni dei concetti di base della EDA saranno dunque fornite di pari passo alla spiegazione di come le quantità discusse possono essere calcolate in pratica utilizzando .</p>
</div>
<div id="un-excursus-storico" class="section level2 hasAnchor" number="3.2">
<h2 class="hasAnchor"><span class="header-section-number">3.2</span> Un excursus storico<a href="#un-excursus-storico" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Nel 1907 Francis Galton, cugino di Charles Darwin, matematico e
statistico autodidatta, geografo, esploratore, teorico della
dattiloscopia (ovvero, dell’uso delle impronte digitali a fini
identificativi) e dell’eugenetica, scrisse una lettera alla rivista
scientifica Nature sulla sua visita alla <em>Fat Stock and Poultry
Exhibition</em> di Plymouth. Lì vide alcuni membri del pubblico partecipare
ad un gioco il cui scopo era quello di indovinare il peso della carcassa
di un grande bue che era appena stato scuoiato. Galton si procurò i 787
dei biglietti che erano stati compilati dal pubblico e considerò il
valore medio di 547 kg come la “scelta democratica” dei partecipanti, in
quanto “ogni altra stima era stata giudicata troppo alta o troppo bassa
dalla maggioranza dei votanti”. Il punto interessante è che il peso
corretto di 543 kg si dimostrò essere molto simile alla “scelta
democratica” basata sulle stime dei 787 partecipanti. Galton intitolò la
sua lettera a Nature <em>Vox Populi</em> (voce del popolo), ma questo processo
decisionale è ora meglio conosciuto come la “saggezza delle folle”
(<em>wisdom of crowds</em>). Possiamo dire che, nel suo articolo del 1907,
Galton effettuò quello che ora chiamiamo un riepilogo dei dati, ovvero
calcolò un indice sintetico a partire da un insieme di dati. In questo
capitolo esamineremo le tecniche che sono state sviluppate nel secolo
successivo per riassumere le grandi masse di dati con cui sempre più
spesso ci dobbiamo confrontare. Vedremo come calcolare e interpretare
gli indici di posizione e di dispersione, discuteremo le distribuzioni
di frequenze e le relazioni tra variabili. Vedremo inoltre quali sono le
tecniche di visualizzazione che ci consentono di rappresentare questi
sommari dei dati mediante dei grafici. Ma prima di entrare nei dettagli, prendiamoci un momento per capire perché abbiamo bisogno della statistica e, per ciò che stiamo discutendo qui, della statistica descrittiva.</p>
<p>In generale, che cos’è la statistica? Ci sono molte definizioni. Fondamentalmente, la statistica è un insieme di tecniche che ci consentono di dare un senso al mondo attraverso i dati. Ciò avviene tramite il processo di analisi statistica. L’analisi statistica traduce le domande che abbiamo a proposito del mondo in modelli matematici, utilizza i dati per scegliere i modelli matematici che sono apppropriati per descrivere il mondo e, infine, applica tali modelli per trovare una risposta alle domande che ci siamo posti. La statistica consente quindi di collegare le nostre domande a proposito del mondo ai dati, di utilizzare i dati per trovare le risposte alle domande che ci siamo posti e di valutare l’impatto delle risposte che abbiamo trovato.</p>
</div>
<div id="riassumere-i-dati" class="section level2 hasAnchor" number="3.3">
<h2 class="hasAnchor"><span class="header-section-number">3.3</span> Riassumere i dati<a href="#riassumere-i-dati" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Iniziamo a porci una domanda. Quando riassumiamo i dati, necessariamente buttiamo via delle informazioni; ma è una buona idea procedere in questo modo? Non sarebbe meglio conservare le informazioni specifiche di ciascun soggetto che partecipa ad un esperimento psicologico, al di là di ciò che viene trasmesso dagli indici riassuntivi della statistica descrittiva? Che dire delle informazioni che descrivono come sono stati raccolti i dati, come l’ora del giorno o l’umore del partecipante? Tutte queste informazioni vengono perdute quando riassumiamo i dati. La risposta alla domanda che ci siamo posti è che, in generale, non è una buona idea conservare tutti i dettagli di ciò che sappiamo. È molto più utile riassumere le informazioni perché la semplificazione risultante consente i processi di <em>generalizzazione</em>.</p>
<p>In un contesto letterario, l’importanza della generalizzazione è stata
sottolineata da Jorge Luis Borges nel suo racconto “Funes o della
memoria”, che descrive un individuo che perde la capacità di
dimenticare. Borges si concentra sulla relazione tra generalizzazione e
pensiero: <em>“Pensare è dimenticare una differenza, generalizzare, astrarre. Nel mondo troppo pieno di Funes, c’erano solo dettagli.”</em></p>
<p>Come possiamo ben capire, la vita di Funes non è facile. Se facciamo
riferimento alla psicologia possiamo dire che gli psicologi hanno
studiato a lungo l’utilità della generalizzazione per il pensiero. Un
esempio è fornito dal fenomeno della formazione dei concetti e lo
psicologo che viene in mente a questo proposito è sicuramente Eleanor
Rosch, la quale ha studiato i principi di base della categorizzazione. I
concetti ci forniscono uno strumento potente per organizzare le
conoscenze. Noi siamo in grado di riconoscere facilmente i diversi
esemplare di un concetto – per esempio, “gli uccelli” – anche se i
singoli esemplari che fanno parte di una categoria sono molto diversi
tra loro (l’aquila, il gabbiano, il pettirosso). L’uso dei concetti, cioè
la generalizzazione, è utile perché ci consente di fare previsioni sulle
proprietà dei singoli esemplari che appartengono ad una categoria, anche
se non abbiamo mai avuto esperienza diretta con essi – per esempio,
possiamo fare la predizione che tutti gli uccelli possono volare e
mangiare vermi, ma non possono guidare un’automobile o parlare in
inglese. Queste previsioni non sono sempre corrette, ma sono utili.</p>
<p>Le statistiche descrittive, in un certo senso, ci fornisco l’analogo dei
“prototipi” che, secondo Eleanor Rosch, stanno alla base del processo
psicologico di creazione dei concetti. Un prototipo è l’esemplare più
rappresentativo di una categoria. In maniera simile, una statistica
descrittiva come la media, ad esempio, potrebbe essere intesa come
l’osservazione “tipica”.</p>
<p>La statistica descrittiva ci fornisce gli strumenti per riassumere i
dati che abbiamo a disposizione in una forma visiva o numerica. Le
rappresentazioni grafiche più usate della statistica descrittiva sono
gli istogrammi, i diagrammi a dispersione o i box-plot, e gli indici
sintetici più comuni sono la media, la mediana, la varianza e la
deviazione standard.</p>
</div>
<div id="i-dati-grezzi" class="section level2 hasAnchor" number="3.4">
<h2 class="hasAnchor"><span class="header-section-number">3.4</span> I dati grezzi<a href="#i-dati-grezzi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Per introdurre i principali strumenti della statistica descrittiva considereremo qui i dati raccolti da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>. Questi ricercatori hanno studiato le aspettative negative quale meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello studio, <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> si sono chiesti se individui depressi maturino delle aspettative accurate sul loro umore futuro, oppure se tali aspettative sono distorte negativamente.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. In uno studio viene esaminato un campione costituito da 30 soggetti con almeno un episodio depressivo maggiore e da 37 controlli sani. Gli autori hanno misurato il livello depressivo con il <em>Beck Depression Inventory</em> (BDI-II). Questi sono i dati che considereremo qui.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-8" class="exercise"><strong>(#exr:unlabeled-div-8) </strong></span>Qual è la la gravità della depressione riportata dai soggetti nel campione esaminato da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>?</p>
<p>Per rispondere a questa domanda, iniziamo a leggere in <span class="math inline">\(\R\)</span> i dati, assumendo che il file <code>data.mood.csv</code> si trovi nella cartella <code>data</code> contenuta nella <em>working directory</em>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;rio&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> rio<span class="sc">::</span><span class="fu">import</span>(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;data.mood.csv&quot;</span>),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">header =</span> <span class="cn">TRUE</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>C’è un solo valore BDI-II per ciascun soggetto ma tale valore viene ripetuto tante volte quante volte sono le righe del <code>data.frame</code> associate ad ogni soggetto (ciascuna riga corrispondente ad una prova diversa). È dunque necessario trasformare il <code>data.frame</code> in modo tale da avere un’unica riga per ciascun soggetto, ovvero un unico valore BDI-II per soggetto.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(esm_id) <span class="sc">%&gt;%</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi =</span> <span class="fu">mean</span>(bdi)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span></code></pre></div>
<p>Ci sono dunque 66 soggetti i quali hanno ottenuto i valori sulla scala del BDI-II stampati di seguito. Per semplicità, li presentiamo ordinati dal più piccolo al più grande.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sort</span>(bysubj<span class="sc">$</span>bdi)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [18]  1  1  1  1  1  1  1  1  2  2  2  2  3  3  3  5  7</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [35]  9 12 19 22 22 24 25 25 26 26 26 27 27 28 28 30 30</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [52] 30 31 31 33 33 34 35 35 35 36 39 41 43 43 44</span></span></code></pre></div>
</div>
</div>
<div id="distribuzioni-di-frequenze" class="section level2 hasAnchor" number="3.5">
<h2 class="hasAnchor"><span class="header-section-number">3.5</span> Distribuzioni di frequenze<a href="#distribuzioni-di-frequenze" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>È chiaro che i dati grezzi sono di difficile lettura. Poniamoci dunque il problema di creare una rappresentazione sintetica e comprensibile di questo insieme di valori. Uno dei modi che ci consentono di effettuare una sintesi dei dati è quello di generare una <em>distribuzione di frequenze</em>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>(#def:unlabeled-div-9) </strong></span>Una distribuzione di frequenze è un riepilogo del conteggio della frequenza con cui le modalità osservate in un insieme di dati si verificano in un intervallo di valori.</p>
</div>
<p>In altre parole, la distribuzione di frequenze della variabile <span class="math inline">\(X\)</span> corrisponde all’insieme delle frequenze assegnate a ciascun possibile valore di <span class="math inline">\(X\)</span>.</p>
<p>Per creare una distribuzione di frequenze possiamo procedere effettuando una partizione delle modalità della variabile di interesse in <span class="math inline">\(m\)</span> classi (denotate con <span class="math inline">\(\Delta_i\)</span>) tra loro disgiunte. In tale partizione, la classe <span class="math inline">\(i\)</span>-esima coincide con un intervallo di valori aperto a destra <span class="math inline">\([a_i, b_i)\)</span> o aperto a sinistra <span class="math inline">\((a_i, b_i]\)</span>. Ad ogni classe <span class="math inline">\(\Delta_i\)</span> avente <span class="math inline">\(a_i\)</span> e <span class="math inline">\(b_i\)</span> come limite inferiore e superiore associamo l’ampiezza <span class="math inline">\(b_i - a_i\)</span> (non necessariamente uguale per ogni
classe) e il valore centrale <span class="math inline">\(\bar{x}_i\)</span>. La scelta delle classi è arbitraria, ma è buona norma non definire classi con un numero troppo piccolo (&lt; 5) di osservazioni. Poiché ogni elemento dell’insieme <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> appartiene ad una ed una sola classe <span class="math inline">\(\Delta_i\)</span>, possiamo calcolare le quantità elencate di seguito.</p>
<ul>
<li><p>La <em>frequenza assoluta</em> <span class="math inline">\(n_i\)</span> di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe <span class="math inline">\(\Delta_i\)</span>.</p>
<ul>
<li>Proprietà: <span class="math inline">\(n_1 + n_2 + \dots + n_m = n\)</span>.</li>
</ul></li>
<li><p>La <em>frequenza relativa</em> <span class="math inline">\(f_i = n_i/n\)</span> di ciascuna classe.</p>
<ul>
<li>Proprietà: <span class="math inline">\(f_1+f_2+\dots+f_m =1\)</span>.</li>
</ul></li>
<li><p>La <em>frequenza cumulata</em> <span class="math inline">\(N_i\)</span>, ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla <span class="math inline">\(i\)</span>-esima compresa: <span class="math inline">\(N_i = \sum_{i=1}^m n_i.\)</span></p></li>
<li><p>La <em>frequenza cumulata relativa</em> <span class="math inline">\(F_i\)</span>, ovvero
<span class="math inline">\(F_i = f_1+f_2+\dots+f_m = \frac{N_i}{n} = \frac{1}{n} \sum_{i=1}^m f_i.\)</span></p></li>
</ul>
<div class="exercise">
<p><span id="exr:unlabeled-div-10" class="exercise"><strong>(#exr:unlabeled-div-10) </strong></span>Si calcoli la distribuzione di frequenza assoluta e la distribuzione di frequenza relativa per i valori del BDI-II del campione clinico di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Per costruire una distribuzione di frequenza è innanzitutto necessario scegliere gli intervalli delle classi. Facendo riferimento ai cut-off usati per l’interpretazione del BDI-II, definiamo i seguenti <em>intervalli aperti a destra</em>:</p>
<ul>
<li>depressione minima: [0, 13.5),</li>
<li>depressione lieve: [13.5, 19.5),</li>
<li>depressione moderata: [19.5, 28.5),</li>
<li>depressione severa: [28.5, 63).</li>
</ul>
<p>Esaminando i dati, possiamo notare che 36 soggetti cadono nella prima classe, uno nella seconda classe, e così via. La distribuzione di frequenza della variabile <code>bdi2</code> è riportata nella tabella seguente. Questa distribuzione di frequenza ci aiuta a capire meglio cosa sta succedendo. Se consideriamo la frequenza relativa, ad esempio, possiamo notare che ci sono due valori maggiormente ricorrenti e tali valori corrispondono alle due classi più estreme. Questo ha senso nel caso presente, in quanto il campione esaminato da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> includeva due gruppi di soggetti: soggetti sani (con valori BDI-II bassi) e soggetti depressi (con valori BDI-II alti).<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> In una distribuzione di frequenza tali valori tipici vanno sotto il nome di <em>mode</em> della distribuzione.</p>
<table>
<thead>
<tr class="header">
<th align="center">Lim. classi</th>
<th align="center">Fr. ass.</th>
<th align="center">Fr. rel.</th>
<th align="center">Fr. ass. cum.</th>
<th align="center">Fr. rel. cum.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\([0, 13.5)\)</span></td>
<td align="center">36</td>
<td align="center">36/66</td>
<td align="center">36</td>
<td align="center">36/66</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\([13.5, 19.5)\)</span></td>
<td align="center">1</td>
<td align="center">1/66</td>
<td align="center">37</td>
<td align="center">37/66</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\([19.5, 28.5)\)</span></td>
<td align="center">12</td>
<td align="center">12/66</td>
<td align="center">49</td>
<td align="center">49/66</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\([28.5, 63)\)</span></td>
<td align="center">17</td>
<td align="center">17/66</td>
<td align="center">66</td>
<td align="center">66/66</td>
</tr>
</tbody>
</table>
<p>Poniamoci ora il problema di costruire la tabella precedente utilizzando . Usando la funzione <code>cut()</code>, dividiamo il <em>campo di variazione</em> (ovvero, la differenza tra il valore massimo di una distribuzione ed il valore minimo) di una variabile continua <code>x</code> in intervalli e codifica ciascun valore <code>x</code> nei termini dell’intervallo a cui appartiene. Così facendo otteniamo:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>bysubj<span class="sc">$</span>bdi_level <span class="ot">&lt;-</span> <span class="fu">cut</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  bysubj<span class="sc">$</span>bdi,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">13.5</span>, <span class="fl">19.5</span>, <span class="fl">28.5</span>, <span class="dv">63</span>),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">include.lowest =</span> <span class="cn">TRUE</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;minimal&quot;</span>, <span class="st">&quot;mild&quot;</span>, <span class="st">&quot;moderate&quot;</span>, <span class="st">&quot;severe&quot;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>bysubj<span class="sc">$</span>bdi_level</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] moderate severe   severe   moderate severe  </span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [6] severe   severe   severe   moderate severe  </span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [11] moderate mild     severe   minimal  minimal </span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [16] minimal  severe   moderate minimal  minimal </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [21] minimal  minimal  minimal  moderate minimal </span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [26] minimal  minimal  minimal  minimal  minimal </span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [31] minimal  severe   minimal  minimal  severe  </span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [36] minimal  moderate minimal  minimal  minimal </span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [41] severe   minimal  minimal  severe   severe  </span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [46] moderate severe   severe   minimal  moderate</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [51] minimal  moderate severe   moderate moderate</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [56] minimal  minimal  minimal  minimal  minimal </span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [61] minimal  minimal  minimal  minimal  minimal </span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [66] minimal </span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Levels: minimal mild moderate severe</span></span></code></pre></div>
<p>
Possiamo ora usare la funzione <code>table()</code> la quale ritorna un elenco che associa la frequenza assoluta a ciascuna modalità della variabile – ovvero, ritorna la distribuzione di frequenza assoluta.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(bysubj<span class="sc">$</span>bdi_level)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  minimal     mild moderate   severe </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       36        1       12       17</span></span></code></pre></div>
<p>
La distribuzione di frequenza relativa si ottiene dividendo ciascuna frequenza assoluta per il numero totale di osservazioni:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(bysubj<span class="sc">$</span>bdi_level) <span class="sc">/</span> <span class="fu">sum</span>(<span class="fu">table</span>(bysubj<span class="sc">$</span>bdi_level))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  minimal     mild moderate   severe </span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.54545  0.01515  0.18182  0.25758</span></span></code></pre></div>
<table>
<thead>
<tr class="header">
<th>Limiti delle classi</th>
<th>Frequenza assoluta</th>
<th>Frequenza relativa</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[0, 13.5)</td>
<td>36</td>
<td>36/66</td>
</tr>
<tr class="even">
<td>[13.5, 19.5)</td>
<td>1</td>
<td>1/66</td>
</tr>
<tr class="odd">
<td>[19.5, 28.5)</td>
<td>12</td>
<td>12/66</td>
</tr>
<tr class="even">
<td>[28.5, 63]</td>
<td>17</td>
<td>17/66</td>
</tr>
</tbody>
</table>
</div>
<p>Insiemi di variabili possono anche avere distribuzioni di frequenze, dette distribuzioni congiunte. La distribuzione congiunta di un insieme di variabili <span class="math inline">\(V\)</span> è l’insieme delle frequenze di ogni possibile combinazione di valori delle variabili in <span class="math inline">\(V\)</span>. Ad esempio, se <span class="math inline">\(V\)</span> è un insieme di due variabili, <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, ciascuna delle quali può assumere due valori, 1 e 2, allora una possibile distribuzione congiunta di frequenze relative per <span class="math inline">\(V\)</span> è <span class="math inline">\(f(X = 1, Y = 1) = 0.2\)</span>, <span class="math inline">\(f(X = 1, Y = 2) = 0.1\)</span>, <span class="math inline">\(f(X = 2, Y = 1) = 0.5\)</span>, <span class="math inline">\(f(X = 2, Y = 2) = 0.2\)</span>. Proprio come con le distribuzioni di frequenze relative di una singola variabile, le frequenze relative di una distribuzione congiunta devono sommare a 1.</p>
</div>
<div id="istogramma" class="section level2 hasAnchor" number="3.6">
<h2 class="hasAnchor"><span class="header-section-number">3.6</span> Istogramma<a href="#istogramma" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I dati che sono stati sintetizzati in una distribuzione di frequenze
possono essere rappresentati graficamente in un istogramma.
Un istogramma si costruisce riportando sulle ascisse i limiti delle
classi <span class="math inline">\(\Delta_i\)</span> e sulle ordinate i valori della funzione costante a
tratti</p>
<p><span class="math display">\[
\varphi_n(x)= \frac{f_i}{b_i-a_i}, \quad x\in \Delta_i,\, i=1, \dots, m
\]</span>
che misura la <em>densità della frequenza relativa</em> della variabile <span class="math inline">\(X\)</span>
nella classe <span class="math inline">\(\Delta_i\)</span>, ovvero il rapporto fra la frequenza relativa
<span class="math inline">\(f_i\)</span> e l’ampiezza (<span class="math inline">\(b_i - a_i\)</span>) della classe. In questo modo il
rettangolo dell’istogramma associato alla classe <span class="math inline">\(\Delta_i\)</span> avrà un’area
proporzionale alla frequenza relativa <span class="math inline">\(f_i\)</span>. Si noti che l’area totale
dell’istogramma delle frequenze relative è data della somma delle aree
dei singoli rettangoli e quindi vale 1.0.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-11" class="exercise"><strong>(#exr:unlabeled-div-11) </strong></span>Si utilizzi <span class="math inline">\(\R\)</span> per costruire un istogramma per i valori BDI-II riportati da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Con i quattro intervalli individuati dai cut-off del BDI-II otteniamo la
rappresentazione riportata nella figura @ref(fig:hist1zetsche). Per chiarezza, precisiamo che <code>ggplot()</code> utilizza intervalli aperti a destra. Nel caso della prima barra dell’istogramma, l’ampiezza dell’intervallo è pari a 13.5 e l’area della barra (ovvero, la frequenza relativa) è uguale a 36/66. Dunque l’altezza della barra è uguale a <span class="math inline">\((36 / 66) / 13.5 = 0.040\)</span>. Lo stesso procedimento si applica per il calcolo dell’altezza degli altri rettangoli.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bdi)) <span class="sc">+</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> ..density..),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">13.5</span>, <span class="fl">19.5</span>, <span class="fl">28.5</span>, <span class="fl">44.1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># il valore BDI-II massimo è 44</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">13.5</span>, <span class="fl">19.5</span>, <span class="fl">28.5</span>, <span class="fl">44.1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;BDI-II&quot;</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità di frequenza&quot;</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/hist1zetsche-1.png" alt="Istogramma per i valori BDI-II riportati da Zetsche et al. (2019)." width="576" />
<p class="caption">
(#fig:hist1zetsche)Istogramma per i valori BDI-II riportati da Zetsche et al. (2019).
</p>
</div>
<p>Anche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, in generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale. Questo è il caso dell’istogramma della figura @ref(fig:hist2zetsche).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="sc">%&gt;%</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bdi)) <span class="sc">+</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> ..density..),</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">44.1</span>, <span class="at">length.out =</span> <span class="dv">7</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="fl">0.00</span>, <span class="fl">7.35</span>, <span class="fl">14.70</span>, <span class="fl">22.05</span>, <span class="fl">29.40</span>, <span class="fl">36.75</span>, <span class="fl">44.10</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;BDI-II&quot;</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità di frequanza&quot;</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/hist2zetsche-1.png" alt="Una rappresentazione più comune per l'istogramma dei valori BDI-II nella quale gli intervalli delle classi hanno ampiezze uguali." width="576" />
<p class="caption">
(#fig:hist2zetsche)Una rappresentazione più comune per l’istogramma dei valori BDI-II nella quale gli intervalli delle classi hanno ampiezze uguali.
</p>
</div>
</div>
</div>
<div id="kernel-density-plot" class="section level2 hasAnchor" number="3.7">
<h2 class="hasAnchor"><span class="header-section-number">3.7</span> Kernel density plot<a href="#kernel-density-plot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Il confronto tra le figure @ref(fig:hist1zetsche) e @ref(fig:hist2zetsche) rende chiaro il limite dell’istogramma: il profilo dell’istogramma è arbitrario, in quanto dipende dal numero e dall’ampiezza delle classi. Questo rende difficile l’interpretazione.</p>
<p>Il problema precedente può essere alleviato utilizzando una rappresentazione alternativa della distribuzione di frequenza, ovvero la stima della densità della frequenza dei dati (detta anche stima <em>kernel di densità</em>). Un modo semplice per pensare a tale rappresentazione, che in inglese va sotto il nome di <em>kernel density plot</em> (cioè i grafici basati sulla stima kernel di densità), è quello di immaginare un grande campione di dati, in modo che diventi possibile definire un enorme numero di classi di equivalenza di ampiezza molto piccola, le quali non risultino vuote. In tali circostanze, la funzione di densità empirica non è altro che il profilo <em>lisciato</em> dell’istogramma. La stessa idea si applica anche quando il campione è piccolo. In tali circostanze, invece di raccogliere le osservazioni in barre come negli istogrammi, lo stimatore di densità kernel colloca una piccola “gobba” (<em>bump</em>), determinata da un fattore <span class="math inline">\(K\)</span> (kernel) e da un parametro <span class="math inline">\(h\)</span> di smussamento detto ampiezza di banda (<em>bandwidth</em>), in corrispondenza di ogni osservazione, quindi somma le gobbe risultanti generando una curva smussata.</p>
<p>L’interpretazione che possiamo attribuire al kernel density plot è simile a quella che viene assegnata agli istogrammi: l’area sottesa al kernel density plot in un certo intervallo rappresenta la proporzione di casi della distribuzione che hanno valori compresi in quell’intervallo.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-12" class="exercise"><strong>(#exr:unlabeled-div-12) </strong></span>All’istogramma dei valori BDI-II di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> si sovrapponga un kernel density plot.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="sc">%&gt;%</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bdi)) <span class="sc">+</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> ..density..),</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">44.1</span>, <span class="at">length.out =</span> <span class="dv">7</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> bdi),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">adjust =</span> <span class="fl">0.5</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">0.8</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#fill = colors[2],</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.5</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;BDI-II&quot;</span>,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità di frequenza&quot;</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/zetschehist3-1.png" alt="Kernel density plot e corrispondente istogramma per i valori BDI-II." width="576" />
<p class="caption">
(#fig:zetschehist3)Kernel density plot e corrispondente istogramma per i valori BDI-II.
</p>
</div>
</div>
</div>
<div id="forma-di-una-distribuzione" class="section level2 hasAnchor" number="3.8">
<h2 class="hasAnchor"><span class="header-section-number">3.8</span> Forma di una distribuzione<a href="#forma-di-una-distribuzione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In generale, la forma di una distribuzione descrive come i dati si distribuiscono intorno ai valori centrali. Distinguiamo tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali o multimodali. Un’illustrazione grafica è fornita nella figura @ref(fig:distrib-shapes). Nel pannello 1 la distribuzione è unimodale con asimmetria negativa; nel pannello 2 la distribuzione è unimodale con asimmetria positiva; nel pannello 3 la distribuzione è simmetrica e unimodale; nel pannello 4 la distribuzione è bimodale.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/distrib-shapes-1.png" alt="1: Asimmetria negativa. 2: Asimmetria positiva. 3: Distribuzione unimodale. 4: Distribuzione bimodale." width="576" />
<p class="caption">
(#fig:distrib-shapes)1: Asimmetria negativa. 2: Asimmetria positiva. 3: Distribuzione unimodale. 4: Distribuzione bimodale.
</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-13" class="exercise"><strong>(#exr:unlabeled-div-13) </strong></span>Il kernel density plot della figura @ref(fig:zetschehist3) indica che la distribuzione dei valori del BDI-II nel campione di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> è bimodale. Ciò indica che le osservazioni della distribuzione si addensano in due cluster ben distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
</div>
</div>
<div id="indici-di-posizione" class="section level2 hasAnchor" number="3.9">
<h2 class="hasAnchor"><span class="header-section-number">3.9</span> Indici di posizione<a href="#indici-di-posizione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Nuovamente, se preferite un’introduzione “soft” alla nozione di “tendenza centrale” di una distribuzione statistica, vi rimando nuovamentew al <a href="https://tinystats.github.io/teacups-giraffes-and-statistics/03_mean.html">link</a> che ho già suggerito in precedenza.</p>
<div id="quantili" class="section level3 hasAnchor" number="3.9.1">
<h3 class="hasAnchor"><span class="header-section-number">3.9.1</span> Quantili<a href="#quantili" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La descrizione della distribuzione dei valori BDI-II di
<span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> può essere facilitata dalla determinazione di
alcuni valori caratteristici che sintetizzano le informazioni contenute
nella distribuzione di frequenze. Si dicono <em>quantili</em> (o <em>frattili</em>)
quei valori caratteristici che hanno le seguenti proprietà. I <em>quartili</em>
sono quei valori che ripartiscono i dati <span class="math inline">\(x_i\)</span> in quattro parti
ugualmente numerose (pari ciascuna al 25% del totale). Il primo
quartile, <span class="math inline">\(q_1\)</span>, lascia alla sua sinistra il 25% del campione pensato
come una fila ordinata (a destra quindi il 75%). Il secondo quartile
<span class="math inline">\(q_2\)</span> lascia a sinistra il 50% del campione (a destra quindi il 50%).
Esso viene anche chiamato <em>mediana</em>. Il terzo quartile lascia a sinistra
il 75% del campione (a destra quindi il 25%). Secondo lo stesso
criterio, si dicono <em>decili</em> i quantili di ordine <span class="math inline">\(p\)</span> multiplo di 0.10 e
<em>percentili</em> i quantili di ordine <span class="math inline">\(p\)</span> multiplo di 0.01.</p>
<p>Come si calcolano i quantili? Consideriamo la definizione di quantile <em>non interpolato</em> di ordine <span class="math inline">\(p\)</span> <span class="math inline">\((0 &lt; p &lt; 1)\)</span>. Si procede innanzitutto
ordinando i dati in ordine crescente, <span class="math inline">\(\{x_1, x_2, \dots, x_n\}\)</span>. Ci
sono poi due possibilità. Se il valore <span class="math inline">\(np\)</span> non è intero, sia <span class="math inline">\(k\)</span>
l’intero tale che <span class="math inline">\(k &lt; np &lt; k + 1\)</span> – ovvero, la parte intera di <span class="math inline">\(np\)</span>.
Allora <span class="math inline">\(q_p = x_{k+1}.\)</span> Se <span class="math inline">\(np = k\)</span> con <span class="math inline">\(k\)</span> intero, allora
<span class="math inline">\(q_p = \frac{1}{2}(x_{k} + x_{k+1}).\)</span> Se vogliamo calcolare il primo
quartile <span class="math inline">\(q_1\)</span>, ad esempio, utilizziamo <span class="math inline">\(p = 0.25\)</span>. Dovendo calcolare
gli altri quantili basta sostituire a <span class="math inline">\(p\)</span> il valore appropriato.</p>
<p>Gli indici di posizione, tra le altre cose, hanno un ruolo importante,
ovvero vengono utilizzati per creare una rappresentazione grafica di una
distribuzione di valori che è molto popolare e può essere usata in
alternativa ad un istogramma (in realtà vedremo poi come possa essere
combinata con un istogramma). Tale rappresentazione va sotto il nome di
box-plot.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-14" class="exercise"><strong>(#exr:unlabeled-div-14) </strong></span>Per fare un esempio, consideriamo i nove soggetti del campione clinico di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> che hanno riportato un unico episodio di depressione maggiore. Per tali soggetti i valori ordinati del BDI-II (per semplicità li chiameremo <span class="math inline">\(x\)</span>) sono i seguenti: 19, 26, 27, 28, 28, 33, 33, 41, 43.
Per il calcolo del secondo quartile (non interpolato), ovvero per il calcolo della mediana, dobbiamo considerare la quantità <span class="math inline">\(np = 9 \cdot 0.5 = 4.5\)</span>, non intero. Quindi, <span class="math inline">\(q_1 = x_{4 + 1} = 27\)</span>.
Per il calcolo del quantile (non interpolato) di ordine <span class="math inline">\(p = 2/3\)</span> dobbiamo considerare la quantità <span class="math inline">\(np = 9 \cdot 2/3 = 6\)</span>, intero. Quindi, <span class="math inline">\(q_{\frac{2}{3}} = \frac{1}{2} (x_{6} + x_{7}) = \frac{1}{2} (33 + 33) = 33\)</span>.</p>
</div>
</div>
<div id="diagramma-a-scatola" class="section level3 hasAnchor" number="3.9.2">
<h3 class="hasAnchor"><span class="header-section-number">3.9.2</span> Diagramma a scatola<a href="#diagramma-a-scatola" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il <em>diagramma a scatola</em> (o box plot) è uno strumento grafico utile al
fine di ottenere informazioni circa la dispersione e l’eventuale
simmetria o asimmetria di una distribuzione. Per costruire un box-plot
si rappresenta sul piano cartesiano un rettangolo (cioè la “scatola”) di
altezza arbitraria la cui base corrisponde alla dist intanza
interquartile (IQR = <span class="math inline">\(q_{0.75} - q_{0.25}\)</span>). La linea interna alla
scatola rappresenta la mediana <span class="math inline">\(q_{0.5}\)</span>. Si tracciano poi ai lati della
scatola due segmenti di retta i cui estremi sono detti “valore
adiacente” inferiore e superiore. Il valore adiacente inferiore è il
valore più piccolo tra le osservazioni che risulta maggiore o uguale al
primo quartile meno la distanza corrispondente a 1.5 volte la distanza
interquartile. Il valore adiacente superiore è il valore più grande tra le osservazioni che risulta minore o uguale a <span class="math inline">\(Q_3+1.5\)</span> IQR. I valori esterni ai valori adiacenti (chiamati <em>valori anomali</em>) vengono rappresentati individualmente nel box-plot per meglio evidenziarne la presenza e la posizione.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-10-1.png" alt="Box-plot: $M$ è la mediana, $\bar{x}$ è la media aritmetica e IQR è la distanza interquartile (~$Q_3 - Q_1$~)." width="576" />
<p class="caption">
(#fig:unnamed-chunk-10)Box-plot: <span class="math inline">\(M\)</span> è la mediana, <span class="math inline">\(\bar{x}\)</span> è la media aritmetica e IQR è la distanza interquartile (<sub><span class="math inline">\(Q_3 - Q_1\)</span></sub>).
</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-15" class="exercise"><strong>(#exr:unlabeled-div-15) </strong></span>Per i dati di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>, si utilizzi un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.</p>
<p>Nella figura @ref(fig:violin-zetsche) sinistra sono rappresentati i dati grezzi. La linea curva che circonda (simmetricamente) le osservazioni è l’<em>istogramma lisciato</em> (kernel density plot) che abbiamo descritto in precedenza. Nella figura @ref(fig:violin-zetsche) destra sono rappresentanti gli stessi dati: il kernel density plot è lo stesso di prima, ma al suo interno è stato collocato un box-plot. Entrambe le rappresentazioni suggeriscono che la distribuzione dei dati è all’incirca simmetrica nel gruppo clinico. Il gruppo di controllo mostra invece un’asimmetria positiva.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(esm_id, group) <span class="sc">%&gt;%</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi =</span> <span class="fu">mean</span>(bdi),</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">nr_of_episodes =</span> <span class="fu">mean</span>(nr_of_episodes, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() <span class="sc">%&gt;%</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>bysubj<span class="sc">$</span>group <span class="ot">&lt;-</span> forcats<span class="sc">::</span><span class="fu">fct_recode</span>(</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>  bysubj<span class="sc">$</span>group,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;Controlli</span><span class="sc">\n</span><span class="st"> sani&quot;</span> <span class="ot">=</span> <span class="st">&quot;ctl&quot;</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;Depressione</span><span class="sc">\n</span><span class="st"> maggiore&quot;</span> <span class="ot">=</span> <span class="st">&quot;mdd&quot;</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> bysubj <span class="sc">%&gt;%</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> bdi)) <span class="sc">+</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>(<span class="at">trim =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_dotplot</span>(<span class="at">binaxis =</span> <span class="st">&quot;y&quot;</span>, <span class="at">stackdir =</span> <span class="st">&quot;center&quot;</span>, <span class="at">dotsize =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;BDI-II&quot;</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> bysubj <span class="sc">%&gt;%</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> bdi)) <span class="sc">+</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>(<span class="at">trim =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">width =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;BDI-II&quot;</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/violin-zetsche-1.png" alt="Due versioni di un violin plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019)." width="576" />
<p class="caption">
(#fig:violin-zetsche)Due versioni di un violin plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019).
</p>
</div>
</div>
</div>
<div id="sina-plot" class="section level3 hasAnchor" number="3.9.3">
<h3 class="hasAnchor"><span class="header-section-number">3.9.3</span> Sina plot<a href="#sina-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si noti che i box plot non sono necessariamente la rappresentazione migliore della distribuzione di una variabile. Infatti, richiedono la comprensione di concetti complessi (quali i quantili e la differenza interquantile) che non sono necessari se vogliamo presentare in maniera grafica la distribuzione della variabile e, in generale, non sono compresi da un pubblico di non specialisti. Inoltre, i box plot nascondono informazioni che di solito sono cruciali da vedere. È dunque preferibile presentare direttamente i dati.</p>
<p>Nella figura @ref(fig:sina-zetsche) viene presentato un cosiddetto “sina plot”. In tale rappresentazione grafica vengono mostrate le singole osservazioni divise in classi. Ai punti viene aggiunto un jitter, così da evitare sovrapposizioni. L’ampiezza del jitter lungo l’asse <span class="math inline">\(x\)</span> è determinata dalla distribuzione della densità dei dati all’interno di ciascuna classe; quindi il grafico mostra lo stesso contorno di un <em>violin plot</em>, ma trasmette informazioni sia sul numero di punti dati, sia sulla distribuzione della densità, sui valori anomali e sulla distribuzione dei dati in un formato molto semplice, comprensibile e sintetico.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-16" class="exercise"><strong>(#exr:unlabeled-div-16) </strong></span>Si generi un sina plot per i dati della figura @ref(fig:violin-zetsche). Si aggiunga alla figura una rappresentazione della mediana.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>zetsche_summary <span class="ot">&lt;-</span> bysubj <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(group) <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi_mean =</span> <span class="fu">mean</span>(bdi),</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi_sd =</span> <span class="fu">sd</span>(bdi),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi_median =</span> <span class="fu">median</span>(bdi)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>bysubj <span class="sc">%&gt;%</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> group, <span class="at">y =</span> bdi, <span class="at">color =</span> group)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  ggforce<span class="sc">::</span><span class="fu">geom_sina</span>(<span class="fu">aes</span>(<span class="at">color =</span> group, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> .<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> bdi_median, <span class="at">ymin =</span> bdi_median, <span class="at">ymax =</span> bdi_median),</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> zetsche_summary, <span class="at">width =</span> <span class="fl">0.3</span>, <span class="at">size =</span> <span class="dv">3</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;BDI-II&quot;</span>,</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;Gruppo&quot;</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_grey</span>(<span class="at">start =</span> <span class="fl">0.7</span>, <span class="at">end =</span> <span class="dv">0</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/sina-zetsche-1.png" alt="Sina plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019) con l'indicazione della mediana per ciascun gruppo." width="576" />
<p class="caption">
(#fig:sina-zetsche)Sina plot per i valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019) con l’indicazione della mediana per ciascun gruppo.
</p>
</div>
<p>Per un esempio in una recente pubblicazione, possiamo considerare le figure 3 e 6 di <span class="citation">Lazic, Semenova, and Williams (<a href="#ref-lazic2020determining" role="doc-biblioref">2020</a>)</span>.</p>
</div>
</div>
<div id="leccellenza-grafica" class="section level3 hasAnchor" number="3.9.4">
<h3 class="hasAnchor"><span class="header-section-number">3.9.4</span> L’eccellenza grafica<a href="#leccellenza-grafica" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Non c’è un unico modo “corretto” per la rappresentazione grafica dei dati. Ciascuno dei grafici che abbiamo discusso in precedenza ha i suoi pregi e i suoi difetti. Un ricercatore che ha molto influenzato il modo in cui viene realizzata la visualizzazione dei dati scientifici è Edward Tufte, soprannominato dal New York Times il “Leonardo da Vinci dei dati.” Secondo Tufte, “l’eccellenza nella grafica consiste nel comunicare idee complesse in modo chiaro, preciso ed efficiente”. Nella visualizzazione delle informazioni, l’“eccellenza grafica” ha l’obiettivo di comunicare al lettore il maggior numero di idee nella maniera più diretta e semplice possibile. Secondo <span class="citation">Tufte (<a href="#ref-tufte_visual_display" role="doc-biblioref">2001</a>)</span>, le rappresentazioni grafiche dovrebbero:</p>
<ol style="list-style-type: decimal">
<li>mostrare i dati;</li>
<li>indurre l’osservatore a riflettere sulla sostanza piuttosto che
sulla progettazione grafica, o qualcos’altro;</li>
<li>evitare di distorcere quanto i dati stanno comunicando (“integrità
grafica”);</li>
<li>presentare molte informazioni in forma succinta;</li>
<li>rivelare la coerenza tra le molte dimensioni dei dati;</li>
<li>incoraggiare l’osservatore a confrontare differenti sottoinsiemi di dati;</li>
<li>rivelare i dati a diversi livelli di dettaglio, da una visione ampia
alla struttura di base;</li>
<li>servire ad uno scopo preciso (descrizione, esplorazione, o la
risposta a qualche domanda);</li>
<li>essere fortemente integrate con le descrizioni statistiche e verbali
dei dati fornite nel testo.</li>
</ol>
<p>In base a questi principi, figura @ref(fig:sina-zetsche) sembra fornire la
rappresentazione migliore dei dati di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>. Il seguente <a href="https://www.biostat.wisc.edu/~kbroman/presentations/graphs2018.pdf">link</a> fornisce diverse interessanti illustrazioni dei principi elencati sopra.</p>
</div>
</div>
<div id="commenti-e-considerazioni-finali-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una distribuzione è una rappresentazione del modo in cui le diverse modalità di una variabile <span class="math inline">\(X\)</span> si distribuiscono nelle unità statistiche che compongono il campione o la popolazione oggetto di studio. Il modo più diretto per trasmettere descrivere le proprietà della distribuzione di una variabile discreta è quello di fornire una rappresentazione grafica della distribuzione di frequenza. In seguito vedremo la corrispondente rappresentazione che viene usata nel caso delle variabili continue.</p>
<!--chapter:end:010_freq_distr.Rmd-->
</div>
</div>
<div id="ch:loc-scale" class="section level1 hasAnchor" number="4">
<h1 class="hasAnchor"><span class="header-section-number">4</span> Indici di posizione e di scala<a href="#ch:loc-scale" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>L’analisi grafica, esaminata in precedenza, costituisce la base di partenza di qualsivoglia analisi quantitativa dei dati. Tramite opportune rappresentazioni grafiche possiamo individuare alcune caratteristiche importanti di una distribuzione: per esempio, è possibile capire se la distribuzione è simmetrica o asimmetrica; oppure se è unimodale o multimodale. Successivamente, possiamo calcolare degli indici numerici che descrivono in modo sintetico le caratteristiche di base dei dati esaminati.</p>
<div id="indici-di-tendenza-centrale" class="section level2 hasAnchor" number="4.1">
<h2 class="hasAnchor"><span class="header-section-number">4.1</span> Indici di tendenza centrale<a href="#indici-di-tendenza-centrale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tra le misure di tendenza centrale, ovvero tra gli indici che forniscono un’idea dei valori attorno ai quali sono prevalentemente concentrati i dati di un campione, quella più comunemente usata è la media.</p>
<div id="media" class="section level3 hasAnchor" number="4.1.1">
<h3 class="hasAnchor"><span class="header-section-number">4.1.1</span> Media<a href="#media" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tutti conosciamo la media aritmetica di <span class="math inline">\(\{x_1, x_2, \dots, x_n\}\)</span>, ovvero il numero reale <span class="math inline">\(\bar{x}\)</span> definito da</p>
<p><span class="math display">\[\begin{equation}
\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i.
(\#eq:mean)
\end{equation}\]</span></p>
<p>Nella @ref(eq:mean) abbiamo usato la notazione delle sommatorie per descrivere una somma di valori. Questa notazione è molto usata in statistica e viene descritta in Appendice.</p>
<p>La media gode della seguente importante proprietà: la somma degli scarti tra ciascuna modalità <span class="math inline">\(x_i\)</span> e la media aritmetica <span class="math inline">\(\bar{x}\)</span> è nulla, cioè</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^n (x_i - \bar{x}) = 0.\notag
\label{eq:diffmeansumzero}
\end{equation}\]</span></p>
<p>Infatti,</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (x_i - \bar{x}) &amp;= \sum_i x_i - \sum_i \bar{x}\notag\\
&amp;= \sum_i x_i - n \bar{x}\notag\\
&amp;= \sum_i x_i - \sum_i x_i = 0.\notag
\end{aligned}
\]</span></p>
<p>Ciò ci consente di pensare alla media come al baricentro della distribuzione.</p>
<p>Un’altra proprietà della media è la seguente. La somma dei quadrati degli scarti tra ciascuna modalità <span class="math inline">\(x_i\)</span> e una costante arbitraria <span class="math inline">\(a\)</span>, cioè</p>
<p><span class="math display">\[\begin{equation}
\varphi(a) = \sum_{i=1}^n (x_i - a)^2,\notag
\end{equation}\]</span></p>
<p>è minima per <span class="math inline">\(a = \bar{x}\)</span>.</p>
<div class="remark">
<p><span id="unlabeled-div-17" class="remark"><em>Remark</em>. </span>Il concetto statistico di media ha suscitato molte battute. Per esempio,
il fatto che, in media, ciascuno di noi ha un numero di gambe circa pari
a 1.9999999. Oppure, il fatto che, in media, ciascuno di noi ha un
testicolo. Ma la media ha altri problemi, oltre al fatto di ispirare
battute simili alle precedenti. In particolare, dobbiamo notare che la
media non è sempre l’indice che meglio rappresenta la tendenza centrale
di una distribuzione. In particolare, ciò non accade quando la
distribuzione è asimmetrica, o in presenza di valori anomali (<em>outlier</em>)
– si veda il pannello di destra della figura @ref(fig:violin-zetsche). In tali circostanze, la tendenza centrale della distribuzione è meglio rappresentata dalla mediana o dalla media spuntata.</p>
</div>
<!-- ::: {.exercise} -->
<!-- Si calcoli la media dei valori BDI-II per i due gruppi di soggetti di @zetschefuture2019. -->
<!-- ```{r} -->
<!-- bysubj %>%  -->
<!--   group_by(group) %>%  -->
<!--   summarise( -->
<!--     avg_bdi = mean(bdi) -->
<!--   ) -->
<!-- ``` -->
<!-- ::: -->
</div>
<div id="media-spuntata" class="section level3 hasAnchor" number="4.1.2">
<h3 class="hasAnchor"><span class="header-section-number">4.1.2</span> Media spuntata<a href="#media-spuntata" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <em>media spuntata</em> <span class="math inline">\(\bar{x}_t\)</span> (<em>trimmed mean</em>) non è altro che la
media dei dati calcolata considerando solo il 90% (o altra percentuale)
dei dati centrali. Per calcolare <span class="math inline">\(\bar{x}_t\)</span> si ordinando i dati secondo
una sequenza crescente, <span class="math inline">\(x_1 \leq x_2 \leq x_3 \leq \dots \leq x_n\)</span>, per
poi eliminare il primo 5% e l’ultimo 5% dei dati della serie così
ordinata. La media spuntata è data dalla media aritmetica dei dati rimanenti.</p>
<!-- ::: {.exercise} -->
<!-- Si calcoli la media spuntata dei valori BDI-II per i due gruppi di soggetti di @zetschefuture2019 escludendo il 10% dei valori più estremi in ciascun gruppo. -->
<!-- ```{r} -->
<!-- bysubj %>% -->
<!--   group_by(group) %>% -->
<!--   summarise( -->
<!--     avg_trim_bdi = mean(bdi, trim = 0.1) -->
<!--   ) -->
<!-- ``` -->
<!-- ::: -->
</div>
<div id="moda-e-mediana" class="section level3 hasAnchor" number="4.1.3">
<h3 class="hasAnchor"><span class="header-section-number">4.1.3</span> Moda e mediana<a href="#moda-e-mediana" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In precedenza abbiamo già incontrato altri due popolari indici di
tendenza centrale: la <em>moda</em> (<em>Mo</em>), ovvero il valore centrale della
classe con la frequenza massima (può succedere che una distribuzione
abbia più mode; in tal caso si dice <em>multimodale</em> e questo operatore
perde il suo significato di indice di tendenza centrale) e la <em>mediana</em>
<span class="math inline">\(\tilde{x}\)</span>.</p>
<!-- ::: {.exercise} -->
<!-- Si calcolino i quantili di ordine 0.25, 0.5 e 0.75 dei valori BDI-II per i due gruppi di soggetti di @zetschefuture2019. -->
<!-- ```{r} -->
<!-- bysubj %>% -->
<!--   group_by(group) %>% -->
<!--   summarise( -->
<!--     q25 = quantile(bdi, probs = 0.25), -->
<!--     q50 = quantile(bdi, probs = 0.50), -->
<!--     q75 = quantile(bdi, probs = 0.75) -->
<!--   ) -->
<!-- ``` -->
<!-- ::: -->
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>Si noti che solitamente i software restituiscono un valore <em>interpolato</em> del <span class="math inline">\(p\)</span>-esimo quantile <span class="math inline">\(q_p\)</span> <span class="math inline">\((0 &lt; p &lt; 1)\)</span>, il quale viene calcolato mediante specifiche procedure. Il risultato fornito dai software, dunque, non sarà identico a quello trovato utilizzando la definizione non interpolata di quantile che abbiamo presentato qui. Se, per qualche ragione, vogliamo conoscere l’algoritmo usato per la determinazione dei quantili interpolati, dobbiamo leggere la documentazione del software.</p>
</div>
</div>
</div>
<div id="indici-di-dispersione" class="section level2 hasAnchor" number="4.2">
<h2 class="hasAnchor"><span class="header-section-number">4.2</span> Indici di dispersione<a href="#indici-di-dispersione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le medie e gli indici di posizione descritti in precedenza forniscono
delle sintesi dei dati che mettono in evidenza la tendenza centrale
delle osservazioni. Tali indici, tuttavia, non considerano un aspetto
importante della distribuzione dei dati, ovvero la variabilità dei
valori numerici della variabile statistica. È dunque necessario
sintetizzare la distribuzione di una variabile statistica oltre che con
le misure di posizione anche tramite l’utilizzo di indicatori che
valutino la dispersione delle unità statistice.</p>
<div class="remark">
<p><span id="unlabeled-div-19" class="remark"><em>Remark</em>. </span>Un’introduzione “soft” al tema degli indici di posizione è fornita nel seguente <a href="https://tinystats.github.io/teacups-giraffes-and-statistics/04_variance.html">link</a>.</p>
</div>
<div id="indici-basati-sullordinamento-dei-dati" class="section level3 hasAnchor" number="4.2.1">
<h3 class="hasAnchor"><span class="header-section-number">4.2.1</span> Indici basati sull’ordinamento dei dati<a href="#indici-basati-sullordinamento-dei-dati" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>È possibile calcolare degli indici di variabilità basati
sull’ordinamento dei dati. L’indice più ovvio è l’intervallo di
variazione, ovvero la distanza tra il valore massimo e il valore minimo
di una distribuzione di modalità, mentre in precedenza abbiamo già
incontrato la differenza interquartile. Questi due indici, però, hanno
il limite di essere calcolati sulla base di due soli valori della
distribuzione (<span class="math inline">\(x_{\text{max}}\)</span> e <span class="math inline">\(x_{\text{mini}}\)</span>, oppure <span class="math inline">\(x_{0.25}\)</span> e
<span class="math inline">\(x_{0.75}\)</span>). Pertanto non utilizzano tutte le informazioni che sono
disponibili. Inoltre, l’intervallo di variazione ha il limite di essere
pesantemente influenzato dalla presenza di valori anomali.</p>
</div>
<div id="varianza" class="section level3 hasAnchor" number="4.2.2">
<h3 class="hasAnchor"><span class="header-section-number">4.2.2</span> Varianza<a href="#varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dati i limiti delle statistiche precedenti è più comune misurare la variabilità di una variabile statistica come la dispersione dei dati attorno ad un indice di tendenza centrale. Infatti, la misura di variabilità di gran lunga più usata per valutare la variabilità di una variabile statistica è senza dubbio la varianza. La varianza</p>
<p><span class="math display">\[\begin{equation}
s^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
(\#eq:var-descr)
\end{equation}\]</span></p>
<p>è la media dei quadrati degli scarti <span class="math inline">\(x_i - \bar{x}\)</span> tra ogni valore e la media della distribuzione. La varianza è una misura di dispersione più complessa di quelle esaminate in precedenza. È appropriata solo nel caso di distribuzioni simmetriche e, anch’essa, è fortemente influenzata dai valori anomali. Inoltre, è espressa in un’unità di misura che è il quadrato dell’unità di misura dei dati originari e quindi ad essa non può essere assegnata un’interpretazione intuitiva.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-20" class="exercise"><strong>(#exr:unlabeled-div-20) </strong></span>Si calcoli la varianza dei valori BDI-II per i dati di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Applicando la formula precedente, per tutto il campione abbiamo</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(bysubj<span class="sc">$</span>bdi)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 241.9</span></span></code></pre></div>
</div>
</div>
<div id="precisione" class="section level3 hasAnchor" number="4.2.3">
<h3 class="hasAnchor"><span class="header-section-number">4.2.3</span> Precisione<a href="#precisione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si definisce <em>precisione</em> l’inverso della varianza:</p>
<p><span class="math display">\[\begin{equation}
\tau = \frac{1}{\sigma^2}.
(\#eq:precision)
\end{equation}\]</span></p>
<p>Alcuni ritengono che la precisione sia più “intuitiva” della varianza perché dice quanto sono concentrati i valori attorno alla media piuttosto che quanto sono dispersi. In altri termini, si potrebbe argomentare che siamo più interessati a quanto sia precisa una misurazione piuttosto che a quanto sia imprecisa. Più sono dispersi i valori attorno alla media (alta varianza), meno sono precisi (poca precisione); minore è la varianza, maggiore è la precisione.</p>
<p>La precisione è uno dei due parametri naturali della distribuzione gaussiana. Nei termini della @ref(eq:precision), la distribuzione gaussiana (si veda il Capitolo @ref(distr-rv-cont)) può essere espressa nel modo seguente</p>
<p><span class="math display">\[
{\displaystyle f(y)=\sqrt{\frac{\tau}{2\pi}} e^{-{\frac {1}{2}}\tau\left({y-\mu }\right)^{2}}},
\]</span>
anziché come</p>
<p><span class="math display">\[
{\displaystyle f(y)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {y-\mu }{\sigma }}\right)^{2}}}.
\]</span></p>
</div>
<div id="scarto-tipo" class="section level3 hasAnchor" number="4.2.4">
<h3 class="hasAnchor"><span class="header-section-number">4.2.4</span> Scarto tipo<a href="#scarto-tipo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Per le ragioni espresse sopra, la misura più usata della dispersione di una distribuzione di dati è lo <em>scarto quadratico medio</em> (o <em>scarto tipo</em>, o <em>deviazione standard</em>), ovvero la radice quadrata della varianza<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. A differenza della varianza, dunque, lo scarto tipo è espresso nella stessa unità di misura dei dati. Come nel caso della varianza, anche lo scarto tipo <span class="math inline">\(s\)</span> dovrebbe essere usato soltanto quando la media è adeguata per misurare il centro della distribuzione, ovvero, nel caso di distribuzioni simmetriche. Come nel caso della media <span class="math inline">\(\bar{x}\)</span>, anche lo scarto tipo è fortemente influenzato dai dati anomali (<em>outlier</em>), ovvero dalla presenza di uno o di pochi dati che sono molto più distanti dalla media rispetto agli altri valori della distribuzione. Quando tutte le osservazioni sono uguali, <span class="math inline">\(s = 0\)</span>, altrimenti <span class="math inline">\(s &gt; 0\)</span>.</p>
<p>Allo scarto tipo può essere assegnata una semplice interpretazione: lo scarto tipo è <em>simile</em> (ma non identico) allo scarto semplice medio campionario, ovvero alla media aritmetica dei valori assoluti degli scarti dalla media. Lo scarto tipo ci dice, dunque, quanto sono distanti, in media, le singole osservazioni dal centro della distribuzione. Un’interpretazione più precisa del significato dello scarto tipo è fornita nel Paragrafo successivo.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-21" class="exercise"><strong>(#exr:unlabeled-div-21) </strong></span>Si calcoli lo scarto tipo per i valori BDI-II di dati di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Applicando la formula precedente, per tutto il campione abbiamo</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(bysubj<span class="sc">$</span>bdi)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 15.55</span></span></code></pre></div>
</div>
</div>
<div id="deviazione-mediana-assoluta" class="section level3 hasAnchor" number="4.2.5">
<h3 class="hasAnchor"><span class="header-section-number">4.2.5</span> Deviazione mediana assoluta<a href="#deviazione-mediana-assoluta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una misura robusta della dispersione statistica di un campione è la deviazione mediana assoluta (<em>Median Absolute Deviation</em>, MAD) definita come la mediana del valore assoluto delle deviazioni dei dati dalla mediana, ovvero:</p>
<p><span class="math display">\[
{\displaystyle \operatorname {MAD} =\operatorname {median} \left(\ \left|X_{i}-\operatorname {median} (X)\right|\ \right)}
\]</span>
Nel caso di una distribuzione dei dati unimodale simmetrica di forma campanulare (ovvero, normale) si ha che</p>
<p><span class="math display">\[
{\displaystyle \text{deviazione standard} \approx 1.4826\ \operatorname {MAD} .\,}
\]</span>
Pertanto, solitamente i software restituiscono il valore MAD moltiplicato per una tale costante.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-22" class="exercise"><strong>(#exr:unlabeled-div-22) </strong></span>Si calcoli il valore MAD per i valori BDI-II riportati da <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Applicando la formula precedente, per tutto il campione abbiamo</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fl">1.4826</span> <span class="sc">*</span> <span class="fu">median</span>(<span class="fu">abs</span>(bysubj<span class="sc">$</span>bdi <span class="sc">-</span> <span class="fu">median</span>(bysubj<span class="sc">$</span>bdi)))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 15.57</span></span></code></pre></div>
</div>
</div>
<div id="indici-di-variabilità-relativi" class="section level3 hasAnchor" number="4.2.6">
<h3 class="hasAnchor"><span class="header-section-number">4.2.6</span> Indici di variabilità relativi<a href="#indici-di-variabilità-relativi" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A volte può essere interessante effettuare un confronto fra due misure di variabilità di grandezze incommensurabili, ovvero di caratteri rilevati mediante differenti unità di misura. In questi casi, le misure di variabilità precedentemente descritte si rivelano inadeguate in quanto dipendono dall’unità di misura adottata. Diventa dunque necessario ricorrere a particolari numeri adimensionali detti indici relativi di variabilità. Il più importante di tali indici è il coefficiente di variazione, ovvero il numero puro</p>
<p><span class="math display">\[
C_v = \frac{\sigma}{\bar{x}}
\]</span>
ottenuto dal rapporto tra la deviazione standard e la media dei dati. Un altro indice relativo di variabilità è la differenza interquartile rapportata al primo quartile oppure al terzo quartile oppure alla mediana, cioè:</p>
<p><span class="math display">\[
\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \qquad \frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \qquad \frac{x_{0.75} - x_{0.25}}{x_{0.50}}.
\]</span></p>
</div>
</div>
<div id="commenti-e-considerazioni-finali-2" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le statistiche descrittive ci forniscono degli indici sintetici che riassumono i dati, ovvero le nostre misurazioni dell’intera popolazione o di un campione estratto da una popolazione. Le statistiche descrittive comprendono gli indici di tendenza centrale e gli indici di dispersione. Gli indici di tendenza centrale includono la media, la mediana e la moda, mentre gli indici di dispersione includono lo scarto tipo, la varianza, la curtosi e l’asimmetria.</p>
<!--chapter:end:011_loc_scale.Rmd-->
</div>
</div>
<div id="ch:correlation" class="section level1 hasAnchor" number="5">
<h1 class="hasAnchor"><span class="header-section-number">5</span> Le relazioni tra variabili<a href="#ch:correlation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Nella loro ricerca, <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> hanno misurato il livello di depressione dei soggetti utilizzando due scale psicometriche: il Beck Depression Inventory II (BDI-II) e la Center for Epidemiologic Studies Depression Scale (CES-D). Il BDI-II è uno strumento self-report che valutare la presenza e l’intensità di sintomi depressivi in pazienti adulti e adolescenti di almeno 13 anni di età con diagnosi psichiatrica
mentre la CES-D è una scala self-report progettata per misurare i
sintomi depressivi che sono stati vissuti nella settimana precedente
nella popolazione generale, specialmente quella degli
adolescenti/giovani adulti. Una domanda ovvia che ci può venire in
mente è: quanto sono simili le misure ottenute mediante queste due
scale?</p>
<p>È chiaro che i numeri prodotti dalle scale BDI-II e CES-D non possono
essere identici, e questo per due motivi: (1) la presenza degli errori
di misurazione e (2) l’unità di misura delle due variabili. L’errore di
misurazione corrompe sempre, almeno in parte, qualunque operazione di
misurazione. E questo è vero specialmente in psicologia dove
l’<em>attendibilità</em> degli strumenti di misurazione è minore che in altre
discipline (quali la fisica, ad esempio). Il secondo motivo per cui i
valori delle scale BDI-II e CES-D non possono essere uguali è che
l’unità di misura delle due scale è arbitraria. Infatti, qual è l’unità
di misura della depressione? Chi può dirlo! Ma, al di là delle
differenze derivanti dall’errore di misurazione e dalla differente unità
di misura, ci aspettiamo che, se le due scale misurano entrambe lo
stesso costrutto, allora i valori prodotti dalle due scale dovranno
essere tra loro <em>linearmente associati</em>. Per capire cosa si intende con
“associazione lineare” iniziamo a guardare i dati. Per fare questo
utilizziamo una rappresentazione grafica che va sotto il nome di
diagramma a dispersione.</p>
<div id="diagramma-a-dispersione" class="section level3 hasAnchor" number="5.0.1">
<h3 class="hasAnchor"><span class="header-section-number">5.0.1</span> Diagramma a dispersione<a href="#diagramma-a-dispersione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il diagramma di dispersione è la rappresentazione grafica delle coppie di punti individuati da due variabili <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>Per fare un esempio concreto, consideriamo le variabili BDI-II e CES-D di <span class="citation">Zetsche, Bürkner, and Renneberg (<a href="#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>. Il diagramma di dispersione per tali variabili si ottiene ponendo, ad esempio, i valori BDI-II sull’asse delle ascisse e quelli del CES-D sull’asse delle ordinate. In tale grafico, fornito dalla figura @ref(fig:zetsche-scatter), cascun punto corrisponde ad un individuo del quale, nel caso presente, conosciamo il livello di depressione misurato dalle due scale psicometriche.</p>
<p>Dalla figura @ref(fig:zetsche-scatter) possiamo vedere che i dati mostrano una tendenza a disporsi attorno ad una retta – nel gergo statistico, questo fatto viene espresso dicendo che i punteggi CES-D tendono ad essere linearmente associati ai punteggi BDI-II. È ovvio, tuttavia, che tale relazione lineare è lungi dall’essere perfetta – se fosse perfetta, tutti i punti del diagramma a dispersione si disporrebbero esattamente lungo una retta.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>bysubj <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(esm_id, group) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">bdi =</span> <span class="fu">mean</span>(bdi),</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">cesd =</span> <span class="fu">mean</span>(cesd_sum)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>() <span class="sc">%&gt;%</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>m_cesd <span class="ot">&lt;-</span> bysubj <span class="sc">%&gt;%</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">pull</span>(cesd) <span class="sc">%&gt;%</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>m_bdi <span class="ot">&lt;-</span> bysubj <span class="sc">%&gt;%</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">pull</span>(bdi) <span class="sc">%&gt;%</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>FONT_SIZE <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>bysubj <span class="sc">%&gt;%</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> bdi, <span class="at">y =</span> cesd, <span class="at">color =</span> group)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> .<span class="dv">9</span>) <span class="sc">+</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> m_cesd, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, </span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>) <span class="sc">+</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> m_bdi, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, </span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>) <span class="sc">+</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">x =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">y =</span> <span class="dv">16</span>, <span class="at">label =</span> <span class="st">&quot;I&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>, </span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>            <span class="at">size =</span> FONT_SIZE) <span class="sc">+</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">46</span>, <span class="at">label =</span> <span class="st">&quot;IV&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>, </span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>            <span class="at">size =</span> FONT_SIZE) <span class="sc">+</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">x =</span> <span class="dv">18</span>, <span class="at">y =</span> <span class="dv">46</span>, <span class="at">label =</span> <span class="st">&quot;III&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>, </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>            <span class="at">size =</span> FONT_SIZE) <span class="sc">+</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">x =</span> <span class="dv">18</span>, <span class="at">y =</span> <span class="dv">16</span>, <span class="at">label =</span> <span class="st">&quot;II&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>, </span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>            <span class="at">size =</span> FONT_SIZE) <span class="sc">+</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;BDI-II&quot;</span>,</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;CESD&quot;</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_grey</span>(<span class="at">start =</span> <span class="fl">0.7</span>, <span class="at">end =</span> <span class="dv">0</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/zetsche-scatter-1.png" alt="Associazione tra le variabili BDI-II e CES-D nello studio di Zetsche et al. (2019). In grigio sono rappresentate le osservazioni del gruppo di controllo; in nero quelle dei pazienti." width="576" />
<p class="caption">
(#fig:zetsche-scatter)Associazione tra le variabili BDI-II e CES-D nello studio di Zetsche et al. (2019). In grigio sono rappresentate le osservazioni del gruppo di controllo; in nero quelle dei pazienti.
</p>
</div>
</div>
<div id="covarianza" class="section level3 hasAnchor" number="5.0.2">
<h3 class="hasAnchor"><span class="header-section-number">5.0.2</span> Covarianza<a href="#covarianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il problema che ci poniamo ora è quello di trovare un indice numerico che
descrive di quanto la nube di punti si discosta da una perfetta
relazione lineare tra le due variabili, ovvero che descrive la direzione e la
forza della relazione lineare tra le due variabili. Ci sono vari indici
statistici che possono essere utilizzati a questo scopo.</p>
<p>Iniziamo a considerare il più importante di tali indici, chiamato
<em>covarianza</em>. In realtà la definizione di questo indice non ci
sorprenderà più di tanto in quanto, in una forma solo apparentemente
diversa, l’abbiamo già incontrato in precedenza. Ci ricordiamo infatti
che la varianza di una generica variabile <span class="math inline">\(X\)</span> è definita come la media
degli scarti quadratici di ciascuna osservazione dalla media:</p>
<p><span class="math display">\[\begin{equation}
S_{XX} = \frac{1}{n} \sum_{i=1}^n(X_i - \bar{X}) (X_i - \bar{X}).
(\#eq:variance2)
\end{equation}\]</span></p>
<p>Infatti, la varianza viene talvolta descritta come la “covarianza di una
variabile con sé stessa”.</p>
<p>Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di
una sola variabile, chiediamoci come due variabili <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> “variano
insieme” (co-variano). È facile capire come una risposta a tale domanda
possa essere fornita da una semplice trasformazione della formula
precedente che diventa:</p>
<p><span class="math display">\[\begin{equation}
S_{XY} = \frac{1}{n} \sum_{i=1}^n(X_i - \bar{X}) (Y_i - \bar{Y}).
(\#eq:covariance)
\end{equation}\]</span></p>
<p>L’eq. @ref(eq:covariance) ci fornisce dunque la definizione della covarianza.</p>
<p>Per capire il significato dell’eq. @ref(eq:covariance), supponiamo di dividere il grafico della figura @ref(fig:zetsche-scatter) in quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo i quadranti partendo da quello in basso a sinistra e muovendoci in senso antiorario.</p>
<p>Se prevalgono punti nel I e III quadrante, allora la nuvola di punti
avrà un andamento crescente (per cui a valori bassi di <span class="math inline">\(X\)</span> tendono ad
associarsi valori bassi di <span class="math inline">\(Y\)</span> e a valori elevati di <span class="math inline">\(X\)</span> tendono ad
associarsi valori elevati di <span class="math inline">\(Y\)</span>) e la covarianza segno positivo. Mentre
se prevalgono punti nel II e IV quadrante la nuvola di punti avrà un
andamento decrescente (per cui a valori bassi di <span class="math inline">\(X\)</span> tendono ad
associarsi valori elevati di <span class="math inline">\(Y\)</span> e a valori elevati di <span class="math inline">\(X\)</span> tendono ad
associarsi valori bassi di <span class="math inline">\(Y\)</span>) e la covarianza segno negativo. Dunque,
il segno della covarianza ci informa sulla direzione della relazione
lineare tra due variabili: l’associazione lineare si dice positiva se la
covarianza è positiva, negativa se la covarianza è negativa.</p>
<p>Il segno della covarianza ci informa sulla direzione della relazione, ma
invece il valore assoluto della covarianza ci dice ben poco. Esso,
infatti, dipende dall’unità di misura delle variabili. Nel caso presente
questo concetto è difficile da comprendere, dato che le due variabili in
esame non hanno un’unità di misura (ovvero, hanno un’unità di misura
arbitraria e priva di significato). Ma quest’idea diventa chiara se
pensiamo alla relazione lineare tra l’altezza e il peso delle persone,
ad esempio. La covarianza tra queste due quantità è certamente positiva,
ma il valore assoluto della covarianza diventa più grande se l’altezza
viene misurata in millimetri e il peso in grammi, e diventa più piccolo
l’altezza viene misurata in metri e il peso in chilogrammi. Dunque, il
valore della covarianza cambia al mutare dell’unità di misura delle
variabili anche se l’associazione tra le variabili resta costante.</p>
</div>
<div id="correlazione" class="section level3 hasAnchor" number="5.0.3">
<h3 class="hasAnchor"><span class="header-section-number">5.0.3</span> Correlazione<a href="#correlazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dato che il valore assoluto della covarianza è di difficile
interpretazione – in pratica, non viene mai interpretato – è
necessario trasformare la covarianza in modo tale da renderla immune
alle trasformazioni dell’unità di misura delle variabili. Questa
operazione si dice <em>standardizzazione</em> e corrisponde alla divisione
della covarianza per le deviazioni standard (<span class="math inline">\(s_X\)</span>, <span class="math inline">\(s_Y\)</span>) delle due
variabili:</p>
<p><span class="math display">\[\begin{equation}
r_{XY} = \frac{S_{XY}}{s_X s_Y}.
(\#eq:correlation)
\end{equation}\]</span></p>
<p>La quantià che si ottiene in questo modo viene chiamata <em>correlazione</em> di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, la hanno introdotta).</p>
<p>Il coefficiente di correlazione ha le seguenti proprietà:</p>
<ul>
<li>ha lo stesso segno della covarianza, dato che si ottiene dividendo
la covarianza per due numeri positivi;</li>
<li>è un numero puro, cioè non dipende dall’unità di misura delle
variabili;</li>
<li>assume valori compresi tra -1 e +1.</li>
</ul>
<p>Ad esso possiamo assegnare la seguente interpretazione:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(r_{XY} = -1\)</span> <span class="math inline">\(\rightarrow\)</span> perfetta relazione negativa: tutti i
punti si trovano esattamente su una retta con pendenza negativa (dal
quadrante in alto a sinistra al quadrante in basso a destra);</li>
<li><span class="math inline">\(r_{XY} = +1\)</span> <span class="math inline">\(\rightarrow\)</span> perfetta relazione positiva: tutti i
punti si trovano esattamente su una retta con pendenza positiva (dal
quadrante in basso a sinistra al quadrante in alto a destra);</li>
<li><span class="math inline">\(-1 &lt; r_{XY} &lt; +1\)</span> <span class="math inline">\(\rightarrow\)</span> presenza di una relazione lineare
di intensità diversa;</li>
<li><span class="math inline">\(r_{XY} = 0\)</span> <span class="math inline">\(\rightarrow\)</span> assenza di relazione lineare tra <span class="math inline">\(X\)</span> e
<span class="math inline">\(Y\)</span>.</li>
</ol>
<div class="exercise">
<p><span id="exr:unlabeled-div-23" class="exercise"><strong>(#exr:unlabeled-div-23) </strong></span>Per i dati della figura @ref(fig:zetsche-scatter), la covarianza è 207.426. Il segno positivo della covarianza ci dice che tra le due variabili c’è un’associazione lineare positiva. Per capire qual è l’intensità della relazione lineare tra le due variabili calcoliamo la correlazione.
Essendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali a 15.37 e 14.93, la correlazione diventa uguale a <span class="math inline">\(\frac{207.426}{15.38 \cdot 14.93} = 0.904.\)</span> Tale valore è prossimo a 1.0, il che vuol dire che i punti del diagramma a dispersione non si discostano troppo da una retta con una pendenza positiva.</p>
</div>
</div>
<div id="correlazione-e-causazione" class="section level2 hasAnchor" number="5.1">
<h2 class="hasAnchor"><span class="header-section-number">5.1</span> Correlazione e causazione<a href="#correlazione-e-causazione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Facendo riferimento nuovamente alla figura @ref(fig:zetsche-scatter), possiamo dire che, in molte applicazioni (ma non nel caso presente!) l’asse <span class="math inline">\(x\)</span> rappresenta una quantità nota come <em>variabile indipendente</em> e l’interesse si concentra sulla sua influenza sulla <em>variabile dipendente</em> tracciata sull’asse <span class="math inline">\(y\)</span>. Ciò presuppone però che sia nota la direzione in cui l’influenza causale potrebbe risiedere. È importante tenere bene a mente che la correlazione è soltanto un indice descrittivo della relazione lineare tra due variabili e in nessun caso può essere usata per inferire alcunché sulle relazioni <em>causali</em> che legano le variabili. È ben nota l’espressione: “correlazione non significa causazione”.</p>
<p>Di opinione diversa era invece Karl Pearson (1911), il quale ha affermato: <em>“Quanto spesso, quando è stato osservato un nuovo fenomeno,
sentiamo che viene posta la domanda: ‘qual è la sua causa?’. Questa è
una domanda a cui potrebbe essere assolutamente impossibile rispondere.
Invece, può essere più facile rispondere alla domanda: ‘in che misura
altri fenomeni sono associati con esso?’. Dalla risposta a questa
seconda domanda possono risultare molte preziose conoscenze.”</em></p>
<p>Che alla seconda domanda posta da Pearson sia facile rispondere è indubbio. Che la nostra comprensione di un fenomeno possa aumentare sulla base delle
informazioni fornite unicamente dalle correlazioni, invece, è molto dubbio e quasi certamente falso.</p>
<div id="usi-della-correlazione" class="section level3 hasAnchor" number="5.1.1">
<h3 class="hasAnchor"><span class="header-section-number">5.1.1</span> Usi della correlazione<a href="#usi-della-correlazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Anche se non può essere usata per studiare le relazioni causali, la
correlazione viene usata per molti altri scopi tra i quali, per esempio,
quello di misurare la <em>validità concorrente</em> di un test psiologico. Se
un test psicologico misura effettivamente ciò che ci si aspetta che
misuri (nel caso dell’esempio presente, la depressione), allora dovremo
aspettarci che fornisca una correlazione alta con risultati di altri
test che misurano lo stesso costrutto – come nel caso dei dati di
<span class="citation">(<a href="#ref-zetschefuture2019" role="doc-biblioref">Zetsche, Bürkner, and Renneberg 2019</a>)</span>. Un’altra proprietà desiderabile di un test
psicometrico è la <em>validità divergente</em>: i risultati di test
psicometrici che misurano costrutti diversi dovrebbero essere poco
associati tra loro. In altre parole, in questo secondo caso dovremmo
aspettarci che la correlazione sia bassa.</p>
</div>
<div id="correlazione-di-spearman" class="section level3 hasAnchor" number="5.1.2">
<h3 class="hasAnchor"><span class="header-section-number">5.1.2</span> Correlazione di Spearman<a href="#correlazione-di-spearman" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una misura alternativa della relazione lineare tra due variabili è
fornita dal coefficiente di correlazione di Spearman e dipende soltanto
dalla relazione d’ordine dei dati, non dagli specifici valori dei dati.
Tale misura di associazione è appropriata quando, del fenomeno in esame,
gli psicologi sono stati in grado di misurare soltanto le relazioni
d’ordine tra le diverse modalità della risposta dei soggetti, non
l’intensità della risposta. Le variabili psicologiche che hanno questa
proprietà si dicono <em>ordinali</em>. Nel caso di variabili ordinali, non è
possibile sintetizzare i dati mediante le statistiche descrittive che
abbiamo introdotto in questo capitolo, quali ad esempio la media e la
varianza, ma è invece solo possibile riassumere i dati mediante una
distribuzione di frequenze per le varie modalità della risposta.</p>
</div>
<div id="correlazione-nulla" class="section level3 hasAnchor" number="5.1.3">
<h3 class="hasAnchor"><span class="header-section-number">5.1.3</span> Correlazione nulla<a href="#correlazione-nulla" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un ultimo aspetto da mettere in evidenza a proposito della correlazione riguarda il fatto che la correlazione descrive la direzione e l’intensità della relazione lineare tra due variabili. Relazioni non lineari tra le variabili, anche se sono molto forti, non vengono catturate dalla correlazione. È importante rendersi conto che una correlazione pari a zero non significa che non c’è relazione tra le due variabili, ma solo che tra esse non c’è una relazione <em>lineare</em>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-24" class="exercise"><strong>(#exr:unlabeled-div-24) </strong></span>La figura @ref(fig:zerocorr) fornisce un esempio di correlazione nulla in presenza di una chiara relazione (non lineare) tra due variabili.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/zerocorr-1.png" alt="Due insiemi di dati (fittizi) per i quali i coefficienti di correlazione di Pearson sono entrambi 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili." width="576" />
<p class="caption">
(#fig:zerocorr)Due insiemi di dati (fittizi) per i quali i coefficienti di correlazione di Pearson sono entrambi 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.
</p>
</div>
</div>
</div>
</div>
<div id="commenti-e-considerazioni-finali-3" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La prima fase dell’analisi dei dati riassume i dati mediante gli strumenti della statistica descrittiva. Le tipiche domande che vengono affrontate in questa fase sono: qual è la distribuzione delle variabili di interesse? Quali relazioni a coppie si possono osservare nel campione? Ci sono delle osservazioni ‘anomale’, ovvero estremamente discrepanti rispetto alle altre, sia quando si esaminano le statistiche descrittive univariate (ovvero, quelle che riguardano le caratteristiche di una variabile presa singolarmente), sia quando vengono esaminate le statistiche bivariate (ovvero, le statistiche che descrivono l’associazione tra le variabili)? È importante avere ben chiare le idee su questi punti prima di procedere con qualsiasi procedura statistica di tipo inferenziale. Per rispondere alle domande che abbiamo elencato sopra, ed ad altre simili, è molto utile procedere con delle rappresentazioni grafiche dei dati. È chiaro che, quando disponiamo di grandi moli di dati (come è sempre il caso in psicologia), l’analisi descrittiva dei dati deve essere svolta mediante un software statistico.</p>
<!--chapter:end:012_correlation.Rmd-->
</div>
</div>
<div id="part-il-calcolo-delle-probabilità" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Il calcolo delle probabilità<a href="#part-il-calcolo-delle-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="intro-prob-1" class="section level1 hasAnchor" number="6">
<h1 class="hasAnchor"><span class="header-section-number">6</span> La logica dell’incerto<a href="#intro-prob-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In questa parte della dispensa verrà introdotta la teoria delle probabilità. Prima di entrare nei dettagli, cerchiamo di capire perché la probabilità sia cruciale per la ricerca scientifica.</p>
<!-- Ingenuamente, potremmo pensare che il modo migliore di procedere nella ricerca scientifica sia quello di usare la logica deduttiva (aristotelica) -- con un tale metodo, infatti, siamo sicuri di non commettere errori. Un esempio è il sillogismo, come  -->
<!-- - tutti gli uomini sono mortali,  -->
<!-- - Socrate è un uomo,  -->
<!-- - quindi, Socrate è mortale. -->
<!-- La logica deduttiva, però, non può essere utilizzata in psicologia, né in alcun'altra scienza empirica. Nel sillogismo, la correttezza del ragionamento dipende dalla sua struttura e non dal significato delle parole (come uomo, mortale, ecc.). Nelle scienze empiriche, però, il "significato delle parole" è cruciale. Le "parole" usate nel sillogismo corrispondono ai "concetti teorici" (detti, in psicologia, costrutti) delle teorie scientifiche. Il problema è che la _corrispondenza_ tra relazioni tra costrutti teorici, da una parte, e relazioni tra i fenomeni empirici, dall'altra, dipende dalla validità delle teorie. In fisica, ad esempio, i concetti teorici di massa ($m$), peso ($P$) e forza di gravità ($g$) consentono di descrivere accuratamente ciò che si osserva nel mondo empirico: $P = m \cdot g$. Non è così, invece, nelle scienze sociali, dove le relazioni tra costrutti sono in grado di descrivere _solo in parte_ le relazioni tra i corrispondenti fenomeni empirici.  -->
<p>La teoria delle probabilità è cruciale per la scienza perché la ricerca procede mediante l’inferenza induttiva. Non siamo mai completamente sicuri della verità di una proposizione (ipotesi, teoria): al valore di verità di una proposizione possiamo solo assegnare un giudizio probabilistico. L’approccio bayesiano è una scuola di pensiero che usa la probabilità per quantificare il grado di fiducia che può essere attribuito ad una proposizione. L’inferenza statistica bayesiana è un tipo di inferenza induttiva che ha lo scopo di quantificare la fiducia che si ha nell’ipotesi <span class="math inline">\(H\)</span> dopo il verificarsi del dato d’evidenza <span class="math inline">\(E\)</span>. Per quantificare un tale grado di fiducia l’inferenza statistica bayesiana utilizza la teoria delle probabilità. Una comprensione dell’inferenza statistica bayesiana richiede dunque, preliminarmente, la conoscenze della teoria delle probabilità.</p>
<!-- ## Proposizioni e modelli statistici -->
<!-- Nell'inferenza bayesiana, le proposizioni di una teoria scientifica sono espresse nella forma di un modello statistico, ovvero mediante una legge generale che descrive il modo in cui un fenomeno si manifesta. Tale legge generale viene anche detta _processo generativo dei dati_.^[Per fare un esempio, consideriamo il quoziente d'intelligenza. Sappiamo che il punteggio totale della _Wechsler Adult Intelligence Scale_ ha, nella popolazione, media 100 e deviazione standard 15 (dato che il test WAIS è stato costruito in modo da avere una tale proprietà). Quindi, se prendiamo un campione abbastanza grande di persone, i valori del QI di tali persone avranno, circa, media uguale a 100 e deviazione standard uguale a 15. Se con tali dati costruiamo un istogramma, sappiamo anche che il profilo di tale istogramma sarà ben descritto da una funzione matematica che va sotto il nome di _legge gaussiana_. La rappresentazione grafica della funzione gaussiana è la classica curva a campana che sicuramente avrete già vista. La funzione gaussiana dipende da due parametri: la media (solitamente indicata con $\mu$) e la deviazione standard ($\sigma$). Se cambiamo questi parametri, ma usiamo sempre la stessa formula, otteniamo una curva diversa.  Per esempio, se consideriamo solo la sotto-popolazione dei bambini plus-dotati, la distribuzione dei punteggi QI sarà una gaussiana centrata su 130, con una qualche deviazione standard. In questo esempio, la gaussiana è il modello generatore dei dati e i parametri sono $\mu$ e $\sigma$. Per altri fenomeni, come ad esempio i tempi di reazione nel compito Stroop, o la gravità della sintomatologia ansiosa negli adulti misurata attraverso il test _Beck Anxiety Inventory_, il modello gaussiano non è più appropriato ed è necessario _ipotizzare_ un diverso processo generativo dei dati.] In generale, l'inferenza induttiva bayesiana procede _ipotizzando_ un modello generativo dei dati per poi, sulla base dei dati osservati in un campione e sulla base delle nostre credenze a priori, _inferire_ i valori plausibili dei parametri del modello. In questo processo inferenziale, possiamo individuare cinque fonti di incertezza: -->
<!-- 1. incertezza sui parametri dei modelli; -->
<!-- 2. incertezza su quale sia il modello migliore; -->
<!-- 3. incertezza su cosa fare con l'output dei (migliori) modelli; -->
<!-- 4. incertezza sul funzionamento del software che produce i risultati; -->
<!-- 5. incertezza sul fatto che il/i modello/i (migliore/i) siano coerenti con altri campioni di dati. -->
<!-- * L'approccio bayesiano usa la teoria delle probabilità per descrivere l'incertezza relativa ai punti (1) e (2); -->
<!-- * l'approccio bayesiano si collega alla teoria delle decisioni, che prescrive come affrontare il problema descritto nel punto (3); -->
<!-- * il software utilizzato (nel nostro caso, Stan) fa tutto ciò che è possibile per mitigare la preoccupazione (4); -->
<!-- * l'approccio bayesiano consente di quantificare l'incertezza descritta al punto (5). -->
<!-- ## Oggettività e soggettività -->
<!-- Facendo delle assunzioni non controverse, l'inferenza bayesiana consente di aggiornare le credenze a priori sui valori (sconosciuti) dei parametri $\theta$ di un modello statistico alla luce di nuovi dati $y_1, y_2, \dots, y_N$ che vengono osservati. L'approccio bayesiano è etichettato come "soggettivo" perché non ci dice quale valore dovrebbe essere assegnato ai parametri prima di avere osservato i dati $y_1, y_2, \dots, y_N$. In realtà, l'aggiornamento bayesiano è il modo più razionale di procedere: se, prima di avere osservato i dati, il ricercatore ha una credenza assurda relativamente al valore di $\theta$, dopo avere osservato $y_1, y_2, \dots, y_N$ le sue credenze _a posteriori_ su $\theta$, aggiornate secondo i principi bayesiani, saranno meno assurde. Il problema di questo modo di procedere non è che, a priori, i ricercatori (o chiunque altro) possono avere delle credenze sbagliate, ma bensì il fatto che, avendo osservato $y_1, y_2, \dots, y_N$, le credenze su $\theta$ non vengono aggiornate secondo i principi bayesiani. Infatti, in alcune situazioni, l'osservazione di dati che contraddicono le credenze pregresse non fa altro che rafforzare tali convinzioni errate -- il problema, dunque, non nasce dalla "soggettività" dell'approccio bayesiano, ma quanto dal fatto di non seguire un tale modo di procedere! -->
<!-- Lo scopo delle prossime sezioni della dispensa è quello di introdurre quei concetti base della teoria delle probabilità che risultano necessari per una presentazione delle procedure dell'inferenza induttiva bayesiana. -->
<div id="che-cosè-la-probabilità" class="section level2 hasAnchor" number="6.1">
<h2 class="hasAnchor"><span class="header-section-number">6.1</span> Che cos’è la probabilità?<a href="#che-cosè-la-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La definizione della probabilità è un problema estremamente dibattuto ed aperto. Sono state fornite due possibili soluzioni al problema di definire il concetto di probabilità.</p>
<ol style="list-style-type: lower-alpha">
<li><p>La natura della probabilità è “ontologica” (ovvero, basata sulla metafisica): la probabilità è una proprietà della della realtà, del mondo, di come sono le cose, indipendentemente dalla nostra esperienza. È una visione che qualcuno chiama “oggettiva”.</p></li>
<li><p>La natura della probabilità è “epistemica” (ovvero, basata sulla conoscenza): la probabilità si riferisce alla conoscenza che abbiamo del mondo, non al mondo in sé. Di conseguenza è detta, in contrapposizione alla precedente definizione, “soggettiva”.</p></li>
</ol>
<p>In termini epistemici, la probabilità fornisce una misura della nostra incertezza sul verificarsi di un fenomeno, alla luce delle informazioni disponibili. Potremmo dire che c’è una “scala” naturale che ha per estremi il vero (1: evento certo) da una parte ed il falso (0: evento impossibile) dall’altra. La probabilità è la quantificazione di questa scala: descrive lo stato della nostra incertezza rispetto al contenuto di verità di una proposizione.</p>
<p>L’incertezza nelle nostre previsioni può sorgere per due ragioni fondamentalmente diverse. Il primo è dovuto alla nostra ignoranza delle cause nascoste sottostanti o dei meccanismi che generano i dati. Questa è appunto un’incertezza <em>epistemica</em>. Il secondo tipo di incertezza deriva dalla variabilità intrinseca dei fenomeni, che non può essere ridotta anche se raccogliamo più dati. Questa seconda forma di incertezza è talvolta chiamata <em>aleatoria</em>. Come esempio concreto, consideriamo il lancio di una moneta equilibrata. Sappiamo con certezza che la probabilità di testa è <span class="math inline">\(P = 0.5\)</span>, quindi non c’è incertezza epistemica, ma non questo non è sufficiente per prevedere con certezza il risultato – ovvero, l’incertezza aleatoria persiste anche in assenza di incertezza epistemica.</p>
<p>Nell’interpretazione frequentista, la probabilità <span class="math inline">\(P(E)\)</span> rappresenta la frequenza relativa a lungo termine di un grande numero di ripetizioni di un esperimento casuale sotto le medesime condizioni. Viene stressata qui l’idea che ciò di cui parliamo è qualcosa che emerge nel momento in cui è possibile ripetere l’esperimento casuale tante volte sotto le medesime condizioni – sono invece esclusi gli eventi unici e irripetibili.</p>
<!-- - Nell'interpretazione bayesiana della probabilità, $P(A)$ rappresenta il grado di fiducia che si ha relativamente al verificarsi dell'evento $A$. -->
<!-- Possiamo citare De Finetti, ad esempio, il quale ha formulato la seguente definizione "soggettiva" di probabilità la quale risulta applicabile anche ad esperimenti casuali i cui eventi elementari non siano ritenuti ugualmente possibili e che non siano necessariamente ripetibili più volte sotto le stesse condizioni.  -->
<p>L’interpretazione bayesiana della probabilità fa invece ricorso ad una concezione più ampia, non legata al solo evento in sé ma che include anche il soggetto assegnante la funzione di probabilità. In pratica l’assegnazione di probabilità bayesiana viene effettuata dal decisore, in base alle proprie conoscenze a priori integrate con tutto il generico bagaglio culturale personale. In questo modo, la probabilità non sarà obbligatoriamente la stessa per tutti i soggetti, ma variarierà a seconda delle informazioni a disposizione, dell’esperienza personale e soprattutto del punto di vista proprio di ogni decisore ed è dunque assimilabile al “grado di fiducia” – in inglese <em>degree of belief</em> – di un dato soggetto, in un dato istante e con un dato insieme d’informazioni, circa l’accadere dell’evento <span class="math inline">\(E\)</span>. “[N]essuna scienza ci permetterà di dire: il tale fatto accadrà, andrà così e così, perché ciò è conseguenza di tale legge, e tale legge è una verità assoluta, ma tanto meno ci condurrà a concludere scetticamente: la verità assoluta non esiste, e quindi tale fatto può accadere e può non accadere, può andare così e può andare in tutt’altro modo, nulla io ne so. Quel che si potrà dire è questo: io prevedo che il tale fatto avverrà, e avverrà nel tal modo, perché l’esperienza del passato e l’elaborazione scientifica cui il pensiero dell’uomo l’ha sottoposta mi fanno sembrare ragionevole questa previsione” <span class="citation">(<a href="#ref-definetti1931prob" role="doc-biblioref">Finetti 1931</a>)</span>.</p>
<p>L’impostazione bayesiana, sviluppata da Ramsey e de Finetti, riconduce l’assegnazione di probabilità allo scommettere sul verificarsi di un evento: la probabilità di un evento <span class="math inline">\(E\)</span> è la quota <span class="math inline">\(p(E)\)</span> che un individuo reputa di dover pagare ad un banco per ricevere “1” ovvero “0” verificandosi o non verificandosi <span class="math inline">\(E\)</span>.</p>
<p>Secondo De Finetti, le valutazioni di probabilità degli eventi devono rispondere ai principi di equità e coerenza. Una scommessa risponde al principio di <em>equità</em> se il ruolo di banco e giocatore sono scambiabili in ogni momento del gioco e sempre alle stesse condizioni. Una scommessa risponde al principio di <em>coerenza</em> se non vi sono combinazioni di scommesse che consentano (sia al banco che al giocatore) di realizzare perdite o vincite certe.</p>
<p>L’approccio definettiano dell’impostazione della scommessa si basa dunque sulle assunzioni di razionalità e coerenza del decisore, al quale è fatto esplicito divieto di effettuare scommesse a perdita o guadagno certo. Il decisore, proponendo la scommessa, deve essere disposto a scambiare il posto dello scommettitore con quello del banco.</p>
<p>Il metodo della scommessa, oltre che una definizione, fornisce un mezzo operativo di assegnazione della probabilità. Sulla base di questa definizione operativa, che si può ritenere ragionevolmente soddisfatta dal comportamento di un qualunque individuo che agisca in modo razionale in condizioni di incertezza, possono essere agevolmente dimostrate tutte le proprietà classiche della probabilità: essa non può assumere valori negativi, né può essere superiore all’unità; se <span class="math inline">\(E\)</span> è un evento certo, la sua probabilità è 1; se invece <span class="math inline">\(E\)</span> è un evento impossibile, la sua probabilità è 0.</p>
<p>I problemi posti dall’approccio definettiano riguardano l’arbitrarietà dell’assegnazione soggettività di probabilità la quale sembra negare la validità dell’intero costrutto teorico. In risposta a tale critica, i bayesiani sostengono che gli approcci oggettivisti alla probabilità nascondono scelte arbitrarie preliminari e sono basate su assunzioni implausibili. È molto più onesto esplicitare subito tutte le scelte arbitrarie effettuate nel corso dell’analisi in modo da controllarne coerenza e razionalità.</p>
<!-- In altri termini, de Finetti ritiene che la probabilità debba essere concepita non come una proprietà "oggettiva" dei fenomeni, ma bensì come il "grado di fiducia" -- in inglese _degree of belief_ -- di un dato soggetto, in un dato istante e con un dato insieme d'informazioni, riguardo al verificarsi di un evento. Per denotare sia la probabilità (soggettiva) di un evento sia il concetto di _valore atteso_ (che descriveremo in seguito), @definetti1970teoria utilizza il termine "previsione" (e lo stesso simbolo $P$): *"la previsione [$\dots$] consiste nel considerare ponderatamente tutte le alternative possibili per ripartire fra di esse nel modo che parrà più appropriato le proprie aspettative, le proprie sensazioni di probabilità."* -->
</div>
<div id="variabili-casuali-e-probabilità-di-un-evento" class="section level2 hasAnchor" number="6.2">
<h2 class="hasAnchor"><span class="header-section-number">6.2</span> Variabili casuali e probabilità di un evento<a href="#variabili-casuali-e-probabilità-di-un-evento" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Esaminiamo qui di seguito alcuni concetti di base della teoria delle probabilità, la quale può essere vista come un’estensione della logica.</p>
<div id="eventi-e-probabilità" class="section level3 hasAnchor" number="6.2.1">
<h3 class="hasAnchor"><span class="header-section-number">6.2.1</span> Eventi e probabilità<a href="#eventi-e-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nella teoria delle probabilità il risultato “testa” nel lancio di una moneta è chiamato <em>evento</em>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> Un evento, denotato da una variabile binaria, corrisponde ad uno stato del mondo che si verifica oppure no. Ad esempio, <span class="math inline">\(Y\)</span> = 1 può denotare l’evento per cui il lancio di una moneta produce il risultato testa. Il funzionale <span class="math inline">\(P(Y)\)</span> denota la probabilità con cui si ritiene che l’evento <span class="math inline">\(Y\)</span> sia vero (o la proporzione di volte che si verifica tale evento osservando a lungo termine delle ripetizioni indipendenti di un esperimento casuale). Ad esempio, per il lancio di una moneta equilibrata, la probabilità dell’evento “il risultato del lancio della moneta è testa” è scritta come <span class="math inline">\(P(Y = 1) = 0.5.\)</span></p>
<p>Se la moneta è equilibrata dobbiamo anche avere <span class="math inline">\(P(Y = 0) = 0.5\)</span>. I due eventi <em>Y</em> = 1 e <span class="math inline">\(Y\)</span> = 0 sono <em>mutuamente esclusivi</em> nel senso che non possono entrambi verificarsi contemporaneamente: <span class="math inline">\(P(Y = 1\; \land \; Y = 0) = 0.\)</span> Gli eventi <span class="math inline">\(Y\)</span> = 1 e <span class="math inline">\(Y\)</span> = 0 di dicono <em>esaustivi</em>, nel senso che almeno uno di essi deve verificarsi e nessun altro tipo di evento è possibile. Nella notazione probabilistica, <span class="math inline">\(P(Y = 1\; \lor \; Y = 0) = 1.\)</span> Il connettivo logico “o” (<span class="math inline">\(\lor\)</span>) specifica eventi <em>disgiunti</em>, ovvero eventi che non possono verificarsi contemporaneamente (eventi <em>incompatibili</em>) e per i quali, perciò, la probabilità della loro congiunzione è <span class="math inline">\(P(A \; \land \; B) = 0\)</span>. Il connettivo logico “e” (<span class="math inline">\(\land\)</span>), invece, specifica eventi <em>congiunti</em>, ovvero eventi che possono verificarsi contemporaneamente (eventi <em>compatibili</em>) e per i quali, perciò, la probabilità della loro congiunzione è <span class="math inline">\(P(A \; \land \; B) &gt; 0\)</span>. La probabilità del verificarsi di due eventi congiunti <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> si può denotare, in maniera equivalente, con la notazione precedente, oppure con <span class="math inline">\(P(A \cap B)\)</span>, oppure con <span class="math inline">\(P(A, B)\)</span>.</p>
<p>Si richiede che <span class="math inline">\(0 \leq P(A) \leq 1\)</span>, dove <span class="math inline">\(P(A) = 0\)</span> denota l’evento impossibile e <span class="math inline">\(P(A) = 1\)</span> denota l’evento certo. Scriviamo <span class="math inline">\(P(\lnot A)\)</span> o <span class="math inline">\(P(\bar{A})\)</span> per denotare la probabilità che l’evento <span class="math inline">\(A\)</span> non avvenga; questa probabilità è definita come <span class="math inline">\(P(\bar{A}) = 1 − P(A)\)</span>.</p>
</div>
<div id="spazio-campione-e-risultati-possibili" class="section level3 hasAnchor" number="6.2.2">
<h3 class="hasAnchor"><span class="header-section-number">6.2.2</span> Spazio campione e risultati possibili<a href="#spazio-campione-e-risultati-possibili" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Anche se il lancio di una moneta produce sempre uno specifico risultato nel mondo reale, possiamo anche immaginare i possibili risultati alternativi che si sarebbero potuti osservare. Quindi, anche se in uno specifico lancio la moneta dà testa (<span class="math inline">\(Y\)</span> = 1), possiamo immaginare la possibilità che il lancio possa avere prodotto croce (<span class="math inline">\(Y\)</span> = 0). Tale ragionamento controfattuale è la chiave per comprendere la teoria delle probabilità e l’inferenza statistica.</p>
<p>I risultati possibili che si possono osservare come conseguenza del lancio di una moneta determinano i valori possibili che la variabile casuale può assumere. L’insieme <span class="math inline">\(\Omega\)</span> di tutti i risultati possibili è chiamato <em>spazio campione</em> (<em>sample space</em>). Lo spazio campione può essere concettualizzato come un’urna contenente una pallina per ogni possibile risultato del lancio della moneta. Su ogni pallina è scritto il valore della variabile casuale. Uno specifico lancio di una moneta – ovvero, l’osservazione di uno specifico valore di una variabile casuale – è chiamato <em>esperimento casuale</em>.</p>
<p>Il lancio di un dado ci fornisce l’esempio di un altro esperimento casuale. Supponiamo di essere interessati all’evento “il lancio del dado produce un numero dispari”. Un <em>evento</em> seleziona un sottoinsieme dello spazio campione: in questo caso, l’insieme dei risultati <span class="math inline">\(\{1, 3, 5\}\)</span>. Se esce 3, per esempio, diciamo che si è verificato l’evento “dispari” (ma l’evento “dispari” si sarebbe anche verificato anche se fosse uscito 1 o 5).</p>
</div>
</div>
<div id="variabili-casuali-1" class="section level2 hasAnchor" number="6.3">
<h2 class="hasAnchor"><span class="header-section-number">6.3</span> Variabili casuali<a href="#variabili-casuali-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sia <span class="math inline">\(Y\)</span> il risultato del lancio di moneta equilibrata, non di un generico lancio di una moneta, ma un’istanza specifica del lancio di una specifica moneta in un dato momento. Definita in questo modo, <span class="math inline">\(Y\)</span> è una <em>variabile casuale</em>, ovvero una variabile i cui valori non possono essere previsti con esattezza. Se la moneta è equilibrata, c’è una probabilità del 50% che il lancio della moneta dia come risultato “testa” e una probabilità del 50% che dia come risultato “croce”. Per facilitare la trattazione, le variabili casuali assumono solo valori numerici. Per lo specifico lancio della moneta in questione, diciamo, ad esempio, che la variabile casuale <span class="math inline">\(Y\)</span> assume il valore 1 se esce testa e il valore 0 se esce croce.</p>
<p>Una variabile casuale può essere <em>discreta</em> o <em>continua</em>. Una variabile casuale discreta può assumere un numero finito di valori <span class="math inline">\(x_1, \dots ,x_n\)</span>, in corrispondenza degli eventi <span class="math inline">\(E_i, \dots, E_n\)</span> che si verificano con le rispettive probabilità <span class="math inline">\(p_1, \dots, p_n\)</span>. Un esempio è il punteggio totale di un test psicometrico costituito da item su scala Likert. Invece un esempio di una variabile casuale continua è la distanza tra due punti, che può assumere infiniti valori all’interno di un certo intervallo. L’insieme <span class="math inline">\(S\)</span> dei valori che la variabile casuale può assumere è detto <em>spazio dei valori</em> o <em>spazio degli stati</em>.</p>
<p>La caratteristica fondamentale di una variabile casuale è data dall’insieme delle probabilità dei suoi valori, detta <em>distribuzione di probabilità</em>. Nel seguito useremo la notazione <span class="math inline">\(P(\cdot)\)</span> per fare riferimento alle distribuzioni di probabilità delle variabili casuali discrete e <span class="math inline">\(p(\cdot)\)</span> per fare riferimento alla densità di probabilità delle variabili casuali continue. In questo contesto, l’insieme dei valori che la variabile casuale può assumere è detto <em>supporto</em> della sua distribuzione di probabilità. Il supporto di una variabile casuale può essere finito (come nel caso di una variabile casuale uniforme di supporto <span class="math inline">\([a, b]\)</span>) o infinito (nel caso di una variabile causale gaussiana il cui supporto coincide con la retta reale).</p>
</div>
<div id="usare-la-simulazione-per-stimare-le-probabilità" class="section level2 hasAnchor" number="6.4">
<h2 class="hasAnchor"><span class="header-section-number">6.4</span> Usare la simulazione per stimare le probabilità<a href="#usare-la-simulazione-per-stimare-le-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- I metodi basati sulla simulazione consentono di stimare le probabilità degli eventi in un modo diretto, se siamo in grado di generare molteplici e casuali realizzazioni delle variabili casuali coinvolte nelle definizioni degli eventi. -->
<p>In questa dispensa verrà adottata l’interpretazione bayesiana delle probabilità. Tuttavia, le regole di base della teoria delle probabilità sono le stesse, indipendentemente dall’interpretazione adottata. Pertanto, negli esempi seguenti, possiamo utilizzare la simulazione per stimare le probabilità degli eventi in un modo diretto, ovvero mediante la generazione di molteplici osservazioni delle variabili casuali derivate dagli eventi di interesse.</p>
<p>Ad esempio, per simulare in <span class="math inline">\(\R\)</span> il lancio di una moneta equilibrata iniziamo con il definire un vettore che contiene i risultati possibili del lancio della moneta (ovvero i valori possibili della variabile casuale <span class="math inline">\(Y\)</span>):</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>coin <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>L’estrazione casuale di uno di questi due possibili valori (ovvero, la simulazione di uno specifico lancio di una moneta) si realizza con la funzione <code>sample()</code>:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sample</span>(coin, <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<p>In maniera equivalente, la stessa operazione si può realizzare mediante l’istruzione</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>Supponiamo di ripetere questo esperimento casuale 100 volte e di registrare i risultati così ottenuti. La stima della probabilità dell’evento <span class="math inline">\(P(Y = 1)\)</span> è data dalla frequenza relativa del numero di volte in cui abbiamo osservato l’evento di interesse (<span class="math inline">\(Y = 1\)</span>):</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  y[m] <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>estimate <span class="ot">=</span> <span class="fu">sum</span>(y) <span class="sc">/</span> M</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;estimated Pr[Y = 1] =&quot;</span>, estimate)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.53</span></span></code></pre></div>
<p>Ripetiamo questa procedura 10 volte.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>flip_coin <span class="ot">&lt;-</span> <span class="cf">function</span>(M) {</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    y[m] <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  estimate <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">/</span> M</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;estimated Pr[Y = 1] =&quot;</span>, estimate, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">flip_coin</span>(<span class="dv">100</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.44 </span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.52 </span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.46 </span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.57 </span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.47 </span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.46 </span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.48 </span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.49 </span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.47 </span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.62</span></span></code></pre></div>
<p>Dato che la moneta è equilibrata, la stima delle probabilità dell’evento <span class="math inline">\(Pr[Y = 1]\)</span> è simile a al valore che ci aspettiamo, ovvero <span class="math inline">\(P(Y = 1)\)</span> = 0.5, ma il risultato ottenuto nelle simulazioni non è sempre esatto. Proviamo ad aumentare il numero di lanci in ciascuna simulazione:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">flip_coin</span>(<span class="dv">1000</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.497 </span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.529 </span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.493 </span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.511 </span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.506 </span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.52 </span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.49 </span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.495 </span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.489 </span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.496</span></span></code></pre></div>
<p>In questo secondo caso, gli errori tendono ad essere più piccoli che nel caso precedente. Cosa succede se in ciascuna simulazione esaminiamo i risultati di 10,000 lanci della moneta?</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">flip_coin</span>(<span class="fl">1e4</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4885 </span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4957 </span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4902 </span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.5032 </span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.5048 </span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4931 </span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4965 </span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.499 </span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4979 </span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; estimated Pr[Y = 1] = 0.4973</span></span></code></pre></div>
<p>Ora le stime ottenute sono molto vicine alla vera probabilità che vogliamo stimare (cioè 0.5, perché la moneta è equilibrata). I risultati delle simulazioni precedenti pongono dunque il problema di determinare quale sia il numero di lanci di cui abbiamo bisogno per assicurarci che le stime siano accurate (ovvero, vicine al valore corretto della probabilità)</p>
</div>
<div id="la-legge-dei-grandi-numeri" class="section level2 hasAnchor" number="6.5">
<h2 class="hasAnchor"><span class="header-section-number">6.5</span> La legge dei grandi numeri<a href="#la-legge-dei-grandi-numeri" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La visualizzazione mediante grafici contribuisce alla comprensione dei concetti della statistica e della teoria delle probabilità. Un modo per descrivere ciò che accade all’aumentare del numero <span class="math inline">\(M\)</span> di ripetizioni del lancio della moneta consiste nel registrare la stima della probabilità dell’evento <span class="math inline">\(P(Y = 1)\)</span> in funzione del numero di ripetizioni dell’esperimento casuale per ogni <span class="math inline">\(m \in 1:M\)</span>. Possiamo ottenere un grafico dell’andamento della stima di <span class="math inline">\(P(Y = 1)\)</span> in funzione di <span class="math inline">\(m\)</span> nel modo seguente:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>nrep <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>estimate <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nrep)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>flip_coin <span class="ot">&lt;-</span> <span class="cf">function</span>(m) {</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(m, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  phat <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">/</span> m</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  phat</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nrep) {</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  estimate[i] <span class="ot">&lt;-</span> <span class="fu">flip_coin</span>(i)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span>nrep, </span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  estimate</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Numero di lanci della moneta&quot;</span>, </span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Stima di P(Y = 1)&quot;</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/legge-grandi-n-1-1.png" alt="Stima della probabilità di successo in funzione del numero dei lanci di una moneta." width="576" />
<p class="caption">
(#fig:legge-grandi-n-1)Stima della probabilità di successo in funzione del numero dei lanci di una moneta.
</p>
</div>
<p>Dato che la figura @ref(fig:legge-grandi-n-1) espressa su una scala lineare non rivela chiaramente l’andamento della simulazione, imponiamo una scala logaritmica sull’asse delle ascisse (<span class="math inline">\(x\)</span>). Su scala logaritmica, i valori tra 1 e 10 vengono tracciati all’incirca con la stessa ampiezza che si osserva tra i valori 50 e 700, eccetera.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">yintercept =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">size =</span> <span class="dv">1</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>      <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">200</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>      <span class="dv">700</span>, <span class="dv">2500</span>, <span class="dv">10000</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Numero dei lanci della moneta (scala logaritmica)&quot;</span>,</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Stima di P(Y = 1)&quot;</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/legge-grandi-n-2-1.png" alt="Stima della probabilità di successo in funzione del numero dei lanci di una moneta." width="576" />
<p class="caption">
(#fig:legge-grandi-n-2)Stima della probabilità di successo in funzione del numero dei lanci di una moneta.
</p>
</div>
<p>La <em>legge dei grandi numeri</em> ci dice che, all’aumentare del numero di ripetizioni dell’esperimento casuale, la media dei risultati ottenuti tende al valore atteso, man mano che vengono eseguite più prove. Nella figura @ref(fig:legge-grandi-n-2) vediamo infatti che, all’aumentare del numero <em>M</em> di lanci della moneta, la stima di <span class="math inline">\(P(Y = 1)\)</span> converge al valore 0.5.</p>
</div>
<div id="variabili-casuali-multiple" class="section level2 hasAnchor" number="6.6">
<h2 class="hasAnchor"><span class="header-section-number">6.6</span> Variabili casuali multiple<a href="#variabili-casuali-multiple" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le variabili casuali non esistono isolatamente. Abbiamo iniziato con una sola variabile casuale <span class="math inline">\(Y\)</span> che rappresenta il risultato di un singolo, specifico lancio di una moneta equlibrata. Ma supponiamo ora di lanciare la moneta tre volte. I risultati di ciascuno dei tre lanci possono essere rappresentati da una diversa variabile casuale, ad esempio, <span class="math inline">\(Y_1 , Y_2 , Y_3\)</span>. Possiamo assumere che ogni lancio sia indipendente, ovvero che non dipenda dal risultato degli altri lanci. Per ciascuna di queste variabili <span class="math inline">\(Y_n\)</span>, con <span class="math inline">\(n \in 1:3\)</span>, abbiamo che <span class="math inline">\(P(Y_n =1)=0.5\)</span> e <span class="math inline">\(P(Y_n =0)=0.5\)</span>.</p>
<p>È possibile combinare più variabili casuali usando le operazioni aritmetiche. Se <span class="math inline">\(Y_1 , Y_2, Y_3\)</span> sono variabili casuali che rappresentano tre lanci di una moneta equilibrata (o un lancio di tre monete equilibrate), possiamo definire la somma di tali variabili casuali come</p>
<p><span class="math display">\[
Z = Y_1 + Y_2 + Y_3.
\]</span></p>
<p>Possiamo simulare i valori assunti dalla variabile casuale <em>Z</em> simulando i valori di <span class="math inline">\(Y_1, Y_2, Y_3\)</span> per poi sommarli.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>y3 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(y1, y2, y3)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1 0 1</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">c</span>(y1, y2, y3))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;z =&quot;</span>, z, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; z = 2</span></span></code></pre></div>
<p>ovvero,</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">3</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  y[i] <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>y</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0 1 1</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;z =&quot;</span>, z, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; z = 2</span></span></code></pre></div>
<p>oppure, ancora più semplicemente:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>y</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1 0 1</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;z =&quot;</span>, z, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; z = 2</span></span></code></pre></div>
<p>Possiamo ripetere questa simulazione <span class="math inline">\(M = 1e5\)</span> volte:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  z[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>e calcolare una stima della probabilità che la variabile casuale <span class="math inline">\(Z\)</span> assuma i valori 0, 1, 2, 3:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(z) <span class="sc">/</span> M</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; z</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      0      1      2      3 </span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.1258 0.3750 0.3748 0.1244</span></span></code></pre></div>
<p>Nel caso di 4 monete equilibrate, avremo:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">4</span>, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  z[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(z) <span class="sc">/</span> M</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; z</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       0       1       2       3       4 </span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.06340 0.24917 0.37360 0.25022 0.06361</span></span></code></pre></div>
<p>Una variabile casuale le cui modalità possono essere costituite solo da numeri interi è detta <em>variabile casuale discreta</em>:</p>
<p><span class="math display">\[
\mathbb{Z} = \dots, -2, -1, 0, 1, 2, \dots
\]</span></p>
</div>
<div id="sec:fun-mass-prob" class="section level2 hasAnchor" number="6.7">
<h2 class="hasAnchor"><span class="header-section-number">6.7</span> Funzione di massa di probabilità<a href="#sec:fun-mass-prob" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>È conveniente avere una funzione che associa una probabilità a ciascun possibile valore di una variabile casuale. In generale, ciò è possibile se e solo se la variabile casuale è discreta, così com’è stata definita nel Paragrafo precedente. Ad esempio, se consideriamo <span class="math inline">\(Z = Y_1 + \dots + Y_4\)</span> come il numero di risultati “testa” in 4 lanci della moneta, allora possiamo definire la seguente funzione:</p>
<p><span class="math display">\[
\begin{array}{rclll}
p_Z(0) &amp; = &amp; 1/16 &amp; &amp; \mathrm{TTTT}
\\
p_Z(1) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HTTT, THTT, TTHT, TTTH}
\\
p_Z(2) &amp; = &amp; 6/16 &amp; &amp; \mathrm{HHTT, HTHT, HTTH, THHT, THTH, TTTH}
\\
p_Z(3) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HHHT, HHTH, HTHH, THHH}
\\
p_Z(4) &amp; = &amp; 1/16 &amp; &amp; \mathrm{HHHH}
\end{array}
\]</span></p>
<p>Il lancio di quattro monete può produrre sedici possibili risultati. Dato che i lanci sono indipendenti e le monete sono equilibrate, ogni possibile risultato è ugualmente probabile. Nella tabella in alto, le sequenze dei risultati possibili del lancio delle 4 monete sono riportate nella colonna più a destra. Le probabilità si ottengono dividendo il numero di sequenze che producono lo stesso numero di eventi testa per il numero dei risultati possibili.</p>
<p>La funzione <span class="math inline">\(p_Z\)</span> è stata costruita per associare a ciascun valore <span class="math inline">\(u\)</span> della variabile casuale <span class="math inline">\(Z\)</span> la probabilità dell’evento <span class="math inline">\(Z = u\)</span>. Convenzionalmente, queste probabilità sono scritte come</p>
<p><span class="math display">\[
P_Z(z) = \mbox{P}(Z = z).
\]</span></p>
<p>La parte a destra dell’uguale si può leggere come: “la probabilità che la variabile casuale <span class="math inline">\(Z\)</span> assuma il valore <span class="math inline">\(z\)</span>”. Una funzione definita come sopra è detta <em>funzione di massa di probabilità</em> della variabile casuale <span class="math inline">\(Z\)</span>. Ad ogni variabile casuale discreta è associata un’unica funzione di massa di probabilità.</p>
<p>Una rappresentazione grafica della stima della funzione di massa di probabilità per l’esperimento casuale del lancio di quattro monete equilibrate è fornita nella figura @ref(fig:barplot-mdf-4coins).</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>nflips <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(M, nflips, <span class="fl">0.5</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span>nflips</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nflips <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>nflips) {</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  y[n <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(u <span class="sc">==</span> n) <span class="sc">/</span> M</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>bar_plot <span class="ot">&lt;-</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">Z =</span> x, <span class="at">count =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Z, <span class="at">y =</span> count)) <span class="sc">+</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>,</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Probabilità stimata P(Z = z)&quot;</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>bar_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/barplot-mdf-4coins-1.png" alt="Grafico di $M = 100,000$ simulazioni della funzione di massa di probabilità di una variabile casuale definita come il numero di teste in quattro lanci di una moneta equilibrata." width="576" />
<p class="caption">
(#fig:barplot-mdf-4coins)Grafico di <span class="math inline">\(M = 100,000\)</span> simulazioni della funzione di massa di probabilità di una variabile casuale definita come il numero di teste in quattro lanci di una moneta equilibrata.
</p>
</div>
<p>Se <span class="math inline">\(A\)</span> è un sottoinsieme della variabile casuale <span class="math inline">\(Z\)</span>, allora denotiamo
con <span class="math inline">\(P_{z}(A)\)</span> la probabilità assegnata ad <span class="math inline">\(A\)</span> dalla distribuzione
<span class="math inline">\(P_{z}\)</span>. Mediante una distribuzione di probabilità <span class="math inline">\(P_{z}\)</span> è dunque
possibile determinare la probabilità di ciascun sottoinsieme
<span class="math inline">\(A \subset Z\)</span> come</p>
<p><span class="math display">\[\begin{equation}
P_{z}(A) = \sum_{z \in A} P_{z}(Z).
\end{equation}\]</span></p>
<p>Una funzione di massa di probabilità soddisfa le proprietà <span class="math inline">\(0 \leq P(X=x) \leq 1\)</span> e <span class="math inline">\(\sum_{x \in X} P(x) = 1\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>(#exm:unlabeled-div-25) </strong></span>Nel caso dell’esempio discusso nel Paragrafo @ref(sec:fun-mass-prob), la probabilità che la variabile casuale <span class="math inline">\(Z\)</span> sia un numero dispari è</p>
<p><span class="math display">\[
P(\text{Z è un numero dispari}) = P_{z}(Z = 1) + P_{z}(Z = 3) = \frac{4}{16} + \frac{4}{16} = \frac{1}{2}.
\]</span></p>
</div>
<div id="funzione-di-ripartizione" class="section level3 hasAnchor" number="6.7.1">
<h3 class="hasAnchor"><span class="header-section-number">6.7.1</span> Funzione di ripartizione<a href="#funzione-di-ripartizione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data una variabile casuale discreta <span class="math inline">\(X\)</span> possiamo calcolare la probabilità che <span class="math inline">\(X\)</span> non superi un certo valore <span class="math inline">\(x\)</span>, ossia la sua <em>funzione di ripartizione</em>. Poichè <span class="math inline">\(X\)</span> assume valori discreti possiamo cumulare le probabilità mediante una somma:</p>
<p><span class="math display">\[\begin{equation}
F(x_k) = P(X \leq x_k) = \sum_{x \leq x_k} P(x).
\end{equation}\]</span></p>
</div>
</div>
<div id="commenti-e-considerazioni-finali-4" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In questo capitolo abbiamo visto come si costruisce lo spazio campione di un esperimento casuale, quali sono le proprietà di base della probabilità e come si assegnano le probabilità agli eventi definiti sopra uno spazio campione discreto. Abbiamo anche introdotto le nozioni di variabile casuale, ovvero di una variabile che assume i suoi valori in maniera casuale. Abbiamo descritto il modo di specificare la probabilità con cui sono una variabile casuale assume i suoi differenti valori, ovvero la funzione di ripartizione <span class="math inline">\(F(X) = P(X &lt; x)\)</span> e la funzione di massa di probabilità.</p>
<!--chapter:end:015_prob_intro.Rmd-->
</div>
</div>
<div id="chapter-prob-cond" class="section level1 hasAnchor" number="7">
<h1 class="hasAnchor"><span class="header-section-number">7</span> Probabilità condizionata<a href="#chapter-prob-cond" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Il fondamento della statistica bayesiana è il teorema di Bayes e il teorema di Bayes è una semplice ridescrizione della probabilità condizionata. Esaminiamo dunque la probabilità condizionata.</p>
<div id="sec:bayes-cancer" class="section level2 hasAnchor" number="7.1">
<h2 class="hasAnchor"><span class="header-section-number">7.1</span> Probabilità condizionata su altri eventi<a href="#sec:bayes-cancer" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>L’attribuzione di una probabilità ad un evento è sempre condizionata dalle conoscenze che abbiamo a disposizione. Per un determinato stato di conoscenze, attribuiamo ad un dato evento una certa probabilità di verificarsi; ma se il nostro stato di conoscenze cambia, allora cambierà anche la probabilità che attribuiremo all’evento in questione. Infatti, si può pensare che tutte le probabilità siano probabilità condizionate, anche se l’evento condizionante non è sempre esplicitamente menzionato.</p>
<p>Per introdurre la probabilità condizionata, <span class="citation">Albert and Hu (<a href="#ref-albert2019probability" role="doc-biblioref">2019</a>)</span> utilizzando il famoso paradosso delle tre carte. “Ci sono tre carte, delle quali la prima (<span class="math inline">\(A\)</span>) è rossa su entrambi i lati, la seconda (<span class="math inline">\(B\)</span>) su un lato è rossa e sull’altro è bianca e la terza (<span class="math inline">\(C\)</span>) è bianca su entrambi i lati. Ponendo su un tavolo una delle tre carte, scelta a caso, ottengo che il lato visibile è di colore rosso. Qual è la probabilità che anche il lato non visibile sia di colore rosso? La risposta intuitiva porta solitamente a rispondere che la probabilità ricercata sia pari al 50%, in quanto solo due carte (la <span class="math inline">\(A\)</span> e la <span class="math inline">\(B\)</span>) possono mostrare il colore rosso e solo una di queste (la <span class="math inline">\(A\)</span>) può mostrare anche sull’altro lato il colore rosso; tuttavia si dimostra che la risposta giusta è 2/3.” (da Wikipedia)</p>
<p><span class="citation">Albert and Hu (<a href="#ref-albert2019probability" role="doc-biblioref">2019</a>)</span> propongono di risolvere il problema con una simulazione in <span class="math inline">\(\textsf{R}\)</span>: prima di tutto si sceglie una carta a caso, e poi si sceglie un lato della carta. Ci sono tre carte possibili, che chiamiamo “c_rossa”, “c_bianca”, e “c_entrambi”. Per la carta rossa, ci sono due lati rossi; per la carta bianca ci sono due lati bianchi e la carta “entrambi” ha un lato rosso e un lato bianco.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Carta =</span> <span class="fu">c</span>(</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;c_rossa&quot;</span>, <span class="st">&quot;c_rossa&quot;</span>, <span class="st">&quot;c_bianca&quot;</span>, <span class="st">&quot;c_bianca&quot;</span>, <span class="st">&quot;c_entrambi&quot;</span>, </span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;c_entrambi&quot;</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lato =</span> <span class="fu">c</span>(</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;rosso&quot;</span>, <span class="st">&quot;rosso&quot;</span>, <span class="st">&quot;bianco&quot;</span>, <span class="st">&quot;bianco&quot;</span>, <span class="st">&quot;rosso&quot;</span>, <span class="st">&quot;bianco&quot;</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 6 × 2</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Carta      Lato  </span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt;      &lt;chr&gt; </span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 c_rossa    rosso </span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 c_rossa    rosso </span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 c_bianca   bianco</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4 c_bianca   bianco</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5 c_entrambi rosso </span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 c_entrambi bianco</span></span></code></pre></div>
<p>Estraiamo una carta a caso e classifichiamo il risultato ottenuto in base al tipo di carta e lato osservato. Ripetiamo l’esperimento 1,000 volte:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">84735</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>carte <span class="ot">&lt;-</span> <span class="fu">sample_n</span>(df, <span class="fl">1e3</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(carte<span class="sc">$</span>Carta, carte<span class="sc">$</span>Lato)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             </span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              bianco rosso</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   c_bianca      353     0</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   c_entrambi    143   160</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   c_rossa         0   344</span></span></code></pre></div>
<p>Se si osserva il colore rosso (seconda colonna nella tabella precedente), questo risultato è dovuto ad una carta <span class="math inline">\(A\)</span> (“rossa”) in 344 casi e ad una carta <span class="math inline">\(B\)</span> (“entrambi”) in 160 casi. Quindi, nella simulazione il risultato per cui è stato osservato un colore rosso (344 + 160) è associato ad una carta <span class="math inline">\(A\)</span> (“rossa”) in circa 2/3 dei casi – se il lato visibile è di colore rosso, allora c’è una probabilità di 2/3 che anche il lato non visibile sia di colore rosso.</p>
<p>Questo esempio dimostra come le nostre intuizioni a proposito della probabilità condizionata non sono sempre corrette. Consideriamo un altro problema più articolato.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-26" class="exercise"><strong>(#exr:unlabeled-div-26) </strong></span>Supponiamo che lo screening per la diagnosi precoce del tumore mammario si avvalga di un test che è accurato al 90%, nel senso che classifica correttamente il 90% delle donne colpite dal cancro e il 90% delle donne che non hanno il cancro al seno. Supponiamo che l’1% delle donne sottoposte allo screening abbia effettivamente il cancro al seno (e d’altra parte, il 99% non lo ha). Ci chiediamo: (1) qual è la probabilità che una donna scelta a caso ottenga una mammografia positiva, e (2) se la mammografia è positiva, qual è la probabilità che vi sia effettivamente un tumore al seno?</p>
<p>Per risolvere questo problema, supponiamo che il test in questione venga somministrato ad un grande campione di donne, diciamo a 1000 donne. Di queste 1000 donne, 10 (ovvero, l’1%) hanno il cancro al seno. Per queste 10 donne, il test darà un risultato positivo in 9 casi (ovvero, nel 90% dei casi). Per le rimanenti 990 donne che non hanno il cancro al seno, il test darà un risultato positivo in 99 casi (se la probabilità di un vero positivo è del 90%, la probabilità di un falso positivo è del 10%). Questa situazione è rappresentata nella figura @ref(fig:mammografia).</p>
<p>Combinando i due risultati precedenti, vediamo che il test dà un risultato positivo per 9 donne che hanno effettivamente il cancro al seno e per 99 donne che non ce l’hanno, per un totale di 108 risultati positivi. Dunque, la probabilità di ottenere un risultato positivo al test è <span class="math inline">\(\frac{108}{1000}\)</span> = 11%. Ma delle 108 donne che hanno ottenuto un risultato positivo al test, solo 9 hanno il cancro al seno. Dunque, la probabilità di essere una donna che ha veramente il cancro al seno, dato un risultato positivo al test, è pari a <span class="math inline">\(\frac{9}{108}\)</span> = 8%.</p>
<div class="figure" style="text-align: center">
<img src="images/mammografia.png" alt="Rappresentazione ad albero che riporta le frequenze attese dei risultati di una mammografia in un campione di 1,000 donne." width="77%" />
<p class="caption">
(#fig:mammografia)Rappresentazione ad albero che riporta le frequenze attese dei risultati di una mammografia in un campione di 1,000 donne.
</p>
</div>
</div>
<p>Nell’esercizio precedente, la probabilità dell’evento “ottenere un risultato positivo al test” è una probabilità non condizionata, mentre la probabilità dell’evento “avere il cancro al seno, dato che il test ha prodotto un risultato positivo” è una probabilità condizionata.</p>
<p>In termini generali, la probabilità condizionata <span class="math inline">\(P(A \mid B)\)</span> rappresenta la probabilità che si verifichi l’evento <span class="math inline">\(A\)</span> sapendo che si è verificato l’evento <span class="math inline">\(B\)</span>. Arriviamo dunque alla seguente definizione.</p>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>(#def:unlabeled-div-27) </strong></span>Dato un qualsiasi evento <span class="math inline">\(A\)</span>, si chiama <em>probabilità condizionata</em> di
<span class="math inline">\(A\)</span> dato <span class="math inline">\(B\)</span> il numero</p>
<p><span class="math display">\[\begin{equation}
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad \text{con}\, P(B) &gt; 0,
(\#eq:probcond)
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(P(A\cap B)\)</span> è la <em>probabilità congiunta</em> dei due eventi, ovvero la probabilità che si verifichino entrambi.</p>
</div>
<p>Concludiamo con un problema molto semplice per consolidare la nostra comprensione del concetto di probabilità condizionata.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-28" class="exercise"><strong>(#exr:unlabeled-div-28) </strong></span>Da un mazzo di 52 carte (13 carte per ciascuno dei 4 semi) ne viene estratta una in modo casuale. Qual è la probabilità che esca una figura di cuori? Sapendo che la carta estratta ha il seme di cuori, qual è la probabilità che il valore numerico della carta sia 7, 8 o 9?</p>
<p>Ci sono 13 carte di cuori, dunque la risposta alla prima domanda è 1/4 (probabilità non condizionata). Per rispondere alla seconda domanda consideriamo solo le 13 carte di cuori; la probabilità cercata è dunque 3/13 (probabilità condizionata).</p>
</div>
<!-- ### La fallacia del condizionale trasposto -->
<!-- Un errore comune che si commette è quello di credere che $P(A \mid B)$ sia uguale a $P(B \mid A)$. Tale fallacia ha particolare risalto in ambito forense tanto che è conosciuta con il nome di "fallacia del procuratore". In essa, una piccola probabilità dell'evidenza, data l'innocenza, viene erroneamente interpretata come la probabilità dell'innocenza, data l'evidenza. -->
<!-- Consideriamo il caso di un esame del DNA. Un esperto forense potrebbe affermare, ad esempio, che "se l'imputato è innocente, c'è solo una possibilità su un miliardo che vi sia una corrispondenza tra il suo DNA e il DNA trovato sulla scena del crimine". Ma talvolta questa probabilità è erroneamente interpretata come avesse il seguente significato: "date le prove del DNA, c'è solo una possibilità su un miliardo che l'imputato sia innocente". -->
<!-- Le considerazioni precedenti risultano più chiare se facciamo nuovamente riferimento all'esercizio sul tumore mammario descritto sopra. In tale esercizio abbiamo visto come la probabilità di cancro dato un risultato positivo al test sia uguale a 0.08. Tale probabilità è molto diversa dalla probabilità di un risultato positivo al test data la presenza del cancro. Infatti, questa seconda probabilità è uguale a 0.90 ed è descritta nel problema come una delle caratteristiche del test in questione. -->
</div>
<div id="la-regola-moltiplicativa" class="section level2 hasAnchor" number="7.2">
<h2 class="hasAnchor"><span class="header-section-number">7.2</span> La regola moltiplicativa<a href="#la-regola-moltiplicativa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dalla definizione di probabilità condizionata è possibile esprimere la probabilità congiunta tramite le condizionate. La <em>regola moltiplicativa</em> (o <em>legge delle probabilità composte</em>, o <em>regola della catena</em>) afferma che la probabilità che si verifichino due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> è pari alla probabilità di uno dei due eventi moltiplicato con la probabilità dell’altro evento condizionato al verificarsi del primo:</p>
<p><span class="math display">\[\begin{equation}
P(A \cap B) = P(B)P(A \mid B) = P(A)P(B \mid A).
(\#eq:probcondinv)
\end{equation}\]</span></p>
<p>La @ref(eq:probcondinv) si estende al caso di <span class="math inline">\(n\)</span> eventi <span class="math inline">\(A_1, \dots, A_n\)</span> nella forma seguente:</p>
<p><span class="math display">\[\begin{equation}
P\left( \bigcap_{k=1}^n A_k \right) = \prod_{k=1}^n \left(  A_k  \ \Biggl\lvert \ \bigcap_{j=1}^{k-1} A_j \right)
(\#eq:probcomposte)
\end{equation}\]</span></p>
<p>Per esempio, nel caso di quattro eventi abbiamo</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
P(A_1 \cap A_2 \cap A_3 \cap A_4) = {}&amp; P(A_1) \cdot P(A_2 \mid A_1) \cdot  P(A_3 \mid A_1 \cap A_2) \cdot \\
&amp; P(A_4 \mid A_1 \cap A_2 \cap A_{3}).\notag
\end{split}
\end{equation}\]</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-29" class="exercise"><strong>(#exr:unlabeled-div-29) </strong></span>Da un’urna contenente 6 palline bianche e 4 nere si estrae una pallina per volta, senza reintrodurla nell’urna. Indichiamo con <span class="math inline">\(B_i\)</span> l’evento: “esce una pallina bianca alla <span class="math inline">\(i\)</span>-esima estrazione” e con <span class="math inline">\(N_i\)</span> l’estrazione di una pallina nera. L’evento: “escono due palline bianche nelle prime due estrazioni” è rappresentato dalla intersezione
<span class="math inline">\(\{B_1 \cap B_2\}\)</span> e la sua probabilità vale, per la @ref(eq:probcondinv)</p>
<p><span class="math display">\[
P(B_1 \cap B_2) = P(B_1)P(B_2 \mid B_1).
\]</span></p>
<p><span class="math inline">\(P(B_1)\)</span> vale 6/10, perché nella prima estrazione <span class="math inline">\(\Omega\)</span> è costituito da 10 elementi: 6 palline bianche e 4 nere. La probabilità condizionata <span class="math inline">\(P(B_2 \mid B_1)\)</span> vale 5/9, perché nella seconda estrazione, se è verificato l’evento <span class="math inline">\(B_1\)</span>, lo spazio campionario consiste di 5 palline bianche e 4 nere. Si ricava
pertanto:</p>
<p><span class="math display">\[
P(B_1 \cap B_2) = \frac{6}{10} \cdot \frac{5}{9} = \frac{1}{3}.
\]</span></p>
<p>In modo analogo si ha che</p>
<p><span class="math display">\[
P(N_1 \cap N_2) = P(N_1)P(N_2 \mid N_1) = \frac{4}{10} \cdot \frac{3}{9} = \frac{4}{30}.
\]</span></p>
<p>Se l’esperimento consiste nell’estrazione successiva di 3 palline, la probabilità che queste siano tutte bianche vale, per la @ref(eq:probcomposte):</p>
<p><span class="math display">\[
P(B_1 \cap B_2 \cap B_3)=P(B_1)P(B_2 \mid B_1)P(B_3 \mid B_1 \cap B_2),
\]</span></p>
<p>dove la probabilità <span class="math inline">\(P(B_3 \mid B_1 \cap B_2)\)</span> si calcola supponendo che si sia verificato l’evento condizionante <span class="math inline">\(\{B_1 \cap B_2\}\)</span>. Lo spazio campionario per questa probabilità condizionata è costituito da 4 palline bianche e 4 nere, per cui <span class="math inline">\(P(B_3 \mid B_1 \cap B_2) = 1/2\)</span> e quindi:</p>
<p><span class="math display">\[
P (B_1 \cap B_2 \cap B_3) = \frac{6}{10}\cdot\frac{5}{9} \cdot\frac{4}{8}  = \frac{1}{6}.
\]</span></p>
<p>La probabilità dell’estrazione di tre palline nere è invece:</p>
<p><span class="math display">\[
\begin{aligned}
P(N_1 \cap N_2 \cap N_3) &amp;= P(N_1)P(N_2 \mid N_1)P(N_3 \mid N_1 \cap N_2)\notag\\
&amp;= \frac{4}{10} \cdot \frac{3}{9} \cdot \frac{2}{8} = \frac{1}{30}.\notag
\end{aligned}
\]</span></p>
</div>
</div>
<div id="lindipendendenza-stocastica" class="section level2 hasAnchor" number="7.3">
<h2 class="hasAnchor"><span class="header-section-number">7.3</span> L’indipendendenza stocastica<a href="#lindipendendenza-stocastica" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Un concetto molto importante per le applicazioni statistiche della probabilità è quello dell’indipendenza stocastica. La definizione @ref(eq:probcond) consente di esprimere il concetto di indipendenza di un evento da un altro in forma intuitiva: se <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> sono eventi indipendenti, allora il verificarsi di <span class="math inline">\(A\)</span> non influisce sulla probabilità del verificarsi di <span class="math inline">\(B\)</span>, ovvero non la condiziona, e il verificarsi di <span class="math inline">\(B\)</span> non influisce sulla probabilità del verificarsi di <span class="math inline">\(A\)</span>. Infatti, per la @ref(eq:probcond), si ha che, se <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> sono due eventi indipendenti, risulta:</p>
<p><span class="math display">\[
P(A \mid B) = \frac{P(A)P(B)}{P(B)} = P(A),
\]</span></p>
<p><span class="math display">\[
P(B \mid A) = \frac{P(A)P(B)}{P(A)} = P(B).
\]</span></p>
<p>Possiamo dunque dire che due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> sono indipendenti se</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
P(A \mid B) &amp;= P(A), \\
P(B \mid A) &amp;= P(B).
\end{split}
\end{equation}\]</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-30" class="exercise"><strong>(#exr:unlabeled-div-30) </strong></span>Nel lancio di due dadi non truccati, si considerino gli eventi: <span class="math inline">\(A\)</span> = {esce un 1 o un 2 nel primo lancio} e <span class="math inline">\(B\)</span> = {il punteggio totale è 8}. Gli eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> sono indipendenti?</p>
<p>Rappresentiamo qui sotto lo spazio campione dell’esperimento casuale.</p>
<div class="figure" style="text-align: center">
<img src="images/sampling-space-dice.png" alt="Rappresentazione dello spazio campionario dei risultati dell'esperimento casuale corrispondente al lancio di due dadi bilanciati. Sono evidenziati gli eventi elementari che costituiscono l'evento $B$: 'il punteggio totale è 8'." width="70%" />
<p class="caption">
(#fig:sampling-space-dice)Rappresentazione dello spazio campionario dei risultati dell’esperimento casuale corrispondente al lancio di due dadi bilanciati. Sono evidenziati gli eventi elementari che costituiscono l’evento <span class="math inline">\(B\)</span>: ‘il punteggio totale è 8’.
</p>
</div>
<p>Gli eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> non sono statisticamente indipendenti. Infatti, le loro probabilità valgono <span class="math inline">\(P(A) = 12/36\)</span> e <span class="math inline">\(P(B) = 5/36\)</span> e la probabilità della loro intersezione è</p>
<p><span class="math display">\[
P(A \cap B) = 1/36 = 3/108 \neq P(A)P(B) = 5/108.
\]</span></p>
</div>
<div class="remark">
<p><span id="unlabeled-div-31" class="remark"><em>Remark</em>. </span>Il concetto di indipendenza è del tutto differente da quello di incompatibilità. Si noti infatti che due eventi <em>A</em> e <em>B</em> incompatibili (per i quali si ha <span class="math inline">\(A \cap B = \emptyset\)</span>) sono statisticamente dipendenti, poiché il verificarsi dell’uno esclude il verificarsi dell’altro: <span class="math inline">\(P(A \cap B)=0 \neq P(A)P(B)\)</span>.</p>
</div>
</div>
<div id="il-teorema-della-probabilità-totale" class="section level2 hasAnchor" number="7.4">
<h2 class="hasAnchor"><span class="header-section-number">7.4</span> Il teorema della probabilità totale<a href="#il-teorema-della-probabilità-totale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dato un insieme finito <span class="math inline">\(A_i\)</span> di eventi, nel calcolo della probabilità dell’unione di tutti gli eventi, se gli eventi considerati non sono a due a due incompatibili, si deve tenere conto delle loro intersezioni. In particolare, la probabilità dell’unione di due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> è pari alla somma delle singole probabilità <span class="math inline">\(P(A)\)</span> e <span class="math inline">\(P(B)\)</span> diminuita della probabilità della loro intersezione:</p>
<p><span class="math display">\[\begin{equation}
P(A \cup B) = P(A) + P(B) - P(A \cap B).
(\#eq:probunione)
\end{equation}\]</span></p>
<p>Nel caso di tre eventi, si ha</p>
<p><span class="math display">\[
\begin{split}
P(A \cup B \cup C) &amp;= P(A)+P(B)+P(C)-P(A\cap B)-P(A\cap C) - \\
&amp; \qquad P(B\cap C) + P(A\cap B\cap C).
\end{split}
\]</span></p>
<p>La formula per il caso di <span class="math inline">\(n\)</span> eventi si ricava per induzione.</p>
<p>Per il caso di due soli eventi, se <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> sono indipendenti, la @ref(eq:probunione) si modifica nella relazione seguente:</p>
<p><span class="math display">\[\begin{equation}
P(A \cup B) = P(A) + P(B) - P(A)P(B).
\end{equation}\]</span></p>
<p>Nel caso di due eventi <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> incompatibili, se cioè <span class="math inline">\(P(A \cap B) = \varnothing\)</span>, si ha che</p>
<p><span class="math display">\[
A\cap B=\varnothing \Rightarrow P(A\cup B)=P(A)+P(B).
\]</span></p>
<p>Si può dimostrare per induzione che ciò vale anche per un insieme finito di eventi <span class="math inline">\(A_{n}\)</span> a due a due incompatibili, ovvero che:</p>
<p><span class="math display">\[
A_i\cap A_j=\varnothing, i\neq j \Rightarrow P\left(\bigcup_{i=1}^n A_i\right)=\sum_{i=1}^nP(A_i).
\]</span></p>
</div>
<div id="il-teorema-della-probabilità-assoluta" class="section level2 hasAnchor" number="7.5">
<h2 class="hasAnchor"><span class="header-section-number">7.5</span> Il teorema della probabilità assoluta<a href="#il-teorema-della-probabilità-assoluta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Il teorema della probabilità assoluta consente di calcolare la probabilità di un evento <span class="math inline">\(E\)</span> di cui sono note le probabilità condizionate rispetto ad altri eventi <span class="math inline">\((H_i)_{i\geq 1}\)</span>, a condizione che essi costituiscano una partizione dell’evento certo <span class="math inline">\(\Omega\)</span>, ovvero</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\bigcup_{i=1}^\infty H_i = \Omega\)</span>;</li>
<li><span class="math inline">\(H_j \cap H_j = \emptyset, i\neq j\)</span>;</li>
<li><span class="math inline">\(P(H_i) &gt; 0, i = 1, \dots, \infty\)</span>.</li>
</ol>
<p>Nel caso di una partizione dello spazio campione in <span class="math inline">\(n\)</span> sottoinsiemi abbiamo</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(E)=\sum _{{i=1}}^{n}{\mbox{P}}(H_{i}\cap E)=\sum _{{i=1}}^{n}{\mbox{P}}(H_{i}){\mbox{P}}(E \mid H_{i}).
\end{equation}\]</span></p>
<p>Consideriamo, ad esempio, una partizione dell’evento certo in tre sottoinsiemi.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/tikz-prob-tot-1.png" alt="Partizione dell'evento certo $\Omega$ in tre sottoinsiemi sui quali viene definito l'evento $E$." width="576" />
<p class="caption">
(#fig:tikz-prob-tot)Partizione dell’evento certo <span class="math inline">\(\Omega\)</span> in tre sottoinsiemi sui quali viene definito l’evento <span class="math inline">\(E\)</span>.
</p>
</div>
<p>In tali circostanze si ha che</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(E) = {\mbox{P}}(E \cap H_1) + {\mbox{P}}(E \cap H_2) + {\mbox{P}}(E \cap H_3), \notag
(\#eq:prob-total-1a)
\end{equation}\]</span></p>
<p>ovvero</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(E) = {\mbox{P}}(E \mid H_1) {\mbox{P}}(H_1) + {\mbox{P}}(E \mid H_2) {\mbox{P}}(H_2) + {\mbox{P}}(E \mid H_3) {\mbox{P}}(H_3).
(\#eq:prob-total-1b)
\end{equation}\]</span></p>
<p>In base al teorema della probabilità assoluta, dunque, se l’evento <span class="math inline">\(E\)</span> è costituito da tutti gli eventi elementari in <span class="math inline">\(E \cap H_1\)</span>, <span class="math inline">\(E \cap H_2\)</span> e <span class="math inline">\(E \cap H_3\)</span>, allora la sua probabilità è data dalla somma delle probabilità condizionate <span class="math inline">\(P(E \mid H_i)\)</span>, ciascuna delle quali pesata per la probabilità dell’evento condizionante <span class="math inline">\(H_i\)</span>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-32" class="exercise"><strong>(#exr:unlabeled-div-32) </strong></span>Si considerino tre urne, ciascuna delle quali contiene 100 palline:</p>
<ul>
<li>Urna 1: 75 palline rosse e 25 palline blu,</li>
<li>Urna 2: 60 palline rosse e 40 palline blu,</li>
<li>Urna 3: 45 palline rosse e 55 palline blu.</li>
</ul>
<p>
Una pallina viene estratta a caso da un’urna anch’essa scelta a caso.
Qual è la probabilità che la pallina estratta sia di colore rosso?</p>
<p>Sia <span class="math inline">\(R\)</span> l’evento “la pallina estratta è rossa” e sia <span class="math inline">\(U_i\)</span> l’evento che
corrisponde alla scelta dell’<span class="math inline">\(i\)</span>-esima urna. Sappiamo che</p>
<p><span class="math display">\[
{\mbox{P}}(R \mid U_1) = 0.75, \quad {\mbox{P}}(R \mid U_2) = 0.60, \quad {\mbox{P}}(R \mid U_3) = 0.45.
\]</span></p>
<p>Gli eventi <span class="math inline">\(U_1\)</span>, <span class="math inline">\(U_2\)</span> e <span class="math inline">\(U_3\)</span> costituiscono una partizione dello
spazio campione in quanto <span class="math inline">\(U_1\)</span>, <span class="math inline">\(U_2\)</span> e <span class="math inline">\(U_3\)</span> sono eventi
mutualmente esclusivi ed esaustivi, ovvero <span class="math inline">\({\mbox{P}}(U_1 \cup U_2 \cup U_3) = 1.0\)</span>. In base al teorema della probabilità assoluta, la probabilità di estrarre una pallina rossa è dunque</p>
<p><span class="math display">\[
\begin{split}
{\mbox{P}}(R) &amp;= {\mbox{P}}(R \mid U_1){\mbox{P}}(U_1)+{\mbox{P}}(R \mid U_2){\mbox{P}}(U_2)+{\mbox{P}}(R \mid U_3){\mbox{P}}(U_3) \\
&amp;= 0.75 \cdot \frac{1}{3}+0.60 \cdot \frac{1}{3}+0.45 \cdot \frac{1}{3} \\
&amp;=0.60.
\end{split}
\]</span></p>
</div>
</div>
<div id="indipendenza-condizionale" class="section level2 hasAnchor" number="7.6">
<h2 class="hasAnchor"><span class="header-section-number">7.6</span> Indipendenza condizionale<a href="#indipendenza-condizionale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Aggiungo qui delle considerazioni sul concetto di indipendenza condizionale a cui si farà riferimento nell’ultima parte della dispensa. L’indipendenza condizionale descrive situazioni in cui un’osservazione è irrilevante o ridondante quando si valuta la certezza di un’ipotesi. L’indipendenza condizionale è solitamente formulata nei termini della probabilità condizionata, come un caso speciale in cui la probabilità dell’ipotesi data un’osservazione non informativa è uguale alla probabilità senza tale osservazione non informativa.</p>
<p>Se <span class="math inline">\(A\)</span> è l’ipotesi e <span class="math inline">\(B\)</span> e <span class="math inline">\(C\)</span> sono osservazioni, l’indipendenza condizionale può essere espressa come l’uguaglianza:</p>
<p><span class="math display">\[
P(A \mid B,C)=P(A \mid C).
\]</span></p>
<p>Dato che <span class="math inline">\(P(A \mid B,C)\)</span> è uguale a <span class="math inline">\(P(A \mid C)\)</span>, questa uguaglianza corrisponde all’affermazione che <span class="math inline">\(B\)</span> non fornisce alcun contributo alla certezza di <span class="math inline">\(A\)</span>. In questo caso si dice che <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> condizionalmente indipendenti dato <span class="math inline">\(C\)</span>, scritto simbolicamente come: <span class="math inline">\((A \perp\!\!\!\!\perp B \mid C)\)</span>.</p>
<p>In maniera equivalente, l’indipendenza condizionale <span class="math inline">\((A \perp\!\!\!\!\perp B \mid C)\)</span> si verifica se:</p>
<p><span class="math display">\[
P(A, B \mid C) = P(A \mid C) P(B \mid C).
\]</span></p>
<p>Un esempio è il seguente (da Wikipedia). Siano due eventi le probabilità che le persone <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> tornino a casa in tempo per la cena, e il terzo evento è il fatto che una tempesta di neve ha colpito la città. Mentre sia <span class="math inline">\(A\)</span> che <span class="math inline">\(B\)</span> hanno una probabilità più piccola di tornare a casa in tempo per la cena di quando non c’è la neve, tali probabilità sono indipendenti l’una dall’altra. Cioè, sapere che <span class="math inline">\(A\)</span> è in ritardo non ci dice nulla sul fatto che <span class="math inline">\(B\)</span> sia in ritardo o meno – <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> potrebbero vivere in quartieri diversi, percorrere distanze diverse e utilizzare mezzi di trasporto diversi. Tuttavia, se sapessimo che <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> vivono nello stesso quartiere, usano lo stesso mezzo di trasporto e lavorano nello stesso luogo, allora i due eventi non sarebbero condizionatamente indipendenti.</p>
</div>
<div id="commenti-e-considerazioni-finali-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La probabilità condizionata è importante perché ci fornisce uno strumento per precisare il concetto di indipendenza statistica. Una delle domande più importanti delle analisi statistiche è infatti quella che si chiede se due variabili siano associate tra loro oppure no. In questo Capitolo abbiamo discusso il concetto di indipendenza (come contrapposto al concetto di associazione). In seguito vedremo come sia possibile fare inferenza sull’associazione tra variabili.</p>
<!--chapter:end:016_conditional_prob.Rmd-->
</div>
</div>
<div id="chapter-teo-bayes" class="section level1 hasAnchor" number="8">
<h1 class="hasAnchor"><span class="header-section-number">8</span> Il teorema di Bayes<a href="#chapter-teo-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Il teorema di Bayes assume un ruolo fondamentale nell’interpretazione soggettivista della probabilità perché descrive l’aggiornamento della fiducia che si aveva nel verificarsi di una determinata ipotesi <span class="math inline">\(H\)</span> (identificata con la probabilità assegnata all’ipotesi stessa) in conseguenza del verificarsi dell’evidenza <span class="math inline">\(E\)</span>.</p>
<div id="il-teorema-di-bayes" class="section level2 hasAnchor" number="8.1">
<h2 class="hasAnchor"><span class="header-section-number">8.1</span> Il teorema di Bayes<a href="#il-teorema-di-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-33" class="theorem"><strong>(#thm:unlabeled-div-33) </strong></span>Sia <span class="math inline">\((H_i)_{i\geq 1}\)</span> una partizione dell’evento certo <span class="math inline">\(\Omega\)</span> e sia <span class="math inline">\(E \subseteq \Omega\)</span> un evento tale che <span class="math inline">\(p(E) &gt; 0\)</span>, allora, per <span class="math inline">\(i = 1, \dots, \infty\)</span>:</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(H_i \mid E) = \frac{{\mbox{P}}(E \mid H_i){\mbox{P}}(H_i)}{\sum_{j=1}^{\infty}{\mbox{P}}(H_j)P(E \mid H_j)}.
(\#eq:bayes2)
\end{equation}\]</span></p>
</div>
<p>La formula di Bayes contiene tre concetti fondamentali. I primi due distinguono il grado di fiducia precedente al verificarsi dell’evidenza <span class="math inline">\(E\)</span> da quello successivo al verificarsi dell’evidenza <span class="math inline">\(E\)</span>. Pertanto, dati gli eventi <span class="math inline">\(H, E \subseteq \Omega\)</span></p>
<ul>
<li>si definisce <em>probabilità a priori</em> la probabilità che viene attribuita al verificarsi dell’ipotesi <span class="math inline">\(H\)</span> prima di sapere che si è verificato l’evento <span class="math inline">\(E\)</span>, tenendo conto delle caratteristiche cognitive del decisore (esperienza, modo di pensare, ecc.);</li>
<li>si definisce <em>probabilità a posteriori</em> la probabilità assegnata ad <span class="math inline">\(H\)</span> una volta che sia noto <span class="math inline">\(E\)</span>, ovvero l’aggiornamento della probabilità a priori alla luce della nuova evidenza <span class="math inline">\(E\)</span>.</li>
</ul>
<p>Il terzo concetto definisce la probabilità che ha l’evento <span class="math inline">\(E\)</span> di verificarsi quando è vera l’ipotesi <span class="math inline">\(H\)</span>, ovvero la probabilità dell’evidenza in base all’ipotesi. Pertanto, dati gli eventi <span class="math inline">\(H, E \subseteq \Omega\)</span></p>
<ul>
<li>si definisce <em>verosimiglianza</em> di <span class="math inline">\(H\)</span> dato <span class="math inline">\(E\)</span> la probabilità condizionata che si verifichi <span class="math inline">\(E\)</span>, se è vera <span class="math inline">\(H\)</span>: <span class="math inline">\(P (E \mid H)\)</span>.</li>
</ul>
<p>Si noti che, per il calcolo della quantità a denominatore, si ricorre al teorema della probabilità assoluta.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>(#exr:unlabeled-div-34) </strong></span>Per fare un esempio, considerando una partizione dell’evento certo <span class="math inline">\(\Omega\)</span> in due soli eventi che chiamiamo ipotesi <span class="math inline">\(H_1\)</span> e <span class="math inline">\(H_2\)</span>. Supponiamo conosciute le probabilità a priori <span class="math inline">\(P(H_1)\)</span> e <span class="math inline">\(P(H_2)\)</span>. Consideriamo un terzo evento <span class="math inline">\(E \subseteq \Omega\)</span> con probabilità non nulla di cui si conosce la verosimiglianza, ovvero si conoscono le probabilità condizionate <span class="math inline">\({\mbox{P}}(E \mid H_1)\)</span> e <span class="math inline">\(P(E \mid H_2)\)</span>. Supponendo che si sia verificato l’evento <span class="math inline">\(E\)</span>, vogliamo conoscere le probabilità a posteriori delle ipotesi, ovvero <span class="math inline">\(P(H_1 \mid E)\)</span> e <span class="math inline">\(P(H_2 \mid E)\)</span>.</p>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-33-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Per trovare le probabilità cercate scriviamo:</p>
<p><span class="math display">\[
\begin{split}
{\mbox{P}}(H_1 \mid E) &amp;= \frac{{\mbox{P}}(E \cap H_1)}{{\mbox{P}}(E)}\notag\\
              &amp;= \frac{{\mbox{P}}(E \mid H_1){\mbox{P}}(H_1)}{{\mbox{P}}(E)}.
\end{split}
\]</span></p>
<p>Sapendo che <span class="math inline">\(E = (E \cap H_1) \cup (E \cap H_2)\)</span> e che <span class="math inline">\(H_1\)</span> e <span class="math inline">\(H_2\)</span> sono eventi disgiunti, ovvero <span class="math inline">\(H_1 \cap H_2 = \emptyset\)</span>, ne segue che possiamo calcolare <span class="math inline">\({\mbox{P}}(E)\)</span> utilizzando il teorema della probabilità assoluta:</p>
<p><span class="math display">\[
\begin{split}
{\mbox{P}}(E) &amp;= {\mbox{P}}(E \cap H_1) + {\mbox{P}}(E \cap H_2)\notag\\
     &amp;= {\mbox{P}}(E \mid H_1)P(H_1) + {\mbox{P}}(E \mid H_2){\mbox{P}}(H_2).
\end{split}
\]</span></p>
<p>
Sostituendo tale risultato nella formula precedente otteniamo:</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(H_1 \mid E) = \frac{{\mbox{P}}(E \mid H_1){\mbox{P}}(H_1)}{{\mbox{P}}(E \mid H_1){\mbox{P}}(H_1) + {\mbox{P}}(E \mid H_2)P(H_2)}.
(\#eq:bayes1)
\end{equation}\]</span></p>
<p>Un lettore attento si sarà reso conto che, in precedenza, abbiamo già applicato il teorema di Bayes quando abbiamo risolto l’esercizio riportato nella Sezione @ref(sec:bayes-cancer). In quel caso, le due ipotesi erano “malattia”, che possiamo denotare con <span class="math inline">\(M\)</span>, e “malattia assente”, <span class="math inline">\(M^\complement\)</span>. L’evidenza <span class="math inline">\(E\)</span> è costituita dal risultato positivo al test, ovvero <span class="math inline">\(+\)</span>. Con questa nuova notazione la @ref(eq:bayes1) diventa:</p>
<p><span class="math display">\[\begin{equation}
{\mbox{P}}(M \mid +) = \frac{{\mbox{P}}(+ \mid M) {\mbox{P}}(M)}{{\mbox{P}}(+ \mid M) {\mbox{P}}(M) + {\mbox{P}}(+ \mid M^\complement) {\mbox{P}}(M^\complement)}\notag
\end{equation}\]</span></p>
<p>Inserendo i dati nella formula, otteniamo</p>
<p><span class="math display">\[\begin{align}
{\mbox{P}}(M \mid +) &amp;= \frac{0.9 \cdot 10/1000}{0.9 \cdot 10/1000 + 99 / 990 \cdot 990 / 1000} \notag\\
&amp;= \frac{9}{108}.\notag
\end{align}\]</span></p>
</div>
</div>
<div id="commenti-e-considerazioni-finali-6" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Il teorema di Bayes rende esplicito il motivo per cui la probabilità non possa essere pensata come uno stato oggettivo, quanto piuttosto come un’inferenza soggettiva e condizionata. Il denominatore del membro di destra della @ref(eq:bayes2) è un semplice fattore di normalizzazione. Nel numeratore compaiono invece due quantità: <span class="math inline">\({\mbox{P}}(H_i\)</span>) e <span class="math inline">\({\mbox{P}}(E \mid H_i)\)</span>. La probabilità <span class="math inline">\({\mbox{P}}(H_i\)</span>) è la probabilità <em>probabilità a priori</em> (<em>prior</em>) dell’ipotesi <span class="math inline">\(H_i\)</span> e rappresenta l’informazione che l’agente bayesiano possiede a proposito dell’ipotesi <span class="math inline">\(H_i\)</span>. Diremo che <span class="math inline">\({\mbox{P}}(H_i)\)</span> codifica il grado di fiducia che l’agente ripone in <span class="math inline">\(H_i\)</span> precedentemente al verificarsi dell’evidenza <span class="math inline">\(E\)</span>. Nell’interpretazione bayesiana, <span class="math inline">\({\mbox{P}}(H_i)\)</span> rappresenta un giudizio personale dell’agente e non esistono criteri esterni che possano determinare se tale giudizio sia coretto o meno. La probabilità condizionata <span class="math inline">\({\mbox{P}}(E \mid H_i)\)</span> rappresenta invece la verosimiglianza di <span class="math inline">\(H_i\)</span> dato <span class="math inline">\(E\)</span> e descrive la plausibilità che si verifichi l’evento <span class="math inline">\(E\)</span> se è vera l’ipotesi <span class="math inline">\(H_i\)</span>. Il teorema di Bayes descrive la regola che l’agente deve seguire per aggiornare il suo grado di fiducia nell’ipotesi <span class="math inline">\(H_i\)</span> alla luce del verificarsi dell’evento <span class="math inline">\(E\)</span>. La <span class="math inline">\({\mbox{P}}(H_i \mid E)\)</span> è chiamata probabilità a posteriori dato che rappresenta la nuova probabilità che l’agente assegna all’ipotesi <span class="math inline">\(H_i\)</span> affinché rimanga consistente con le nuove informazioni fornitegli da <span class="math inline">\(E\)</span>.</p>
<p>La probabilità a posteriori dipende sia dall’evidenza <span class="math inline">\(E\)</span>, sia dalla conoscenza a priori dell’agente <span class="math inline">\({\mbox{P}}(H_i)\)</span>. È dunque chiaro come non abbia senso parlare di una probabilità oggettiva: per il teorema di Bayes la probabilità è definita condizionatamente alla probabilità a priori, la quale a sua volta, per definizione, è un’assegnazione soggettiva. Ne segue pertanto che ogni probabilità deve essere considerata come una rappresentazione del grado di fiducia soggettiva dell’agente. Dato che ogni assegnazione probabilistica rappresenta uno stato di conoscenza e che ciascun particolare stato di conoscenza è arbitrario, un accordo tra agenti diversi non è richiesto. Tuttavia, la teoria delle probabilità ci fornisce uno strumento che, alla luce di nuove evidenze, consente di aggiornare in un modo razionale il grado di fiducia che attribuiamo ad un’ipotesi, via via che nuove evidenze vengono raccolte, in modo tale da formulare un’ipotesi a posteriori la quale non è mai definitiva, ma può sempre essere aggiornata in base alle nuove evidenze disponibili. Questo processo si chiama <em>aggiornamento bayesiano</em>. Vedremo nel Capitolo @ref(ch:intro-bayes-inference) come estendere la @ref(eq:bayes2) al caso continuo.</p>
<!-- Esercizi uleriori sono proposti nelle Appendici \@ref(appendix:bayes-updating) e \@ref(appendix:exrc-abs-prob). -->
<!-- ::: {.remark} -->
<!-- Qual è la pronuncia di "Bayesian"? Per saperlo possiamo seguire [questo link](https://bayes-rules.github.io/posts/fun/). -->
<!-- ::: -->
<!-- Il teorema di Bayes costituisce il fondamento dell'approccio più moderno -->
<!-- della statistica, quello appunto detto bayesiano. Chi usa il teorema di -->
<!-- Bayes non è, solo per questo motivo, "bayesiano": ci vuole ben altro. Ci -->
<!-- vuole un modo diverso per intendere il significato della probabilità e -->
<!-- un modo diverso per intendere gli obiettivi dell'inferenza statistica. In anni recenti, una gran parte della comunità scientifica ha riconosciuto all'approccio bayesiano il merito di consentire lo sviluppo di modelli anche molto complessi senza richiedere, d'altra parte, conoscenze matematiche troppo avanzate all'utente. Per questa ragione l'approccio bayesiano sta prendendo sempre più piede, anche in psicologia. -->
<!-- <!-- ::: {.exercise} -->
<!-- <!-- Consideriamo un'urna che contiene 5 palline rosse e 2 palline verdi. Due -->
<!-- <!-- palline vengono estratte, una dopo l'altra. Vogliamo sapere la -->
<!-- <!-- probabilità dell'evento "la seconda pallina estratta è rossa". -->
<!-- <!-- Lo spazio campionario è $\Omega = \{RR, RV, VR, VV\}$. Chiamiamo $R_1$ -->
<!-- <!-- l'evento "la prima pallina estratta è rossa", $V_1$ l'evento "la prima -->
<!-- <!-- pallina estratta è verde", $R_2$ l'evento "la seconda pallina estratta è -->
<!-- <!-- rossa" e $V_2$ l'evento "la seconda pallina estratta è verde". Dobbiamo -->
<!-- <!-- trovare $P(R_2)$ e possiamo risolvere il problema usando il teorema -->
<!-- <!-- della probabilità -->
<!-- <!-- totale \@ref(eq:prob-total-1b): -->
<!-- <!-- $$ -->
<!-- <!-- \begin{split} -->
<!-- <!-- P(R_2) &= P(R_2 \mid R_1) P(R_1) + P(R_2 \mid V_1)P(V_1)\\ -->
<!-- <!-- &= \frac{4}{6} \cdot \frac{5}{7} + \frac{5}{6} \cdot \frac{2}{7} \\ -->
<!-- <!-- &= \frac{30}{42} = \frac{5}{7}. -->
<!-- <!-- \end{split} -->
<!-- <!-- $$ -->
<!-- <!-- Se la prima estrazione è quella di una pallina rossa, nell'urna restano -->
<!-- <!-- 4 palline rosse e due verdi, dunque, la probabilità che la seconda -->
<!-- <!-- estrazione produca una pallina rossa è uguale a 4/6. La probabilità di -->
<!-- <!-- una pallina rossa nella prima estrazione è 5/7. Se la prima estrazione è -->
<!-- <!-- quella di una pallina verde, nell'urna restano 5 palline rosse e una -->
<!-- <!-- pallina verde, dunque, la probabilità che la seconda estrazione produca -->
<!-- <!-- una pallina rossa è uguale a 5/6. La probabilità di una pallina verde -->
<!-- <!-- nella prima estrazione è 2/7. -->
<!-- <!-- ::: -->
<!-- ## Il teorema di Bayes -->
<!-- Introduciamo ora il teorema di Bayes considerando un caso specifico per poi esaminarlo nella sua forma più generale. Sia $\{F_1, F_2\}$ una partizione dello spazio campionario $\Omega$. Consideriamo un terzo evento $E \subset \Omega$ con probabilità non nulla di cui si conoscono le probabilità condizionate rispetto ad $F_1$ e a $F_2$, ovvero ${\mbox{P}}(E \mid F_1)$ e $P(E \mid F_2)$. È chiaro per le ipotesi fatte che se si verifica $E$ deve anche essersi verificato almeno uno degli eventi $F_1$ e $F_2$. Supponendo che si sia verificato l'evento $E$, ci chiediamo: qual è la probabilità che si sia verificato $F_1$ piuttosto che $F_2$? -->
<!-- ```{tikz echo=FALSE, fig.cap="", fig.ext='png', fig.width = 2, cache=TRUE, out.width="45%"} -->
<!-- \usetikzlibrary{ -->
<!--   matrix, patterns, calc, fit, shapes, chains, snakes, -->
<!--   arrows.meta, arrows, backgrounds, trees, positioning, -->
<!--   lindenmayersystems -->
<!-- } -->
<!-- \begin{tikzpicture}[scale=.8] -->
<!--   % \draw[thick] (0,0) -- (0,5) -- (8,5) -- (8,0) -- (0,0); -->
<!--   \draw[thick] (0,0) rectangle (8,5); -->
<!--   \draw[thick, color=gray!15, fill] (4,2.5) ellipse (2.7cm and 1.7cm); -->
<!--   \draw[thick] (3,0) .. controls (6,2) and (2,4) .. (4,5); -->
<!--   \node (n1) at (6,4) {\textcolor{gray}{$E$}}; -->
<!--   \node (n2) at (0.7,2) {$F_1$}; -->
<!--   \node (n2) at (3,2.5) {$E\cap F_1$}; -->
<!--   \node (n2) at (5,2.5) {$E\cap F_2$}; -->
<!--   \node (n3) at (7.5,2) {$F_2$}; -->
<!-- \end{tikzpicture} -->
<!-- ``` -->
<!-- Per rispondere alla domanda precedente scriviamo: -->
<!-- $$ -->
<!-- \begin{split} -->
<!-- {\mbox{P}}(F_1 \mid E) &= \frac{{\mbox{P}}(E \cap F_1)}{{\mbox{P}}(E)}\notag\\ -->
<!--               &= \frac{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1)}{{\mbox{P}}(E)}. -->
<!-- \end{split} -->
<!-- $$ -->
<!-- Sapendo che $E = (E \cap F_1) \cup (E \cap F_2)$ e che $F_1$ e $F_2$ sono eventi disgiunti, ovvero $F_1 \cap F_2 = \emptyset$, ne segue che possiamo calcolare ${\mbox{P}}(E)$ utilizzando il teorema della probabilità assoluta: -->
<!-- $$ -->
<!-- \begin{split} -->
<!-- {\mbox{P}}(E) &= {\mbox{P}}(E \cap F_1) + {\mbox{P}}(E \cap F_2)\notag\\ -->
<!--      &= {\mbox{P}}(E \mid F_1)P(F_1) + {\mbox{P}}(E \mid F_2){\mbox{P}}(F_2). -->
<!-- \end{split} -->
<!-- $$ -->
<!-- \noindent -->
<!-- Sostituendo il risultato precedente nella formula della probabilità condizionata $P(F_1 \mid E)$ otteniamo: -->
<!-- \begin{equation} -->
<!-- {\mbox{P}}(F_1 \mid E) = \frac{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1)}{{\mbox{P}}(E \mid F_1){\mbox{P}}(F_1) + {\mbox{P}}(E \mid F_2)P(F_2)}. -->
<!-- (\#eq:bayes1) -->
<!-- \end{equation} -->
<!-- \noindent -->
<!-- La \@ref(eq:bayes1) si generalizza facilmente al caso di più di due eventi disgiunti, come indicato di seguito. -->
<!-- ::: {.theorem} -->
<!-- Sia $E$ un evento contenuto in $F_1 \cup \dots \cup F_k$, dove gli eventi $F_j, j=1, \dots, k$ sono a due a due incompatibili e necessari. Allora per ognuno dei suddetti eventi $F_j$ vale la seguente formula: -->
<!-- \begin{equation} -->
<!-- {\mbox{P}}(F_j \mid E) = \frac{{\mbox{P}}(E \mid F_j){\mbox{P}}(F_j)}{\sum_{j=1}^{k}{\mbox{P}}(F_j)P(E \mid F_j)}. -->
<!-- (\#eq:bayes2) -->
<!-- \end{equation} -->
<!-- ::: -->
<!-- \noindent -->
<!-- La \@ref(eq:bayes2) prende il nome di *teorema di Bayes* e mostra che la conoscenza del verificarsi dell'evento $E$ modifica la probabilità che avevamo attribuito all'evento $F_j$. Nella \@ref(eq:bayes2) la probabilità condizionata ${\mbox{P}}(F_j \mid E)$ prende il nome di probabilità _a posteriori_ dell'evento $F_j$: il termine "a posteriori" sta a significare "dopo che è noto che si è verificato l'evento $E$". -->
<!--chapter:end:017_bayes_theorem.Rmd-->
</div>
</div>
<div id="chapter-prob-congiunta" class="section level1 hasAnchor" number="9">
<h1 class="hasAnchor"><span class="header-section-number">9</span> Probabilità congiunta<a href="#chapter-prob-congiunta" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>La probabilità congiunta è la probabilità che due o più eventi si verifichino contemporaneamente. In questo Capitolo verrà esaminato in dettaglio il caso discreto.</p>
<div id="funzione-di-probabilità-congiunta" class="section level2 hasAnchor" number="9.1">
<h2 class="hasAnchor"><span class="header-section-number">9.1</span> Funzione di probabilità congiunta<a href="#funzione-di-probabilità-congiunta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dopo aver trattato della distribuzione di probabilità di una variabile casuale, la quale associa ad ogni evento elementare dello spazio campione uno ed un solo numero reale, è naturale estendere questo concetto al caso di due o più variabili casuali.</p>
<p>Iniziamo a descrivere il caso discreto con un esempio. Consideriamo l’esperimento casuale corrispondente al lancio di tre monete equilibrate. Lo spazio campione è</p>
<p><span class="math display">\[
\Omega = \{TTT, TTC, TCT, CTT, CCT, CTC, TCC, CCC\}.
\]</span></p>
<p>Dato che i tre lanci sono tra loro indipendenti, non c’è ragione di aspettarsi che uno degli otto risultati possibili dell’esperimento sia più probabile degli altri, dunque possiamo associare a ciascuno degli otto eventi elementari dello spazio campione la stessa probabilità, ovvero 1/8.</p>
<p>Siano <span class="math inline">\(X \in \{0, 1, 2, 3\}\)</span> = “numero di realizzazioni con il risultato testa nei tre lanci” e <span class="math inline">\(Y \in \{0, 1\}\)</span> = “numero di realizzazioni con il risultato testa nel primo lancio” due variabili casuali definite sullo spazio campione <span class="math inline">\(\Omega\)</span>. Indicando con T = ‘testa’ e C = ‘croce’, si ottiene la situazione riportata nella tabella @ref(tab:tre-monete-distr-cong-1).</p>
<table>
<caption>(#tab:tre-monete-distr-cong-1) Spazio campione dell’esperimento consistente nel lancio di tre monete equilibrate su cui sono state definite le variabili aleatorie <span class="math inline">\(X\)</span> = ‘numero di realizzazioni con il risultato testa nei tre lanci’ e <span class="math inline">\(Y\)</span> = ‘numero di realizzazioni con il risultato testa nel primo lancio’.</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(\omega\)</span></th>
<th align="center"><span class="math inline">\(X\)</span></th>
<th align="center"><span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(P(\omega)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\omega_1\)</span> = TTT</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_2\)</span> = TTC</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\omega_3\)</span> = TCT</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_4\)</span> = CTT</td>
<td align="center">2</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\omega_5\)</span> = CCT</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_6\)</span> = CTC</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\omega_7\)</span> = TCC</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\omega_8\)</span> = CCC</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
</tbody>
</table>
<p>Ci poniamo il problema di associare un valore di probabilità ad ogni coppia <span class="math inline">\((x, y)\)</span> definita su <span class="math inline">\(\Omega\)</span>. La coppia <span class="math inline">\((X = 0, Y = 0)\)</span> si realizza in corrispondenza di un solo evento elementare, ovvero CCC; avrà dunque una probabilità pari a <span class="math inline">\(P(X=0, Y=0) = P(CCC) = 1/8\)</span>. Nel caso della coppia <span class="math inline">\((X = 1, Y = 0)\)</span> ci sono due eventi elementari che danno luogo al risultato considerato, ovvero, CCT e CTC. La probabilità dell’evento composto <span class="math inline">\(P(X=1, Y=0)\)</span> è uguale alla somma delle probabilità dei due eventi elementari che lo costituiscono, cioé <span class="math inline">\(P(X=1, Y=0) = P(\mbox{CCT}) + P(\mbox{CTC}) = 1/8 + 1/8 = 1/4\)</span>. Sono riportati qui sotto i calcoli per tutti i possibili valori di <span class="math inline">\(X, Y\)</span>.</p>
<p><span class="math display">\[\begin{align}
P(X = 0, Y = 0) &amp;= P(\omega_8 = CCC) = 1/8; \notag\\
P(X = 1, Y = 0) &amp;= P(\omega_5 = CCT) + P(\omega_6 = CTC) = 2/8; \notag\\
P(X = 1, Y = 1) &amp;= P(\omega_7 = TCC) = 1/8; \notag\\
P(X = 2, Y = 0) &amp;= P(\omega_4 = CTT) = 1/8; \notag\\
P(X = 2, Y = 1) &amp;= P(\omega_3 = TCT) + P(\omega_2 = TTC) = 2/8; \notag\\
P(X = 3, Y = 1) &amp;= P(\omega_1 = TTT) = 1/8; \notag
\end{align}\]</span></p>
<p>Le probabilità così trovate sono riportate nella tabella @ref(tab:ditr-cong-biv-1) la quale descrive la distribuzione di probabilità congiunta delle variabili casuali <span class="math inline">\(X\)</span> = “numero di realizzazioni con il risultato testa nei tre lanci” e <span class="math inline">\(Y\)</span> = “numero di realizzazioni con il risultato testa nel primo lancio” per l’esperimento casuale consistente nel lancio di tre monete equilibrate.</p>
<table>
<caption>(#tab:ditr-cong-biv-1) Distribuzione di probabilità congiunta per i risultati dell’esperimento consistente nel lancio di tre monete equilibrate.</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x / y\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1/8</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">2/8</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1/8</td>
<td align="center">2/8</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
</tbody>
</table>
<p>In generale, possiamo dire che, dato uno spazio campione discreto <span class="math inline">\(\Omega\)</span>, è possibile associare ad ogni evento elementare <span class="math inline">\(\omega_i\)</span> dello spazio campione una coppia di numeri reali <span class="math inline">\((x, y)\)</span>, essendo <span class="math inline">\(x = X(\omega)\)</span> e <span class="math inline">\(y = Y(\omega)\)</span>, il che ci conduce alla seguente definizione.</p>
<div class="definition">
<p><span id="def:unlabeled-div-35" class="definition"><strong>(#def:unlabeled-div-35) </strong></span>Siano <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> due variabili casuali. La funzione che associa ad ogni coppia <span class="math inline">\((x, y)\)</span> un valore di probabilità prende il nome di funzione di probabilità congiunta:</p>
<p><span class="math display">\[\begin{equation}
P(x, y) = P(X = x, Y = y).
\end{equation}\]</span></p>
</div>
<p>Il termine “congiunta” deriva dal fatto che questa probabilità è legata al verificarsi di una coppia di valori, il primo associato alla variabile casuale <span class="math inline">\(X\)</span> ed il secondo alla variabile casuale <span class="math inline">\(Y\)</span>. Nel caso di due sole variabili casuali si parla di distribuzione bivariata, mentre nel caso di più variabili casuali si parla di distribuzione multivariata.</p>
<div class="remark">
<p><span id="unlabeled-div-36" class="remark"><em>Remark</em>. </span>La regola della catena, <span class="math inline">\(P(A \cap B) = P(A)P(B \mid A)\)</span>, permette il calcolo di qualsiasi membro della distribuzione congiunta di un insieme di variabili casuali utilizzando solo le probabilità condizionate. Nel caso di 4 eventi, per esempio, la regola della catena diventa</p>
<p><span class="math display">\[
P(A_1, A_2, A_3, A_4) = P(A_1) P(A_2 \mid A_1) P(A_3 \mid A_1, A_2) P(A_4 \mid A_1, A_2, A_3).
\]</span></p>
</div>
<div id="proprietà" class="section level3 hasAnchor" number="9.1.1">
<h3 class="hasAnchor"><span class="header-section-number">9.1.1</span> Proprietà<a href="#proprietà" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una distribuzione di massa di probabilità congiunta bivariata deve soddisfare due proprietà:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(0 \leq P(x_i, y_j) \leq 1\)</span>;</p></li>
<li><p>la probabilità totale deve essere uguale a <span class="math inline">\(1.0\)</span>. Tale proprietà può essere espressa nel modo seguente</p></li>
</ol>
<p><span class="math display">\[
\sum_{i} \sum_{j} P(x_i, y_j) = 1.0.
\]</span></p>
</div>
<div id="eventi" class="section level3 hasAnchor" number="9.1.2">
<h3 class="hasAnchor"><span class="header-section-number">9.1.2</span> Eventi<a href="#eventi" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si noti che dalla probabilità congiunta possiamo calcolare la probabilità di qualsiasi evento definito in base alle variabili aleatorie <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. Per capire come questo possa essere fatto, consideriamo nuovamente l’esperimento casuale discusso in precedenza.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-37" class="exercise"><strong>(#exr:unlabeled-div-37) </strong></span>Per la distribuzione di massa di probabilità congiunta riportata nella tabella @ref(tab:ditr-cong-biv-1) si trovi la probabilità dell’evento <span class="math inline">\(X+Y \leq 1\)</span>.</p>
<p>Per trovare la probabilità richiesta dobbiamo sommare le probabilità associate a tutte le coppie <span class="math inline">\((x,y)\)</span> che soddisfano la condizione <span class="math inline">\(X+Y \leq 1\)</span>, ovvero</p>
<p><span class="math display">\[\begin{equation}
P_{XY}(X+Y \leq 1) = P_{XY}(0, 0)+ P_{XY}(0, 1) + P_{XY}(1, 0) = 3/8.\notag
\end{equation}\]</span></p>
</div>
</div>
<div id="sec:marg-distr-discr" class="section level3 hasAnchor" number="9.1.3">
<h3 class="hasAnchor"><span class="header-section-number">9.1.3</span> Funzioni di probabilità marginali<a href="#sec:marg-distr-discr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- La distribuzione marginale di un sottoinsieme di variabili casuali è la distribuzione di probabilità delle variabili contenute nel sottoinsieme. Come spiegato da [Wikipedia](https://it.wikipedia.org/wiki/Distribuzione_marginale): *il termine variabile marginale è usato per riferirsi a quelle variabili nel sottoinsieme delle variabili che vengono trattenute ovvero utilizzate. Questo termine, marginale, è attribuito ai valori ottenuti ad esempio sommando in una tabella di valori lungo le righe oppure lungo le colonne, trascrivendo il risultato appunto a margine rispettivamente della riga o colonna sommata. La distribuzione delle variabili marginali (la distribuzione marginale) è ottenuta mediante marginalizzazione sopra le variabili da "scartare", e le variabili scartate sono dette fuori marginalizzate.* -->
<p>Nel caso di due variabili casuali discrete <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> di cui conosciamo la distribuzione congiunta, la distribuzione marginale di <span class="math inline">\(X\)</span> è calcolata sommando la distribuzione di probabilità congiunta sopra la variabile da “scartare”, in questo caso la <span class="math inline">\(Y\)</span>. La funzione di massa di probabilità marginale <span class="math inline">\(P(X=x)\)</span> è</p>
<p><span class="math display">\[\begin{equation}
P(X = x) = \sum_y P(X, Y = y) = \sum_y P(X \mid Y = y) P(Y = y),
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(P(X = x,Y = y)\)</span> è la distribuzione congiunta di <span class="math inline">\(X, Y\)</span>, mentre <span class="math inline">\(P(X = x \mid Y = y)\)</span> è la distribuzione condizionata di <span class="math inline">\(X\)</span> dato <span class="math inline">\(Y\)</span>. Se esaminiamo <span class="math inline">\(P(X=x)\)</span>, diciamo che la variabile <span class="math inline">\(Y\)</span> è stata marginalizzata. Le probabilità bivariate marginali e congiunte per variabili casuali discrete sono spesso mostrate come tabelle di contingenza.</p>
<p>Si noti che <span class="math inline">\(P(X = x)\)</span> e <span class="math inline">\(P(Y = y)\)</span> sono normalizzate:</p>
<p><span class="math display">\[
\sum_x P(X=x) = 1.0, \quad \sum_y P(Y=y) = 1.0.
\]</span></p>
<p>Nel caso continuo si sostituisce l’integrazione alla somma – si veda la Sezione @ref(sec:margin-vc-cont).</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-38" class="exercise"><strong>(#exr:unlabeled-div-38) </strong></span>Per l’esperimento casuale consistente nel lancio di tre monete equilibrate, si calcolino le probabilità marginali di <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>Nell’ultima colonna a destra e nell’ultima riga in basso della tabella @ref(tab:ditr-cong-biv) sono riportate le distribuzioni di probabilità marginali di <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. <span class="math inline">\(P_X\)</span> si ottiene sommando su ciascuna riga fissata la colonna <span class="math inline">\(j\)</span>, <span class="math inline">\(P_X(X = j) = \sum_y p_{xy}(x = j, y)\)</span>. <span class="math inline">\(P_Y\)</span> si trova sommando su ciascuna colonna fissata la riga <span class="math inline">\(i,\)</span> <span class="math inline">\(P_Y (Y = i) = \sum_x p_{xy}(x, y = i)\)</span>.</p>
<table>
<caption>(#tab:ditr-cong-biv) Distribuzione di probabilità congiunta <span class="math inline">\(p(x,y)\)</span> per i risultati dell’esperimento consistente nel lancio di tre monete equilibrate e probabilità marginali <span class="math inline">\(P(x)\)</span> e <span class="math inline">\(P(y)\)</span>.</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x / y\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center"><span class="math inline">\(P(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1/8</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">2/8</td>
<td align="center">1/8</td>
<td align="center">3/8</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1/8</td>
<td align="center">2/8</td>
<td align="center">3/8</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0</td>
<td align="center">1/8</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(P(y)\)</span></td>
<td align="center">4/8</td>
<td align="center">4/8</td>
<td align="center">1.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="sec:margin-vc-cont" class="section level2 hasAnchor" number="9.2">
<h2 class="hasAnchor"><span class="header-section-number">9.2</span> Marginalizzazione di variabili casuali continue<a href="#sec:margin-vc-cont" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Nella trattazione della statistca bayesiana useremo spesso il concetto di “marginalizzazione” e vedremo equazioni come la seguente:</p>
<p><span class="math display">\[\begin{equation}
p(y) = \int_{\theta} p(y, \theta) = \int_{\theta} p(y \mid \theta) p(\theta),
(\#eq:ex-marg-cont)
\end{equation}\]</span></p>
<p>laddove <span class="math inline">\(y\)</span> e <span class="math inline">\(\theta\)</span> sono due variabili casuali continue – nello specifico, con <span class="math inline">\(y\)</span> denoteremo i dati e con <span class="math inline">\(\theta\)</span> i parametri di un modello statistico. Per ora, possiamo pensare a <span class="math inline">\(y\)</span> e <span class="math inline">\(\theta\)</span> come a due variabili casuali qualsiasi. La @ref(eq:ex-marg-cont) descrive la distribuzione marginale di <span class="math inline">\(y\)</span>.</p>
<p>Per meglio comprendere la @ref(eq:ex-marg-cont) possiamo esaminare il corrispondente caso discreto nel quale sostituiamo semplicemente l’integrale con una somma, il che ci riporta alla situazione descritta nella Sezione @ref(sec:marg-distr-discr). Possiamo dunque scrivere:</p>
<p><span class="math display">\[\begin{equation}
p(y) = \sum_{\theta} p(y, \theta) = \sum_{\theta} p(y \mid \theta) p(\theta).
(\#eq:ex-marginalization)
\end{equation}\]</span></p>
<p>Esaminiamo un semplice esempio numerico. Siano <span class="math inline">\(y\)</span> e <span class="math inline">\(\theta\)</span> due variabili discrete aventi la distribuzione di massa di probabilità congiunta riportata nella tabella @ref(tab:ex-marg).</p>
<table>
<caption>(#tab:ex-marg) Distribuzione di probabilità congiunta <span class="math inline">\(p(y, \theta)\)</span> per due variabili casuali discrete.</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(y / \theta\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="left"><span class="math inline">\(p(y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0.1</td>
<td align="center">0.2</td>
<td align="left">0.3</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">0.3</td>
<td align="center">0.4</td>
<td align="left">0.7</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(p(\theta)\)</span></td>
<td align="center">0.4</td>
<td align="center">0.6</td>
<td align="left">1.0</td>
</tr>
</tbody>
</table>
<p>Applicando la @ref(eq:ex-marginalization), la distribuzione marginale <span class="math inline">\(p(y) = \{0.3, 0.7\}\)</span> può essere trovata nel modo seguente:</p>
<p><span class="math display">\[
\begin{pmatrix}
    0.1 / 0.4 \\
    0.3 / 0.4
\end{pmatrix} \cdot 0.4 +
\begin{pmatrix}
    0.2 / 0.6 \\
    0.4 / 0.6
\end{pmatrix} \cdot 0.6 =
\begin{pmatrix}
    0.3 \\
   0.7
\end{pmatrix}.
\]</span></p>
<p>È possibiile pensare al caso continuo indicato nella @ref(eq:ex-marg-cont) come all’estensione dell’esempio presente ad un numero infinito di valori <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="commenti-e-considerazioni-finali-7" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La funzione di probabilità congiunta tiene simultaneamente conto del
comportamento di due variabili casuali <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> e di come esse si
influenzano reciprocamente. In particolare, si osserva che se le due
variabili discrete <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> non si influenzano, cioè se sono statisticamente indipendenti, allora la distribuzione di massa di probabilità congiunta si ottiene
come prodotto delle funzioni di probabilità marginali di <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:
<span class="math inline">\(P_{X, Y}(x, y) = P_X(x) P_Y(y)\)</span>.</p>
<!--chapter:end:018_joint_prob.Rmd-->
</div>
</div>
<div id="chapter-intro-density-function" class="section level1 hasAnchor" number="10">
<h1 class="hasAnchor"><span class="header-section-number">10</span> La densità di probabilità<a href="#chapter-intro-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Finora abbiamo considerato solo variabili casuali discrete, cioè variabili che assumono solo valori interi. Ma cosa succede se vogliamo usare variabili casuali per rappresentare lunghezze, o volumi, o distanze, o una qualsiasi delle altre proprietà continue nel mondo fisico (o psicologico)? È necessario generalizzare l’approccio usato finora.</p>
<p>Le variabili casuali continue assumono valori reali. L’insieme dei numeri reali è <em>non numerabile</em> perché è più grande dell’insieme degli interi.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Le leggi della probabilità sono le stessa per le variabili casuali discrete e quelle continue. La nozione di funzione di massa di probabilità, invece, deve essere sostituita dal suo equivalente continuo, ovvero dalla <em>funzione di densità di probabilità</em>. Lo scopo di questo Capitolo è quello di chiarire il significato di questa nozione, usando un approccio basato sulle simulazioni.</p>
<div id="spinner-e-variabili-casuali-continue-uniformi" class="section level2 hasAnchor" number="10.1">
<h2 class="hasAnchor"><span class="header-section-number">10.1</span> Spinner e variabili casuali continue uniformi<a href="#spinner-e-variabili-casuali-continue-uniformi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consideriamo il seguente esperimento casuale. Facciamo ruotare ad alta velocità uno spinner simmetrico imperniato su un goniometro e osserviamo la posizione in cui si ferma (individuata dall’angolo acuto con segno tra il suo asse e l’asse orizzontale del goniometro). Chiamiamo <span class="math inline">\(\Theta\)</span> la variabile casuale “pendenza dello spinner”. Nella trattazione seguente useremo i gradi e, di conseguenza, <span class="math inline">\(\Theta \in [0, 360]\)</span>.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-34-1.png" alt="Uno spinner che riposa a 36 gradi, o il dieci percento del percorso intorno al cerchio. La pendenza dello spinner può assumere qualunque valore tra 0 e 360 gradi." width="100%" />
<p class="caption">
(#fig:unnamed-chunk-34)Uno spinner che riposa a 36 gradi, o il dieci percento del percorso intorno al cerchio. La pendenza dello spinner può assumere qualunque valore tra 0 e 360 gradi.
</p>
</div>
<p>Cosa implica per <span class="math inline">\(\Theta\)</span> dire che lo spinner è simmetrico? Possiamo dire che, in ciascuna prova, la rotazione dello spinner produce un angolo qualunque da 0 a 360 gradi. In altri termini, un valore <span class="math inline">\(\Theta\)</span> compreso tra 0 e 36 gradi ha la stessa probabilità di essere osservato di un valore <span class="math inline">\(\Theta\)</span> compreso tra 200 e 236 gradi. Inoltre, poiché 36 gradi è un decimo del percorso intorno al cerchio, la probabilità di ottenere un qualsiasi intervallo di 36 gradi sarà sempre uguale al 10%. Ovvero <span class="math inline">\(\mbox{P}(0 \leq \Theta \leq 36) \ = \ \frac{1}{10}\)</span> e <span class="math inline">\(\mbox{P}(200 \leq \Theta \leq 236) \ = \ \frac{1}{10}\)</span>.</p>
<p>È importante notare che le probabilità precedenti non si riferiscono al fatto che <span class="math inline">\(\Theta\)</span> assume uno specifico valore, ma piuttosto all’evento di osservare <span class="math inline">\(\Theta\)</span> in un intervallo di valori. In generale, la probabilità che la pendenza <span class="math inline">\(\Theta\)</span> dello spinner cada in intervallo è la frazione del cerchio rappresentata dall’intervallo, cioè,</p>
<p><span class="math display">\[
\mbox{P}(\theta_1 \leq \Theta \leq \theta_2) = \frac{\theta_2 - \theta_1}{360}, \qquad 0 \leq \theta_1 \leq \theta_2 \leq 360.
\]</span></p>
<p>La ragione di questo è che le variabili casuali continue non hanno una massa di probabilità. Invece, una massa di probabilità viene assegnata alla realizzazione della variabile casuale in un intervallo di valori.</p>
<div id="il-paradosso-delle-variabili-casuali-continue" class="section level3 hasAnchor" number="10.1.1">
<h3 class="hasAnchor"><span class="header-section-number">10.1.1</span> Il paradosso delle variabili casuali continue<a href="#il-paradosso-delle-variabili-casuali-continue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nel nostro esempio, la pendenza dello spinner è esattamente 36 gradi; ma avrebbe anche potuto essere 36.0376531 gradi, o qualunque altro valore in quell’intorno. Qual è la probabilità che la pendenza dello spinner sia esattamente 36? Paradossalmente, la risposta è zero:</p>
<p><span class="math display">\[
\mbox{P}(\Theta = 36) = 0.
\]</span></p>
<p>Infatti, se la probabilità di un qualunque valore fosse maggiore di zero, ogni altro possibile valore dovrebbe avere la stessa probabilità, dato che abbiamo assunto che tutti i valori <span class="math inline">\(\Theta\)</span> siano egualmente probabili. Ma se poi andiamo a sommare tutte queste probabilità il totale diventerà maggiore di uno, il che non è possibile.</p>
<p>Nel caso delle variabili casuali continue dobbiamo dunque rinunciare a qualcosa, e quel qualcosa è l’idea che, in una distribuzione continua, ciascun valore puntuale della variabile casuale possa avere una massa di probabilità maggiore di zero. Il paradosso sorge perché una realizzazione della variabile casuale continua produce sempre un qualche numero, ma ciscuno di tali numeri ha probabilità nulla.</p>
</div>
</div>
<div id="la-funzione-di-ripartizione-per-una-variabile-casuale-continua" class="section level2 hasAnchor" number="10.2">
<h2 class="hasAnchor"><span class="header-section-number">10.2</span> La funzione di ripartizione per una variabile casuale continua<a href="#la-funzione-di-ripartizione-per-una-variabile-casuale-continua" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Supponiamo che <span class="math inline">\(\Theta \sim \mathcal{U}(0, 360)\)</span> sia la pendenza dello spinner. La funzione di ripartizione (ovvero, la distribuzione cumulativa) è definita esattamente come nel caso delle variabili casuali discrete:</p>
<p><span class="math display">\[
F_{\Theta}(\theta) = \mbox{P}(\Theta \leq \theta).
\]</span></p>
<p>Cioè, è la probabilità che la variabile casuale <span class="math inline">\(\Theta\)</span> assuma un valore minore di o uguale a <span class="math inline">\(\theta\)</span>. In questo caso, poiché si presume che lo spinner sia simmetrico, la funzione di distribuzione cumulativa è</p>
<p><span class="math display">\[
F_{\Theta}(\theta) = \frac{\theta}{360}.
\]</span></p>
<p>Questa è una funzione lineare di <span class="math inline">\(\theta\)</span>, cioè <span class="math inline">\(\frac{1}{360} \cdot \theta\)</span>, come indicato dal grafico della figura @ref(fig:spinner-cdf).</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/spinner-cdf-1.png" alt="Funzione di distribuzione cumulativa per l'angolo $\theta$ (in gradi) risultante da una rotazione di uno spinner simmetrico. La linea tratteggiata mostra il valore a 180 gradi, che corrisponde ad una probabilità di 0.5, e la linea tratteggiata a 270 gradi, che corrisponde ad una probabilità di 0.75." width="576" />
<p class="caption">
(#fig:spinner-cdf)Funzione di distribuzione cumulativa per l’angolo <span class="math inline">\(\theta\)</span> (in gradi) risultante da una rotazione di uno spinner simmetrico. La linea tratteggiata mostra il valore a 180 gradi, che corrisponde ad una probabilità di 0.5, e la linea tratteggiata a 270 gradi, che corrisponde ad una probabilità di 0.75.
</p>
</div>
<p>Possiamo verificare questo risultato mediante simulazione. Per stimare la funzione di ripartizione, simuliamo <span class="math inline">\(M\)</span> valori <span class="math inline">\(\theta^{(m)}\)</span> e poi li ordiniamo in ordine crescente.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">runif</span>(M, <span class="dv">0</span>, <span class="dv">360</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>theta_asc <span class="ot">&lt;-</span> <span class="fu">sort</span>(theta)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">:</span>M) <span class="sc">/</span> M</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>unif_cdf_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> theta_asc,</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob =</span> prob</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>unif_cdf_plot <span class="ot">&lt;-</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  unif_cdf_df <span class="sc">%&gt;%</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> prob)) <span class="sc">+</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">90</span>, <span class="dv">180</span>, <span class="dv">270</span>, <span class="dv">360</span>)) <span class="sc">+</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>)) <span class="sc">+</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">F</span>(Theta)(theta)))</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>unif_cdf_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-35-1.png" alt="Grafico della funzione di ripartizione di una variabile casuale $\Theta$ che rappresenta il risultato di una rotazione di uno spinner simmetrico. Come previsto, tale funzione è una semplice funzione lineare perché la variabile sottostante $\Theta$ ha una distribuzione uniforme." width="576" />
<p class="caption">
(#fig:unnamed-chunk-35)Grafico della funzione di ripartizione di una variabile casuale <span class="math inline">\(\Theta\)</span> che rappresenta il risultato di una rotazione di uno spinner simmetrico. Come previsto, tale funzione è una semplice funzione lineare perché la variabile sottostante <span class="math inline">\(\Theta\)</span> ha una distribuzione uniforme.
</p>
</div>
<p>Anche con <em>M</em> = 1000, tale grafico è praticamente indistinguibile da quello prodotto per via analitica.</p>
<p>Come nel caso delle variabili casuali discrete, la funzione di ripartizione può essere utilizzata per calcolare la probabilità che la variabile casuale assuma valori in un certo intervallo. Ad esempio</p>
<p><span class="math display">\[\begin{align}
\mbox{P}(180 &lt; \Theta \leq 270) &amp;= \mbox{P}(\Theta \leq 270) \ - \ \mbox{P}(\Theta \leq 180) \notag\\
&amp;= F_{\Theta}(270) - F_{\Theta}(180)\notag\\
&amp;= \frac{3}{4} - \frac{1}{2} \notag\\
&amp;= \frac{1}{4}.\noindent
\end{align}\]</span></p>
</div>
<div id="la-distribuzione-uniforme" class="section level2 hasAnchor" number="10.3">
<h2 class="hasAnchor"><span class="header-section-number">10.3</span> La distribuzione uniforme<a href="#la-distribuzione-uniforme" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dopo avere visto come generare numeri casuali uniformi da 0 a 360, consideriamo ora una variabile casuale che assume valori nell’intervallo da 0 a 1. Chiamiamo tale variabile casuale <span class="math inline">\(\Theta\)</span> e assumiamo che abbia una distribuzione continua uniforme sull’intervallo [0, 1]:</p>
<p><span class="math display">\[
\Theta \sim \mathcal{U}(0, 1).
\]</span></p>
<p>Poiché le probabilità assumono valori nell’intervallo [0, 1], possiamo pensare a <span class="math inline">\(\Theta\)</span> come ad un valore di probabilità preso a caso in ciascuna realizzazione dell’esperimento casuale.</p>
<p>La distribuzione uniforme è la più semplice delle distribuzioni di densità di probabilità. Per chiarire le proprietà di tale distribuzione, iniziamo con una simulazione e generiamo 10,000 valori casuali di <span class="math inline">\(\Theta\)</span>. I primi 10 di tali valori sono stampati qui di seguito:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">runif</span>(M)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 0.113703 0.622299 0.609275 0.623379 0.860915</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [6] 0.640311 0.009496 0.232551 0.666084 0.514251</span></span></code></pre></div>
<p>Creiamo ora un istogramma che descrive la distribuzione delle 10,000 realizzazioni <span class="math inline">\(\Theta\)</span> che abbiamo trovato:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>df_prob_unif <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">theta =</span> theta)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>unif_prob_plot <span class="ot">&lt;-</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(df_prob_unif, <span class="fu">aes</span>(theta)) <span class="sc">+</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">binwidth =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">34</span>, <span class="at">center =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">68</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">0.25</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">lim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">500</span>, <span class="dv">1000</span>)) <span class="sc">+</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(Theta, <span class="st">&quot; ~ Uniform(0, 1)&quot;</span>)))</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>unif_prob_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-37-1.png" alt="Istogramma di $10\,000$ realizzazioni $\Theta \sim \mbox{Uniform}(0, 1)$. " width="576" />
<p class="caption">
(#fig:unnamed-chunk-37)Istogramma di <span class="math inline">\(10\,000\)</span> realizzazioni <span class="math inline">\(\Theta \sim \mbox{Uniform}(0, 1)\)</span>.
</p>
</div>
<p>È chiaro che, all’aumentare del numero delle realizzazioni <span class="math inline">\(\Theta\)</span>, il profilo dell’istogramma tenderà a diventare una linea retta. Ciò significa che la funzione di densità di una variabile casuale uniforme continua è una costante. Cioè, se <span class="math inline">\(\Theta \sim \mathcal{U} (a, b)\)</span>, allora <span class="math inline">\(p_{\Theta}(\theta) = c\)</span>, dove <span class="math inline">\(c\)</span> è una costante.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>uniform_pdf_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">p_y =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>uniform_pdf_plot <span class="ot">&lt;-</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(uniform_pdf_df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> p_y)) <span class="sc">+</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">&quot;#333333&quot;</span>) <span class="sc">+</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">color =</span> <span class="st">&quot;#333333&quot;</span>) <span class="sc">+</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)) <span class="sc">+</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">lim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;c&quot;</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta)) <span class="sc">+</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(p[Theta], <span class="st">&quot;(&quot;</span>, theta, <span class="st">&quot; | a, b)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">0</span>, <span class="at">yend =</span> <span class="dv">1</span>),</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">1</span>, <span class="at">yend =</span> <span class="dv">1</span>),</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">1</span>, <span class="at">yend =</span> <span class="dv">0</span>),</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">&quot;dotted&quot;</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span><span class="fl">0.25</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">0</span>, <span class="at">yend =</span> <span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="fl">1.25</span>, <span class="at">yend =</span> <span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>),</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">&quot;#ffffe6&quot;</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">y =</span> <span class="dv">0</span>),</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">&quot;#ffffe6&quot;</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>uniform_pdf_plot</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-38-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>
Dal grafico vediamo che l’area sottesa alla funzione di densità è <span class="math inline">\((b - a)\cdot c\)</span>. Dato che tale area deve essere unitaria, ovvero, <span class="math inline">\((b - a) \cdot c = 1\)</span>, possiamo trovare <span class="math inline">\(c\)</span> dividendo entrambi i termini per <span class="math inline">\(b - a\)</span>,</p>
<p><span class="math display">\[
c  = \frac{\displaystyle{1}}{\displaystyle b - a}.
\]</span></p>
<p>Ovvero, se <span class="math inline">\(\Theta \sim \mathcal{U}(a, b)\)</span>, allora</p>
<p><span class="math display">\[
p_{\Theta}(\theta) = \mathcal{U}(\theta \mid a, b),
\]</span></p>
<p>laddove</p>
<p><span class="math display">\[
\mathcal{U}(\theta \mid a, b) = \frac{1}{b - a}.
\]</span></p>
<p>In conclusione, la densità di una variabile casuale uniforme continua non dipende da <span class="math inline">\(\theta\)</span> — è costante e identica per ogni possibile valore <span class="math inline">\(\theta\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> Vedremo nel prossimo Paragrafo che, eseguendo una trasformazione su questa variabile casuale uniforme, possiamo creare altre variabili casuali di interesse.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-39" class="exercise"><strong>(#exr:unlabeled-div-39) </strong></span>Si consideri una variabile casuale uniforme <span class="math inline">\(X\)</span> definita sull’intervallo [0, 100]. Si trovi la probabilità <span class="math inline">\(P(20 &lt; X &lt; 60)\)</span>.</p>
<p>Per trovare la soluzione è sufficiente calcolare l’area di un rettangolo di base <span class="math inline">\(60 - 20 = 40\)</span> e di altezza 1/100. La probabilità cercata è dunque <span class="math inline">\(P(20 &lt; X &lt; 60) = 40 \cdot 0.01 = 0.4\)</span>.</p>
</div>
</div>
<div id="dagli-istogrammi-alle-densità" class="section level2 hasAnchor" number="10.4">
<h2 class="hasAnchor"><span class="header-section-number">10.4</span> Dagli istogrammi alle densità<a href="#dagli-istogrammi-alle-densità" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Non esiste l’equivalente di una funzione di massa di probabilità per le variabili casuali continue. Esiste invece una <em>funzione di densità di probabilità</em> la quale, nei termini di una simulazione, può essere concepita nel modo seguente: avendo a disposizione un numero enorme di casi, quando l’intervallo <span class="math inline">\(\Delta\)</span> di ciascuna classe <span class="math inline">\(\rightarrow\)</span> 0, il profilo dell’istogramma delle frequenze delle classi di ampiezza <span class="math inline">\(\Delta\)</span> tende a diventare una curva continua. Tale curva continua <span class="math inline">\(f(x)\)</span> è detta funzione di densità di probabilità.</p>
<p>Come si trasformano gli istogrammi all’aumentare del numero di osservazioni? Per fare un esempio, considereremo una funzione di una variabile casuale uniforme <span class="math inline">\([0, 1]\)</span>. Nello specifico, esamineremo la funzione logit:</p>
<p><span class="math display">\[
\alpha = \log \left(\frac{\theta}{1-\theta}\right)
\]</span></p>
<p>Alcuni valori <span class="math inline">\(\alpha\)</span> presi a caso sono i seguenti:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>logit <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">log</span>(x <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> x))</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">runif</span>(M)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">logit</span>(theta)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(alpha[m])</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -2.053</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4993</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4443</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5039</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.823</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5767</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -4.647</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -1.194</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6905</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.05702</span></span></code></pre></div>
<p>Nei grafici seguenti, la numerosità cresce da <span class="math inline">\(10\)</span> a <span class="math inline">\(1\,000\,000\)</span>.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>df_log_odds_growth <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (log10M <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>) {</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  M <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span>log10M</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  alpha <span class="ot">&lt;-</span> <span class="fu">logit</span>(<span class="fu">runif</span>(M))</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>  df_log_odds_growth <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    df_log_odds_growth,</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">alpha =</span> alpha,</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">M =</span> <span class="fu">rep</span>(<span class="fu">sprintf</span>(<span class="st">&quot;M = %d&quot;</span>, M), M)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>log_odds_growth_plot <span class="ot">&lt;-</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>  df_log_odds_growth <span class="sc">%&gt;%</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(alpha)) <span class="sc">+</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">bins =</span> <span class="dv">75</span>) <span class="sc">+</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>M, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">lim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">8.5</span>, <span class="fl">8.5</span>), <span class="at">breaks =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(Phi, <span class="st">&quot; = &quot;</span>, <span class="fu">logit</span>(Theta)))) <span class="sc">+</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;proportion of draws&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.spacing.x =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">&quot;lines&quot;</span>),</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.spacing.y =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">&quot;lines&quot;</span>)</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>log_odds_growth_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-40-1.png" alt="Istogramma di $M$ campioni casuali $\Theta \sim \mbox{Uniform}(0, 1)$ trasformati in valori $\Phi = \mbox{logit}(\Theta).$ Il profilo limite dell'istogramma è evidenziato nella figura in basso a destra che è stata costruita usando $1\,000\,000$ di osservazioni." width="100%" />
<p class="caption">
(#fig:unnamed-chunk-40)Istogramma di <span class="math inline">\(M\)</span> campioni casuali <span class="math inline">\(\Theta \sim \mbox{Uniform}(0, 1)\)</span> trasformati in valori <span class="math inline">\(\Phi = \mbox{logit}(\Theta).\)</span> Il profilo limite dell’istogramma è evidenziato nella figura in basso a destra che è stata costruita usando <span class="math inline">\(1\,000\,000\)</span> di osservazioni.
</p>
</div>
<p>In un istogramma, l’area di ciascuna barra è proporzionale alla frequenza relativa delle osservazioni in quel’intervallo. Perché tutti gli intervalli hanno la stessa ampiezza, anche l’altezza di ciascuna barra sarà proporzionale alla frequenza relativa delle osservazioni in quel’intervallo.</p>
<p>Nella simulazione, possiamo pensare all’area di ciascuna barra dell’istogramma come alla stima della probabilità che la variabile casuale assuma un valore compreso nell’intervallo considerato. All’aumentare del numero <span class="math inline">\(M\)</span> di osservazioni, le probabilità stimate si avvicinano sempre di più ai veri valori della probabilità. All’aumentare del numero degli intervalli (quando l’ampiezza <span class="math inline">\(\Delta\)</span> dell’intervallo <span class="math inline">\(\rightarrow\)</span> 0), il profilo dell’istogramma tende a diventare una curva continua. Tale curva continua è la funzione di densità di probabilità della variabile casuale. Per l’esempio presente, con <span class="math inline">\(M =1\,000\,000\)</span>, otteniamo il grafico riportato nella figura @ref(fig:hist-dens-example).</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">logit</span>(<span class="fu">runif</span>(M))</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>density_limit_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">alpha =</span> alpha)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>density_limit_plot <span class="ot">&lt;-</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  density_limit_df <span class="sc">%&gt;%</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(alpha)) <span class="sc">+</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">stat =</span> <span class="st">&quot;density&quot;</span>, <span class="at">n =</span> <span class="dv">75</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="fl">0.15</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dlogis,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">location =</span> <span class="dv">0</span>, <span class="at">scale =</span> <span class="dv">1</span>),</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;black&quot;</span>,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">0.3</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">lim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">9</span>, <span class="dv">9</span>),</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="sc">-</span><span class="dv">4</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">expression</span>(<span class="fu">paste</span>(Phi, <span class="st">&quot; = &quot;</span>, <span class="fu">logit</span>(Theta)))</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Frequenza relativa&quot;</span>) <span class="sc">+</span></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>()</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>density_limit_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/hist-dens-example-1.png" alt="Istogramma di $M = 1\,000\,000$ campioni casuali $\Theta \sim \mbox{Uniform}(0,1)$ trasformati in valori $\Phi = \mbox{logit}(\Theta)$. La spezzata nera congiunge i punti centrali superiori delle barre dell'istogramma. Nel limite, quando il numero di osservazioni e di barre tende all'infinito, tale spezzata approssima la funzione di densità di probabilità della variabile casuale." width="100%" />
<p class="caption">
(#fig:hist-dens-example)Istogramma di <span class="math inline">\(M = 1\,000\,000\)</span> campioni casuali <span class="math inline">\(\Theta \sim \mbox{Uniform}(0,1)\)</span> trasformati in valori <span class="math inline">\(\Phi = \mbox{logit}(\Theta)\)</span>. La spezzata nera congiunge i punti centrali superiori delle barre dell’istogramma. Nel limite, quando il numero di osservazioni e di barre tende all’infinito, tale spezzata approssima la funzione di densità di probabilità della variabile casuale.
</p>
</div>
<p>Nella statistica descrittiva abbiamo già incontrato una rappresentazione che ha lo stesso significato della funzione di densità, ovvero il <em>kernel density plot</em>. La stima della densità del kernel (KDE), infatti, è un metodo non parametrico per stimare la funzione di densità di probabilità di una variabile casuale.</p>
</div>
<div id="funzione-di-densità-di-probabilità" class="section level2 hasAnchor" number="10.5">
<h2 class="hasAnchor"><span class="header-section-number">10.5</span> Funzione di densità di probabilità<a href="#funzione-di-densità-di-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Per descrivere le probabilità che possono essere associate ad una variabile casuale continua <span class="math inline">\(X\)</span> è necessario definire una funzione <span class="math inline">\(p(X)\)</span> che deve soddisfare le seguenti due proprietà:</p>
<ul>
<li><p><span class="math inline">\(p(x) \geq 0, \forall x\)</span>, ovvero, l’ordinata della funzione di densità è 0 o positiva;</p></li>
<li><p><span class="math inline">\(\int_{-\infty}^{\infty} p(x) \,\operatorname {d}\!x = 1\)</span>, ovvero, l’area sottesa dalla <span class="math inline">\(p(x)\)</span> è unitaria<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>;</p></li>
<li><p><span class="math inline">\(p(a &lt; x &lt; b) = \int_a^b p(x) \,\operatorname {d}\!x\)</span>, se <span class="math inline">\(a \leq b\)</span>, ovvero, l’area sottesa dalla <span class="math inline">\(p(y)\)</span> tra due punti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> corrisponde alla probabilità che la v.c. <span class="math inline">\(x\)</span> assuma un valore compresto tra questi due estremi.</p></li>
</ul>
<p><em>Interpretazione.</em> È possibile che <span class="math inline">\(p(x) &gt; 1\)</span>, quindi una densità di probabilità non può essere interpretata come una probabilità. Piuttosto, la densità <span class="math inline">\(p(x)\)</span> può essere utilizzata per confrontare la fiducia relativa che può essere assegnata a diversi valori <span class="math inline">\(x\)</span>. Considerata una variabile casuale <span class="math inline">\(X\)</span> di cui è disponibile un insieme di realizzazioni, possiamo dire che, se consideriamo due valori <span class="math inline">\(x_k\)</span> e <span class="math inline">\(x_l\)</span> con <span class="math inline">\(p(x_k) &gt; p(x_l)\)</span>, allora possiamo concludere che è più probabile, in termini relativi, osservare realizzazioni <span class="math inline">\(X\)</span> nell’intorno di <span class="math inline">\(x_k\)</span> piuttosto che nell’intorno di <span class="math inline">\(x_l\)</span>.</p>
</div>
<div id="la-funzione-di-ripartizione" class="section level2 hasAnchor" number="10.6">
<h2 class="hasAnchor"><span class="header-section-number">10.6</span> La funzione di ripartizione<a href="#la-funzione-di-ripartizione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La funzione di ripartizione <span class="math inline">\(F(X)\)</span> è quella funzione che associa a ogni valore di una variabile casuale <span class="math inline">\(X\)</span> la probabilità che la variabile assuma valore minore o uguale a un prefissato valore <span class="math inline">\(x_k\)</span>. Come nel caso discreto, anche nel caso continuo la funzione di ripartizione è sempre non negativa, monotona non decrescente tra <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span>, tale che:</p>
<p><span class="math display">\[
\lim_{x \to -\infty} F_x(X) = F_X(-\infty) = 0, \quad \lim_{x \to +\infty} F_X(X) = F_X(+\infty) = 1.
\]</span></p>
<!-- Se $X$ è una variabile aleatoria discreta, tale funzione può essere espressa nel seguente modo: -->
<!-- $$ -->
<!-- F(x_k) = P(X \leq x_k) = \sum_{i=1}^k P(X = x_i), -->
<!-- $$ -->
<!-- dove $P(X = xi)$ rappresenta la probabilità che la variabile aleatoria $X$ assuma la modalità $x_i$.  -->
<p>Se <span class="math inline">\(X\)</span> è una variabile aleatoria continua, la funzione di ripartizione è:</p>
<p><span class="math display">\[
F(x_k) = P(X \leq x_k) = \int_{-\infty}^{x_k} f(x) \,\operatorname {d}\!x .
\]</span></p>
<!--chapter:end:019_density_func.Rmd-->
</div>
</div>
<div id="exp-val-and-variance-rv" class="section level1 hasAnchor" number="11">
<h1 class="hasAnchor"><span class="header-section-number">11</span> Valore atteso e varianza<a href="#exp-val-and-variance-rv" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Spesso risulta utile fornire una rappresentazione sintetica della distribuzione di una variabile casuale attraverso degli indicatori caratteristici piuttosto che fare riferimento ad una sua rappresentazione completa mediante la funzione di ripartizione, o la funzione di massa o di densità di probabilità. Una descrizione più sintetica di una variabile casuale, tramite pochi valori, ci consente di cogliere le caratteristiche essenziali della distribuzione, quali: la posizione, cioè il baricentro della distribuzione di probabilità; la variabilità, cioè la dispersione della distribuzione di probabilità attorno ad un centro; la forma della distribuzione di probabilità, considerando la simmetria e la curtosi (pesantezza delle code). In questo Capitolo introdurremo quegli indici sintetici che descrivono il centro di una distribuzione di probabilità e la sua variabilità.</p>
<div id="valore-atteso" class="section level2 hasAnchor" number="11.1">
<h2 class="hasAnchor"><span class="header-section-number">11.1</span> Valore atteso<a href="#valore-atteso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Quando vogliamo conoscere il comportamento tipico di una variabile casuale spesso vogliamo sapere qual è il suo “valore tipico”. La nozione di “valore tipico”, tuttavia, è ambigua. Infatti, essa può essere definita in almeno tre modi diversi:</p>
<ul>
<li>la <em>media</em> (somma dei valori divisa per il numero dei valori),</li>
<li>la <em>mediana</em> (il valore centrale della distribuzione, quando la variabile è ordinata in senso crescente o decrescente),</li>
<li>la <em>moda</em> (il valore che ricorre più spesso).</li>
</ul>
<p>Per esempio, la media di <span class="math inline">\(\{3, 1, 4, 1, 5\}\)</span> è <span class="math inline">\(\frac{3+1+4+1+5}{5} = 2.8\)</span>, la mediana è <span class="math inline">\(3\)</span> e la moda è <span class="math inline">\(1\)</span>. Tuttavia, la teoria delle probabilità si occupa di variabili casuali piuttosto che di sequenze di numeri. Diventa dunque necessario precisare che cosa intendiamo per “valore tipico” quando facciamo riferimento alle variabili casuali. Giungiamo così alla seguente definizione.
<!-- Supponiamo di ripetere un esperimento casuale molte volte in modo tale che ciascun valore $Y$ si presenti con una frequenza approssimativamente uguale alla sua probabilità. Per esempio, possiamo lanciare una coppia di dadi e osservare i valori di $S$ = "somma dei punti" e di $P$ = "prodotto dei punti". Ci poniamo il problema di definire il "risultato tipico" di questo esperimento in modo tale che esso corrisponda al valore medio dei valori della variabile casuale, quando l'esperimento casuale viene ripetuto tante volte.  --></p>
<div class="definition">
<p><span id="def:unlabeled-div-40" class="definition"><strong>(#def:unlabeled-div-40) </strong></span>Sia <span class="math inline">\(Y\)</span> è una variabile casuale discreta che assume i valori <span class="math inline">\(y_1, \dots, y_n\)</span> con distribuzione <span class="math inline">\(p(y)\)</span>,
ossia</p>
<p><span class="math display">\[
P(Y = y_i) = p(y_i),
\]</span></p>
<p>per definizione il <em>valore atteso</em> di <span class="math inline">\(Y\)</span>, <span class="math inline">\(\E(Y)\)</span>, è</p>
<p><span class="math display">\[\begin{equation}
\E(Y) = \sum_{i=1}^n y_i \cdot p(y_i).
(\#eq:expval-discr)
\end{equation}\]</span></p>
</div>
<p>A parole: il valore atteso (o speranza matematica, o aspettazione, o valor medio) di una variabile casuale è definito come la somma di tutti i valori che la variabile casuale può prendere, ciascuno pesato dalla probabilità con cui il valore è preso.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-41" class="exercise"><strong>(#exr:unlabeled-div-41) </strong></span>Calcoliamo il valore atteso della variabile casuale <span class="math inline">\(Y\)</span> corrispondente al lancio di una moneta equilibrata (testa: <em>Y</em> = 1; croce: <em>Y</em> = 0).</p>
<p><span class="math display">\[
\E(Y) = \sum_{i=1}^{2} y_i \cdot P(y_i) = 0 \cdot \frac{1}{5} + 1 \cdot \frac{1}{5} = 0.5.
\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-42" class="exercise"><strong>(#exr:unlabeled-div-42) </strong></span>Supponiamo ora che <em>Y</em> sia il risultato del lancio di un dado equilibrato. Il valore atteso di <em>Y</em> diventa:</p>
<p><span class="math display">\[
\E(Y) = \sum_{i=1}^{6} y_i \cdot P(y_i) = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \dots + 6 \cdot \frac{1}{6} = \frac{21}{6} = 3.5.
\]</span></p>
</div>
<div id="interpretazione" class="section level3 hasAnchor" number="11.1.1">
<h3 class="hasAnchor"><span class="header-section-number">11.1.1</span> Interpretazione<a href="#interpretazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Che interpretazione può essere assegnata alla nozione di valore atteso? Bruno de Finetti adottò lo stesso termine di <em>previsione</em> (e lo stesso simbolo) tanto per la probabilità che per la speranza matematica. Si può pertanto dire che, dal punto di vista bayesiano, la speranza matematica è l’estensione naturale della nozione di probabilità soggettiva.</p>
</div>
<div id="proprietà-del-valore-atteso" class="section level3 hasAnchor" number="11.1.2">
<h3 class="hasAnchor"><span class="header-section-number">11.1.2</span> Proprietà del valore atteso<a href="#proprietà-del-valore-atteso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La proprietà più importante del valore atteso è la linearità: il valore atteso di una somma di variabili casuali è uguale alla somma dei lori rispettivi valori attesi:</p>
<p><span class="math display">\[\begin{equation}
\E(X + Y) = \E(X) + \E(Y).
(\#eq:prop-expval-linearity)
\end{equation}\]</span></p>
<p>La @ref(eq:prop-expval-linearity) sembra ragionevole quando <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono indipendenti, ma è anche vera quando <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono associati. Abbiamo anche che</p>
<p><span class="math display">\[\begin{equation}
\E(cY) = c \E(Y).
(\#eq:prop-expval-const)
\end{equation}\]</span></p>
<p>La @ref(eq:prop-expval-const) ci dice che possiamo estrarre una costante dall’operatore di valore atteso. Tale proprietà si estende a qualunque numero di variabili casuali. Infine, se due variabili casuali <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono indipendenti, abbiamo che</p>
<p><span class="math display">\[\begin{equation}
\E(X Y) = \E(X) \E(Y).
(\#eq:expval-prod-ind-rv)
\end{equation}\]</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-43" class="exercise"><strong>(#exr:unlabeled-div-43) </strong></span>Si considerino le seguenti variabili casuali: <span class="math inline">\(Y\)</span>, ovvero il numero che si ottiene dal lancio di un dado equilibrato, e <span class="math inline">\(Y\)</span>, il numero di teste prodotto dal lancio di una moneta equilibrata.
Poniamoci il problema di trovare il valore atteso di <span class="math inline">\(X+Y\)</span>.</p>
<p>Per risolvere il problema iniziamo a costruire lo spazio campionario dell’esperimento casuale consistente nel lancio di un dado e di una moneta.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x/ y\)</span></th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">(0, 1)</td>
<td align="center">(0, 2)</td>
<td align="center">(0, 3)</td>
<td align="center">(0, 4)</td>
<td align="center">(0, 5)</td>
<td align="center">(0, 6)</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">(1, 1)</td>
<td align="center">(1, 2)</td>
<td align="center">(1, 3)</td>
<td align="center">(1, 4)</td>
<td align="center">(1, 5)</td>
<td align="center">(1, 6)</td>
</tr>
</tbody>
</table>
<p>
ovvero</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x/ y\)</span></th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<p>
Il risultato del lancio del dado è indipendente dal risultato del lancio della moneta. Pertanto, ciascun evento elementare dello spazio campionario avrà la stessa probabilità di verificarsi, ovvero <span class="math inline">\(Pr(\omega) = \frac{1}{12}\)</span>. Il valore atteso di <span class="math inline">\(X+Y\)</span> è dunque uguale a:</p>
<p><span class="math display">\[
\E(X+Y) = 1 \cdot \frac{1}{12} + 2 \cdot \frac{1}{12} + \dots + 7 \cdot \frac{1}{12} = 4.0.
\]</span></p>
<p>Lo stesso risultato si ottiene nel modo seguente:</p>
<p><span class="math display">\[
\E(X+Y) = \E(X) + E(Y) = 3.5 + 0.5 = 4.0.
\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-44" class="exercise"><strong>(#exr:unlabeled-div-44) </strong></span>Si considerino le variabili casuali <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> definite nel caso del lancio di tre monete equilibrate, dove <span class="math inline">\(X\)</span> conta il numero delle teste nei tre lanci e <span class="math inline">\(Y\)</span> conta il numero delle teste al primo lancio. Si calcoli il valore atteso del prodotto delle variabili casuali <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>La distribuzione di probabilità congiunta <span class="math inline">\(P(X, Y)\)</span> è fornita nella tabella seguente.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x/ y\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center"><span class="math inline">\(p(Y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">1/8</td>
<td align="center">0</td>
<td align="center">1/8</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">2/8</td>
<td align="center">1/8</td>
<td align="center">3/8</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1/8</td>
<td align="center">2/8</td>
<td align="center">3/8</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0</td>
<td align="center">1/8</td>
<td align="center">1/8</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(p(y)\)</span></td>
<td align="center">4/8</td>
<td align="center">4/8</td>
<td align="center">1.0</td>
</tr>
</tbody>
</table>
<p>
Il calcolo del valore atteso di <span class="math inline">\(XY\)</span> si riduce a</p>
<p><span class="math display">\[
\E(XY) = 1 \cdot \frac{1}{8} + 2 \cdot \frac{2}{8} + 3 \cdot \frac{1}{8} = 1.0.
\]</span></p>
<p>Si noti che le variabili casuali <span class="math inline">\(Y\)</span> e <span class="math inline">\(Y\)</span> non sono indipendenti. Dunque non possiamo usare la proprietà @ref(thm:prodindrv). Infatti, il valore atteso di <span class="math inline">\(X\)</span> è</p>
<p><span class="math display">\[
\E(X) = 1 \cdot \frac{3}{8} + 2 \cdot \frac{3}{8} + 3 \cdot \frac{1}{8} = 1.5
\]</span></p>
<p>e il valore atteso di <span class="math inline">\(Y\)</span> è</p>
<p><span class="math display">\[
\E(Y) = 0 \cdot \frac{4}{8} + 1 \cdot \frac{4}{8} = 0.5.
\]</span>
Dunque</p>
<p><span class="math display">\[
1.5 \cdot 0.5 \neq 1.0.
\]</span></p>
</div>
</div>
<div id="variabili-casuali-continue" class="section level3 hasAnchor" number="11.1.3">
<h3 class="hasAnchor"><span class="header-section-number">11.1.3</span> Variabili casuali continue<a href="#variabili-casuali-continue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nel caso di una variabile casuale continua <span class="math inline">\(Y\)</span> il valore atteso diventa:</p>
<p><span class="math display">\[\begin{equation}
\E(Y) = \int_{-\infty}^{+\infty} y p(y) \,\operatorname {d}\!y
(\#eq:def-ev-rv-cont)
\end{equation}\]</span></p>
<p>Anche in questo caso il valore atteso è una media ponderata della <span class="math inline">\(y\)</span>, nella quale ciascun possibile valore <span class="math inline">\(y\)</span> è ponderato per il corrispondente valore della densità <span class="math inline">\(p(y)\)</span>. Possiamo leggere l’integrale pensando che <span class="math inline">\(y\)</span> rappresenti l’ampiezza delle barre infinitamente strette di un istogramma, con la densità <span class="math inline">\(p(y)\)</span> che corrisponde all’altezza di tali barre e la notazione <span class="math inline">\(\int_{-\infty}^{+\infty}\)</span> che corrisponde ad una somma.</p>
<p>Un’altra misura di tendenza centrale delle variabili casuali continue è la moda. La moda della <span class="math inline">\(Y\)</span> individua il valore <span class="math inline">\(y\)</span> più plausibile, ovvero il valore <span class="math inline">\(y\)</span> che massimizza la funzione di densità <span class="math inline">\(p(y)\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\Mo(Y) = \argmax_y p(y).
(\#eq:def-mode)
\end{equation}\]</span></p>
</div>
</div>
<div id="varianza-1" class="section level2 hasAnchor" number="11.2">
<h2 class="hasAnchor"><span class="header-section-number">11.2</span> Varianza<a href="#varianza-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La seconda più importante proprietà di una variabile casuale, dopo che conosciamo il suo valore atteso, è la <em>varianza</em>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-45" class="definition"><strong>(#def:unlabeled-div-45) </strong></span>Se <span class="math inline">\(Y\)</span> è una variabile casuale discreta con distribuzione <span class="math inline">\(p(y)\)</span>, per definizione la varianza di <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mathbb{V}(Y)\)</span>, è</p>
<p><span class="math display">\[\begin{equation}
\mathbb{V}(Y) = \E\Big[\big(Y - \E(Y)\big)^2\Big].
(\#eq:def-var-rv)
\end{equation}\]</span></p>
</div>
<p>A parole: la varianza è la deviazione media quadratica della variabile dalla sua media.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Se denotiamo <span class="math inline">\(\E(Y) = \mu\)</span>, la varianza <span class="math inline">\(\mathbb{V}(Y)\)</span> diventa il valore atteso di <span class="math inline">\((Y - \mu)^2\)</span>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-46" class="exercise"><strong>(#exr:unlabeled-div-46) </strong></span>Posta <span class="math inline">\(S\)</span> uguale alla somma dei punti ottenuti nel lancio di due dadi equilibrati, poniamoci il problema di calcolare la varianza di <span class="math inline">\(S\)</span>.</p>
<p>La variabile casuale <span class="math inline">\(S\)</span> ha la seguente distribuzione di probabilità:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(s\)</span></th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
<th align="center">11</th>
<th align="center">12</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(S = s)\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{6}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
</tr>
</tbody>
</table>
<p>
Essendo <span class="math inline">\(\E(S) = 7\)</span>, la varianza diventa</p>
<p><span class="math display">\[\begin{align}
\mathbb{V}(S) &amp;= \sum \left(S- \mathbb{E}(S)\right)^2 \cdot P(S) \notag\\
&amp;= (2 - 7)^2 \cdot 0.0278 + (3-7)^2 \cdot 0.0556 + \dots + (12 - 7)^2 \cdot 0.0278 \notag\\
&amp;= 5.8333.\notag
\end{align}\]</span></p>
</div>
<div id="formula-alternativa-per-la-varianza" class="section level3 hasAnchor" number="11.2.1">
<h3 class="hasAnchor"><span class="header-section-number">11.2.1</span> Formula alternativa per la varianza<a href="#formula-alternativa-per-la-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>C’è un modo più semplice per calcolare la varianza:</p>
<p><span class="math display">\[\begin{align}
\E\Big[\big(X - \E(Y)\big)^2\Big] &amp;= \E\big(X^2 - 2X\E(Y) + \E(Y)^2\big)\notag\\
&amp;= \E(Y^2) - 2\E(Y)\E(Y) + \E(Y)^2,\notag
\end{align}\]</span></p>
<p>dato che <span class="math inline">\(\E(Y)\)</span> è una costante; pertanto</p>
<p><span class="math display">\[\begin{equation}
\mathbb{V}(Y) = \E(Y^2) - \big(\E(Y) \big)^2.
(\#eq:def-alt-var-rv)
\end{equation}\]</span></p>
<p>A parole: la varianza è la media dei quadrati meno il quadrato della media.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-47" class="exercise"><strong>(#exr:unlabeled-div-47) </strong></span>Consideriamo la variabile casuale <span class="math inline">\(Y\)</span> che corrisponde al numero di teste che si osservano nel lancio di una moneta truccata con probabilità di testa uguale a 0.8.
Il valore atteso di <span class="math inline">\(Y\)</span> è</p>
<p><span class="math display">\[
\E(Y) = 0 \cdot 0.2 + 1 \cdot 0.8 = 0.8.
\]</span>
Usando la formula tradizionale della varianza otteniamo:</p>
<p><span class="math display">\[
\mathbb{V}(Y) = (0 - 0.8)^2 \cdot 0.2 + (1 - 0.8)^2 \cdot 0.8 = 0.16.
\]</span>
Lo stesso risultato si trova con la formula alternativa della varianza. Il valore atteso di <span class="math inline">\(Y^2\)</span> è</p>
<p><span class="math display">\[
\E(Y^2) = 0^2 \cdot 0.2 + 1^2 * 0.8 = 0.8.
\]</span>
e la varianza diventa</p>
<p><span class="math display">\[
\mathbb{V}(Y) = \E(Y^2) - \big(\E(Y) \big)^2 = 0.8 - 0.8^2 = 0.16.
\]</span></p>
</div>
</div>
<div id="variabili-casuali-continue-1" class="section level3 hasAnchor" number="11.2.2">
<h3 class="hasAnchor"><span class="header-section-number">11.2.2</span> Variabili casuali continue<a href="#variabili-casuali-continue-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Nel caso di una variabile casuale continua <span class="math inline">\(Y\)</span>, la varianza diventa:</p>
<p><span class="math display">\[\begin{equation}
\mathbb{V}(Y) = \int_{-\infty}^{+\infty} \large[y - \E(Y)\large]^2 p(y) \,\operatorname {d}\!y
(\#eq:def-var-rv-cont)
\end{equation}\]</span></p>
<p>Come nel caso discreto, la varianza di una v.c. continua <span class="math inline">\(y\)</span> misura approssimativamente la distanza al quadrato tipica o prevista dei possibili valori <span class="math inline">\(y\)</span> dalla loro media.</p>
</div>
</div>
<div id="deviazione-standard" class="section level2 hasAnchor" number="11.3">
<h2 class="hasAnchor"><span class="header-section-number">11.3</span> Deviazione standard<a href="#deviazione-standard" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Quando lavoriamo con le varianze, i termini sono innalzati al quadrato e quindi i numeri possono diventare molto grandi (o molto piccoli). Per trasformare nuovamente i valori nell’unità di misura della scala originaria si prende la radice quadrata. Il valore risultante viene chiamato <em>deviazione standard</em> e solitamente è denotato dalla lettera greca <span class="math inline">\(\sigma\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-48" class="definition"><strong>(#def:unlabeled-div-48) </strong></span>Si definisce scarto quadratico medio (o deviazione standard o scarto tipo) la radice quadrata della varianza:</p>
<p><span class="math display">\[\begin{equation}
\sigma_Y = \sqrt{\mathbb{V}(Y)}.
(\#eq:def-sd)
\end{equation}\]</span></p>
</div>
<p>Interpretiamo la deviazione standard di una variabile casuale come nella statistica descrittiva: misura approssimativamente la distanza tipica o prevista dei possibili valori <span class="math inline">\(y\)</span> dalla loro media.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-49" class="exercise"><strong>(#exr:unlabeled-div-49) </strong></span>Per i dadi equilibrati dell’esempio precedente, la deviazione standard della variabile casuale <span class="math inline">\(S\)</span> è uguale a <span class="math inline">\(\sqrt{5.833} = 2.415\)</span>.</p>
</div>
</div>
<div id="standardizzazione" class="section level2 hasAnchor" number="11.4">
<h2 class="hasAnchor"><span class="header-section-number">11.4</span> Standardizzazione<a href="#standardizzazione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-50" class="definition"><strong>(#def:unlabeled-div-50) </strong></span>Data una variabile casuale <span class="math inline">\(Y\)</span>, si dice variabile standardizzata di <span class="math inline">\(Y\)</span> l’espressione</p>
<p><span class="math display">\[\begin{equation}
Z = \frac{Y - \E(Y)}{\sigma_Y}.
(\#eq:standardization)
\end{equation}\]</span></p>
</div>
<p>Solitamente, una variabile standardizzata viene denotata con la lettera <span class="math inline">\(Z\)</span>.</p>
</div>
<div id="momenti-di-variabili-casuali" class="section level2 hasAnchor" number="11.5">
<h2 class="hasAnchor"><span class="header-section-number">11.5</span> Momenti di variabili casuali<a href="#momenti-di-variabili-casuali" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-51" class="definition"><strong>(#def:unlabeled-div-51) </strong></span>Si chiama <em>momento</em> di ordine <span class="math inline">\(q\)</span> di una v.c. <span class="math inline">\(X\)</span>, dotata di densità <span class="math inline">\(p(x)\)</span>, la
quantità</p>
<p><span class="math display">\[\begin{equation}
\E(X^q) = \int_{-\infty}^{+\infty} x^q p(x) \; dx.
\end{equation}\]</span></p>
<p>Se <span class="math inline">\(X\)</span> è una v.c. discreta, i suoi momenti valgono:</p>
<p><span class="math display">\[\begin{equation}
\E(X^q) = \sum_i x_i^q p(x_i).
\end{equation}\]</span></p>
</div>
<p>I momenti sono importanti parametri indicatori di certe proprietà di <span class="math inline">\(X\)</span>. I più
noti sono senza dubbio quelli per <span class="math inline">\(q = 1\)</span> e <span class="math inline">\(q = 2\)</span>. Il momento del primo ordine corrisponde al valore atteso di <span class="math inline">\(X\)</span>. Spesso i momenti di ordine superiore al primo vengono calcolati rispetto al valor medio di <span class="math inline">\(X\)</span>, operando una traslazione <span class="math inline">\(x_0 = x − \E(X)\)</span> che individua lo scarto dalla media. Ne deriva che il momento centrale di ordine 2 corrisponde alla varianza.</p>
</div>
<div id="covarianza-1" class="section level2 hasAnchor" number="11.6">
<h2 class="hasAnchor"><span class="header-section-number">11.6</span> Covarianza<a href="#covarianza-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La covarianza quantifica la tendenza delle variabili aleatorie <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> a ``variare assieme’’. Per esempio, l’altezza e il peso delle giraffe producono una covarianza positiva perché all’aumentare di una di queste due quantità tende ad aumentare anche l’altra. La covarianza misura la forza e la direzione del legame lineare tra due variabili aleatorie <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span>. Si utilizza la notazione <span class="math inline">\(\mbox{Cov}(X,Y)=\sigma_{xy}\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-52" class="definition"><strong>(#def:unlabeled-div-52) </strong></span>Date due variabili aleatorie <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, chiamiamo covarianza tra <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span> il numero</p>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(X,Y) = \mathbb{E}\Bigl(\bigl(X - \mathbb{E}(X)\bigr) \bigl(Y - \mathbb{E}(Y)\bigr)\Bigr),
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(\mathbb{E}(X)\)</span> e <span class="math inline">\(\mathbb{E}(Y)\)</span> sono i valori attesi di <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span>.</p>
</div>
<p>In maniera esplicita,</p>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(X,Y) = \sum_{(x,y) \in \Omega} (x - \mu_X) (y - \mu_Y) f(x, y).
\label{eq:cov_def}
\end{equation}\]</span></p>
<p>La definizione è analoga, algebricamente, a quella di varianza e risulta infatti</p>
<p><span class="math display">\[\begin{equation}
\mathbb{V}(x) = cov(X, X)
\end{equation}\]</span></p>
<p>e</p>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X).
\label{eq:cov_vc_alt}
\end{equation}\]</span></p>
<div class="proof">
<p><span id="unlabeled-div-53" class="proof"><em>Proof</em>. </span>La proprietà precedente si dimostra nel modo seguente:</p>
<p><span class="math display">\[\begin{align}
\mbox{Cov}(X,Y) &amp;= \mathbb{E}\Bigl(\bigl(X-\mathbb{E}(X)\bigr) \bigl(Y-\mathbb{E}(Y)\bigr)\Bigr)\notag\\
          %&amp;= \mathbb{E}(XY) - \mathbb{E}(Y)X -\mathbb{E}(X)Y + \mathbb{E}(X)\mathbb{E}(Y) )\notag\\
          &amp;= \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X) - \mathbb{E}(X)\mathbb{E}(Y) + \mathbb{E}(X)\mathbb{E}(Y)\notag\\
          &amp;= \mathbb{E}(XY) - \mathbb{E}(Y)\mathbb{E}(X)\notag.
\end{align}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-54" class="exercise"><strong>(#exr:unlabeled-div-54) </strong></span>Consideriamo le variabili casuali definite nell’Esercizio 2.4. Si calcoli la covarianza di <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>Abbiamo che <span class="math inline">\(\mu_X = 1.5\)</span> e <span class="math inline">\(\mu_Y = 0.5\)</span>. Ne segue che la covarianza di <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> è:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mbox{Cov}(X,Y) &amp;= \sum_{(x,y) \in\ \Omega} (x - \mu_X) (y - \mu_Y) f(x, y)\\
&amp;= (0-1.5)(0-0.5)\cdot \frac{1}{8} + (0-1.5)(1-0.5) \cdot 0 \\
   &amp;\hskip0.05\textwidth\relax + (1-1.5)(0-0.5)\cdot \frac{2}{8} + (1-1.5)(1-0.5) \cdot \frac{1}{8} \\
    &amp;\hskip0.05\textwidth\relax + (2-1.5)(0-0.5) \cdot \frac{1}{8} + (2-1.5)(1-0.5) \cdot \frac{2}{8} \\
   &amp;\hskip0.05\textwidth\relax + (3-1.5)(0-0.5) \cdot 0 +  (3-1.5)(1-0.5)\cdot\frac{1}{8} \\
   &amp;= \frac{1}{4}. \notag
\end{split}
\end{equation}\]</span></p>
<p>Lo stesso risultato può essere trovato nel modo seguente. Iniziamo a calcolare il valore atteso del prodotto <span class="math inline">\(XY\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}(XY) = 0 \cdot\frac{4}{8} + 1 \cdot\frac{1}{8} + 2 \cdot\frac{2}{8} + 3 \cdot\frac{1}{8} = 1.0.
\]</span></p>
<p>Dunque, la covarianza tra <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> diventa</p>
<p><span class="math display">\[\begin{align}
\mbox{Cov}(X,Y) &amp;= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)\notag\\
&amp;= 1 -  1.5\cdot 0.5 \notag\\
&amp;= 0.25.\notag
\end{align}\]</span></p>
</div>
</div>
<div id="correlazione-1" class="section level2 hasAnchor" number="11.7">
<h2 class="hasAnchor"><span class="header-section-number">11.7</span> Correlazione<a href="#correlazione-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La covarianza dipende dall’unità di misura delle due variabili e quindi non consente di stabilire l’intensità della relazione.
Una misura standardizzata della relazione che intercorre fra due variabili è invece rappresentata dalla correlazione. La correlazione si ottiene dividendo la covarianza per le deviazioni standard delle due variabili aleatorie.</p>
<div class="defn">
<p>Il coefficiente di correlazione tra <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span> è il numero definito da</p>
<p><span class="math display">\[\begin{equation}
\rho(X,Y) =\frac{\mbox{Cov}(X,Y)}{\sqrt{\mathcal{V}(X)\mathcal{V}(Y)}}.
\end{equation}\]</span></p>
</div>
<p>Si può anche scrivere <span class="math inline">\(\rho_{X,Y}\)</span> al posto di <span class="math inline">\(\rho(X,Y)\)</span>.</p>
<p>Il coefficiente di correlazione <span class="math inline">\(\rho_{xy}\)</span> è un numero puro, cioè non
dipende dall’unità di misura delle variabili, e assume valori compresi tra -1 e +1.</p>
</div>
<div id="proprietà-1" class="section level2 hasAnchor" number="11.8">
<h2 class="hasAnchor"><span class="header-section-number">11.8</span> Proprietà<a href="#proprietà-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>La covarianza tra una variabile aleatoria <span class="math inline">\(X\)</span> e una costante <span class="math inline">\(c\)</span> è nulla:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(c,X) = 0,
\end{equation}\]</span></p>
<ul>
<li>la covarianza è simmetrica:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(X,Y) = \mbox{Cov}(Y,X),
\end{equation}\]</span></p>
<ul>
<li>vale</li>
</ul>
<p><span class="math display">\[\begin{equation}
-1 \leq \rho(X,Y) \leq 1,
\end{equation}\]</span></p>
<ul>
<li>la correlazione non dipende dall’unità di misura:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\rho(aX, bY) = \rho(X,Y), \qquad \forall a, b &gt; 0,
\end{equation}\]</span></p>
<ul>
<li>se <span class="math inline">\(Y = a + bX\)</span> è una funzione lineare di <span class="math inline">\(X\)</span> con costanti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>, allora <span class="math inline">\(\rho(X,Y) = \pm 1\)</span>, a seconda del segno di <span class="math inline">\(b\)</span>,</li>
<li>la covarianza tra <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, ciascuna moltiplicata per una costante, è uguale al prodotto delle costanti per la covarianza tra <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(aX,bY) = ab \;\mbox{Cov}(X,Y), \qquad \forall a,b \in \Real,
\end{equation}\]</span></p>
<ul>
<li>vale</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mathbb{V}(X \pm Y) = \mathbb{V}(X) + \mathbb{V}(Y) \pm 2 \cdot \mbox{Cov}(X,Y),
\end{equation}\]</span></p>
<ul>
<li>vale</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}(X + Y, Z) = \mbox{Cov}(X,Z) + \mbox{Cov}(Y,Z),
\end{equation}\]</span></p>
<ul>
<li>per una sequenza di variabili aleatorie <span class="math inline">\(X_1, \dots, X_n\)</span>, si ha</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mathbb{V}\left( \sum_{i=1}^n X_i\right) = \sum_{i=1}^n
\mathbb{V}(X_i) + 2\sum_{i,j: i&lt;j}cov(X_i, X_j),
\end{equation}\]</span></p>
<ul>
<li>vale</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^m b_jY_j\right) = \sum_{i=1}^n \sum_{j=1}^m a_j b_j\mbox{Cov}(X_j, Y_j),
\end{equation}\]</span></p>
<ul>
<li>se <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> sono indipendenti, allora</li>
</ul>
<p><span class="math display">\[\begin{equation}
\mbox{Cov}\left(\sum_{i=1}^n a_i X_i, \sum_{j=1}^n b_jX_j\right) = \sum_{i=1}^n a_i b_i \mathbb{V}(X_i).
\end{equation}\]</span></p>
<div id="incorrelazione" class="section level3 hasAnchor" number="11.8.1">
<h3 class="hasAnchor"><span class="header-section-number">11.8.1</span> Incorrelazione<a href="#incorrelazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si dice che <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span> sono incorrelate, o linermente indipendenti, se la loro covarianza è nulla,</p>
<p><span class="math display">\[\begin{equation}
\sigma_{XY} = \mathbb{E} \big[(X - \mu_X) (y-\mu_u) \big] = 0,
\end{equation}\]</span></p>
<p>che si può anche scrivere come</p>
<p><span class="math display">\[\begin{equation}
\rho_{XY} = 0, \quad \mathbb{E}(XY) = \mathbb{E}(X) \mathbb{E}(Y).
\end{equation}\]</span></p>
<p>Si introduce così un secondo tipo di indipendenza, più debole, dopo quello di indipendenza stocastica. Viceversa, però, se <span class="math inline">\(\mbox{Cov}(X, Y) = 0\)</span>, non è detto che <span class="math inline">\(X\)</span> ed <span class="math inline">\(Y\)</span> siano indipendenti.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-55" class="exercise"><strong>(#exr:unlabeled-div-55) </strong></span>Siano <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> due variabili aleatorie discrete avente una distribuzione di massa di probabilità congiunta pari a</p>
<p><span class="math display">\[
f_{XY}(x,y) = \frac{1}{4} \quad (x,y) \in \{(0,0), (1,1), (1, -1), (2,0) \}
\]</span></p>
<p>e zero altrimenti. Si calcoli la covarianza <span class="math inline">\(\rho_{XY}\)</span>. Le due variabili aleatorie <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono mutuamente indipendenti?</p>
<p>La distribuzione marginale della <span class="math inline">\(X\)</span> è</p>
<p><span class="math display">\[
\begin{cases}
X = 0, \quad  P_X = 1/4, \\
X = 1, \quad P_X = 2/4, \\
X = 2, \quad P_X = 1/4.
\end{cases}
\]</span></p>
<p><span class="math display">\[
\mathbb{E}(X) = 0 \frac{1}{4} + 1 \frac{2}{4} + 2 \frac{1}{4} = 1.
\]</span></p>
<p><span class="math display">\[
\mathbb{E}(X^2) = 0^2 \frac{1}{4} + 1^2 \frac{2}{4} + 2^2 \frac{1}{4} = \frac{3}{2}.
\]</span></p>
<p><span class="math display">\[
\mathbb{V}(X) = \frac{3}{2} - 1^2 = \frac{1}{2}.
\]</span></p>
<p>La distribuzione marginale della <span class="math inline">\(Y\)</span> è</p>
<p><span class="math display">\[
\begin{cases}
Y = -1, \quad  P_Y = 1/4, \\
Y = 0, \quad P_Y = 2/4, \\
Y = 1, \quad P_Y = 1/4.
\end{cases}
\]</span></p>
<p><span class="math display">\[
\mathbb{E}(Y) = 0 \frac{2}{4} + 1 \frac{1}{4} + (-1) \frac{1}{4} = 0.
\]</span></p>
<p><span class="math display">\[
\mathbb{E}(Y^2) = 0^2 \frac{2}{4} + 1^2 \frac{1}{4} + (-1)^2 \frac{1}{4} = \frac{1}{2}.
\]</span></p>
<p><span class="math display">\[
\mathbb{V}(X) = \frac{1}{2} - 0^2 = \frac{1}{2}.
\]</span></p>
<p>Calcoliamo ora la covarianza tra <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}(XY) = \sum_x\sum_y xy f_{XY} (x,y) =
(0\cdot 0)\frac{1}{4} +
(1\cdot 1)\frac{1}{4} +
(1\cdot -1)\frac{1}{4} +
(2\cdot 0)\frac{1}{4} = 0.
\]</span></p>
<p><span class="math display">\[
\mbox{Cov}(X,Y) = \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = 0 - 1\cdot0 = 0.
\]</span></p>
<p>Quindi le due variabili aleatorie hanno covarianza pari a zero. Tuttavia, esse non sono indipendenti, in quanto non è vero che</p>
<p><span class="math display">\[
f_{XY} (x,y) = f_X(x) f_Y(y)
\]</span></p>
<p>per tutti gli <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. In conclusione, anche se condizione di indipendenza implica una covarianza nulla, questo esempio mostra come l’inverso non sia necessariamente vero. La covarianza può essere zero anche quando le due variabili aleatorie non sono indipendenti.</p>
</div>
</div>
</div>
<div id="conclusioni" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Conclusioni<a href="#conclusioni" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La densità di probabilità congiunta bivariata tiene simultaneamente conto del comportamento di due variabili aleatorie <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> e di come esse si influenzino. Se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono legate linearmente, allora il coefficiente di correlazione</p>
<p><span class="math display">\[\begin{equation}
\rho = \frac{\mbox{Cov}(X, Y)}{\sigma_X \sigma_Y}\notag
\end{equation}\]</span></p>
<p>fornisce l’indice maggiormente utilizzato per descrivere l’intensità e il segno dell’associazione lineare. Nel caso di un’associazione lineare perfetta, <span class="math inline">\(Y = a + bX\)</span>, avremo <span class="math inline">\(\rho = 1\)</span> con <span class="math inline">\(b\)</span> positivo ed <span class="math inline">\(\rho = -1\)</span> con <span class="math inline">\(b\)</span> negativo. Se il coefficiente di correlazione è pari a 0 le variabili si dicono incorrelate. Condizione sufficiente (ma non necessaria) affinché <span class="math inline">\(\rho = 0\)</span> è che le due variabili siano tra loro indipendenti.</p>
<!--chapter:end:020_expval_var.Rmd-->
</div>
</div>
<div id="part-distribuzioni-teoriche-di-probabilità" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART) Distribuzioni teoriche di probabilità<a href="#part-distribuzioni-teoriche-di-probabilità" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="distr-rv-discr" class="section level1 hasAnchor" number="12">
<h1 class="hasAnchor"><span class="header-section-number">12</span> Distribuzioni di v.c. discrete<a href="#distr-rv-discr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In questo Capitolo verranno esaminate le principali distribuzioni di probabilità delle variabili casuali discrete. Un esperimento casuale che può dare luogo a solo due possibili esiti (successo, insuccesso) è modellabile con una variabile casuale di Bernoulli. Una sequenza di prove di Bernoulli costituisce un processo Bernoulliano. Il numero di successi dopo <span class="math inline">\(n\)</span> prove di Bernoulli corrisponde ad una variabile casuale che segue la legge binomiale. La distribuzione binomiale risulta da un insieme di prove di Bernoulli solo se il numero totale <span class="math inline">\(n\)</span> è fisso per disegno. Se il numero di prove è esso stesso una variabile casuale, allora il numero di successi nella corrispondente sequenza di prove bernoulliane segue al distribuzione di Poisson. Concluderemo con la distribuzione discreta uniforme.</p>
<div id="una-prova-bernoulliana" class="section level2 hasAnchor" number="12.1">
<h2 class="hasAnchor"><span class="header-section-number">12.1</span> Una prova Bernoulliana<a href="#una-prova-bernoulliana" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Se un esperimento casuale ha solo due esiti possibili, allora le repliche indipendenti di questo esperimento sono chiamate “prove Bernoulliane” (il lancio di una moneta è il tipico esempio).</p>
<div class="definition">
<p><span id="def:unlabeled-div-56" class="definition"><strong>(#def:unlabeled-div-56) </strong></span>Viene detta variabile di Bernoulli una variabile casuale discreta <span class="math inline">\(Y = \{0, 1\}\)</span> con la seguente distribuzione di probabilità:</p>
<p><span class="math display">\[
P(Y \mid \theta) =
  \begin{cases}
    \theta     &amp; \text{se $Y = 1$}, \\
    1 - \theta &amp; \text{se $Y = 0$},
  \end{cases}
\]</span></p>
<p>con <span class="math inline">\(0 \leq \theta \leq 1\)</span>. Convenzionalmente l’evento <span class="math inline">\(\{Y = 1\}\)</span> con probabilità <span class="math inline">\(\theta\)</span> viene chiamato “successo” mentre l’evento <span class="math inline">\(\{Y = 0\}\)</span> con probabilità <span class="math inline">\(1-\theta\)</span> viene chiamato “insuccesso”.</p>
</div>
<p>Applicando l’operatore di valore atteso e di varianza, otteniamo</p>
<p><span class="math display">\[\begin{align}
\E(Y) &amp;= 0 \cdot P(Y=0) + 1 \cdot P(Y=1) = \theta, \\
\Var(Y) &amp;= (0 - \theta)^2 \cdot P(Y=0) + (1 - \theta)^2 \cdot P(Y=1) = \theta(1-\theta).
(\#eq:ev-var-bern)
\end{align}\]</span></p>
<p>Scriviamo <span class="math inline">\(Y \sim \Bernoulli(\theta)\)</span> per indicare che la variabile casuale <span class="math inline">\(Y\)</span> ha una distribuzione Bernoulliana di parametro <span class="math inline">\(\theta\)</span>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-57" class="exercise"><strong>(#exr:unlabeled-div-57) </strong></span>Nel caso del lancio di una moneta equilibrata la variabile casuale di Bernoulli assume i valori <span class="math inline">\(0\)</span> e <span class="math inline">\(1\)</span>. La distribuzione di massa di probabilità è pari a <span class="math inline">\(\frac{1}{2}\)</span> in corrispondenza di entrambi iv valori. La funzione di distribuzione vale <span class="math inline">\(\frac{1}{2}\)</span> per <span class="math inline">\(Y = 0\)</span> e <span class="math inline">\(1\)</span> per <span class="math inline">\(Y = 1\)</span>.</p>
</div>
</div>
<div id="una-sequenza-di-prove-bernoulliane" class="section level2 hasAnchor" number="12.2">
<h2 class="hasAnchor"><span class="header-section-number">12.2</span> Una sequenza di prove Bernoulliane<a href="#una-sequenza-di-prove-bernoulliane" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione binomiale è rappresentata dall’elenco di tutti i
possibili numeri di successi <span class="math inline">\(Y = \{0, 1, 2, \dots n\}\)</span> che possono
essere osservati in <span class="math inline">\(n\)</span> prove Bernoulliane indipendenti di probabilità
<span class="math inline">\(\theta\)</span>, a ciascuno dei quali è associata la relativa probabilità. Esempi di una distribuzione binomiale sono i risultati di una serie di lanci di
una stessa moneta o di una serie di estrazioni da un’urna (con
reintroduzione). La distribuzione binomiale di parametri <span class="math inline">\(n\)</span> e <span class="math inline">\(\theta\)</span> è in realtà una famiglia di distribuzioni: al variare dei parametri <span class="math inline">\(\theta\)</span> e <span class="math inline">\(n\)</span> variano le probabilità.</p>
<div class="definition">
<p><span id="def:unlabeled-div-58" class="definition"><strong>(#def:unlabeled-div-58) </strong></span>La probabilità di ottenere <span class="math inline">\(y\)</span> successi e <span class="math inline">\(n-y\)</span> insuccessi in <span class="math inline">\(n\)</span> prove
Bernoulliane è data dalla distribuzione binomiale:</p>
<p><span class="math display">\[\begin{align}
P(Y=y) &amp;= \binom{n}{y} \theta^{y} (1-\theta)^{n-y} \notag \\
&amp;= \frac{n!}{y!(n-y)!} \theta^{y} (1-\theta)^{n-y},
(\#eq:binomialdistribution)
\end{align}\]</span></p>
<p>dove <span class="math inline">\(n\)</span> = numero di prove Bernoulliane, <span class="math inline">\(\theta\)</span> = probabilità di successo in ciascuna prova e <span class="math inline">\(y\)</span> = numero di successi.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-59" class="proof"><em>Proof</em>. </span>La @ref(eq:binomialdistribution) può essere derivata nel modo seguente. Indichiamo con <span class="math inline">\(S\)</span> il successo e con <span class="math inline">\(I\)</span> l’insuccesso di ciascuna prova. Una sequenza di <span class="math inline">\(n\)</span> prove Bernoulliane darà come esito una sequenza di <span class="math inline">\(n\)</span> elementi <span class="math inline">\(S\)</span> e <span class="math inline">\(I\)</span>. Ad esempio, una sequenza che contiene <span class="math inline">\(y\)</span> successi è la seguente:</p>
<p><span class="math display">\[
\overbrace{SS\dots S}^\text{$y$ volte} \overbrace{II\dots I}^\text{$n-y$ volte}
\]</span>
Essendo <span class="math inline">\(\theta\)</span> la probabilità di <span class="math inline">\(S\)</span> e <span class="math inline">\(1-\theta\)</span> la probabilità di <span class="math inline">\(I\)</span>, la probabilità di ottenere la specifica sequenza riportata sopra è</p>
<p><span class="math display">\[\begin{equation}
\overbrace{\theta \theta\dots \theta}^\text{$y$ volte} \overbrace{(1-\theta)(1-\theta)\dots (1-\theta)}^\text{$n-y$ volte} = \theta^y \cdot (1-\theta)^{n-y}.
(\#eq:demo-bino-kernel)
\end{equation}\]</span></p>
<p>Non siamo però interessati alla probabilità di una <em>specifica</em> sequenza di <span class="math inline">\(S\)</span> e <span class="math inline">\(I\)</span> ma, bensì, alla probabilità di osservare una <em>qualsiasi</em> sequenza di <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove. In altre parole, vogliamo la probabilità dell’unione di tutti gli eventi corrispondenti a <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove.</p>
<p>È immediato notare che una qualsiasi altra sequenza contenente esattamente <span class="math inline">\(y\)</span> successi avrà sempre come probabilità <span class="math inline">\(\theta^y \cdot (1-\theta)^{n-y}\)</span>: il prodotto infatti resta costante anche se cambia l’ordine dei fattori.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> Per trovare il risultato cercato dobbiamo moltiplicare la @ref(eq:demo-bino-kernel) per il numero di sequenze possibili di <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove.</p>
<p>Il numero di sequenze che contengono esattamente <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove. La risposta è fornita dal coefficiente binomiale<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>:</p>
<p><span class="math display">\[\begin{equation}
\binom{n}{y} = \frac{n!}{y!(n-y)!},
(\#eq:binomial-coefficient)
\end{equation}\]</span></p>
<p>dove il simbolo <span class="math inline">\(n!\)</span> si legge <span class="math inline">\(n\)</span> fattoriale ed è uguale al prodotto di <span class="math inline">\(n\)</span> numeri interi decrescenti a partire da <span class="math inline">\(n\)</span>. Per definizione <span class="math inline">\(0! = 1\)</span>.</p>
<p>Essendo la probabilità dell’unione di <span class="math inline">\(K\)</span> elementi incompatibili uguale alla somma delle loro rispettive probabilità, e dato che le sequenze di <span class="math inline">\(y\)</span> successi in <span class="math inline">\(n\)</span> prove hanno tutte la stessa probabilità, per trovare la formula della distributione binomiale @ref(eq:binomialdistribution) è sufficiente moltiplicare la @ref(eq:demo-bino-kernel) per la @ref(eq:binomial-coefficient).</p>
</div>
<p>La distribuzione di probabilità di alcune distribuzioni binomiali, per due valori di <span class="math inline">\(n\)</span> e <span class="math inline">\(\theta\)</span>, è fornita nella figura @ref(fig:example-binomial-distr).</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/example-binomial-distr-1.png" alt="Alcune distribuzioni binomiali. Nella figura, il parametro $\theta$ è indicato con $p$." width="576" />
<p class="caption">
(#fig:example-binomial-distr)Alcune distribuzioni binomiali. Nella figura, il parametro <span class="math inline">\(\theta\)</span> è indicato con <span class="math inline">\(p\)</span>.
</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-60" class="exercise"><strong>(#exr:unlabeled-div-60) </strong></span>Usando la @ref(eq:binomialdistribution), si trovi la probabilità di <span class="math inline">\(y = 2\)</span> successi in <span class="math inline">\(n = 4\)</span> prove Bernoulliane indipendenti con <span class="math inline">\(\theta = 0.2\)</span></p>
<p><span class="math display">\[
\begin{aligned}
P(Y=2) &amp;= \frac{4!}{2!(4-2)!} 0.2^{2} (1-0.2)^{4-2} \notag  \\
&amp;= \frac{4 \cdot 3 \cdot 2 \cdot 1}{(2 \cdot 1)(2 \cdot 1)}
0.2^{2} 0.8^{2} = 0.1536. \notag
\end{aligned}
\]</span></p>
<p>Ripetendo i calcoli per i valori <span class="math inline">\(y = 0, \dots, 4\)</span> troviamo la distribuzione binomiale di parametri <span class="math inline">\(n = 4\)</span> e <span class="math inline">\(\theta = 0.2\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center">y</th>
<th align="center">P(Y = y)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0.4096</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">0.4096</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">0.1536</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">0.0256</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">0.0016</td>
</tr>
<tr class="even">
<td align="center">sum</td>
<td align="center">1.0</td>
</tr>
</tbody>
</table>
<p>Lo stesso risultato si ottiene usando la sequente istruzione :</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.2</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4096 0.4096 0.1536 0.0256 0.0016</span></span></code></pre></div>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-61" class="exercise"><strong>(#exr:unlabeled-div-61) </strong></span>Lanciando <span class="math inline">\(5\)</span> volte una moneta onesta, qual è la probabilità che esca testa almeno tre volte?</p>
<p>In , la soluzione si trova con</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">dbinom</span>(<span class="dv">4</span>, <span class="dv">5</span>, <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">dbinom</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.5</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5</span></span></code></pre></div>
<p>Alternativamente, possiamo trovare la probabilità dell’evento complementare a quello definito dalla funzione di ripartizione calcolata mediante <code>pbinom()</code>, ovvero</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="fl">0.5</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5</span></span></code></pre></div>
</div>
<div id="valore-atteso-e-deviazione-standard" class="section level3 hasAnchor" number="12.2.1">
<h3 class="hasAnchor"><span class="header-section-number">12.2.1</span> Valore atteso e deviazione standard<a href="#valore-atteso-e-deviazione-standard" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La media (numero atteso di successi in <span class="math inline">\(n\)</span> prove) e la deviazione
standard di una distribuzione binomiale sono molto semplici:</p>
<p><span class="math display">\[\begin{align}
\mu    &amp;= n\theta,  \notag \\
\sigma &amp;= \sqrt{n\theta(1-\theta)}.
\end{align}\]</span></p>
<div class="proof">
<p><span id="unlabeled-div-62" class="proof"><em>Proof</em>. </span>Essendo <span class="math inline">\(Y\)</span> la somma di <span class="math inline">\(n\)</span> prove Bernoulliane indipendenti <span class="math inline">\(Y_i\)</span>, è facile vedere che</p>
<p><span class="math display">\[\begin{align}
\E(Y) &amp;= \E \left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \E(Y_i) = n\theta, \\
\Var(Y) &amp;= \Var \left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \Var(Y_i) = n \theta (1-\theta).
\end{align}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-63" class="exercise"><strong>(#exr:unlabeled-div-63) </strong></span>Si trovino il valore atteso e la varianza del lancio di quattro monete con probabilità di successo pari a <span class="math inline">\(\theta = 0.2\)</span>.</p>
<p>Il valore atteso è <span class="math inline">\(\mu = n\theta = 4 \cdot 0.2 = 0.8.\)</span> Ciò significa che, se l’esperimento casuale venisse ripetuto infinite volte, l’esito testa verrebbe osservato un numero medio di volte pari a 0.8. La varianza è <span class="math inline">\(n \theta (1-\theta) = 4 \cdot(1 - 0.2) = 0.8\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
</div>
</div>
</div>
<div id="distribuzione-di-poisson" class="section level2 hasAnchor" number="12.3">
<h2 class="hasAnchor"><span class="header-section-number">12.3</span> Distribuzione di Poisson<a href="#distribuzione-di-poisson" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione di Poisson è una distribuzione di probabilità discreta che esprime le probabilità per il numero di eventi che si verificano successivamente ed indipendentemente in un dato intervallo di tempo, sapendo che mediamente se ne verifica un numero <span class="math inline">\(\lambda\)</span>. La distribuzione di Poisson serve dunque per contare il numero di volte in cui un evento ha luogo in un determinato intervallo di tempo. La stessa distribuzione può essere estesa anche per contare gli eventi che hanno luogo in una determinata porzione di spazio.</p>
<div class="definition">
<p><span id="def:unlabeled-div-64" class="definition"><strong>(#def:unlabeled-div-64) </strong></span>La distribuzione di Poisson può essere intesa come limite della distribuzione binomiale, dove la probabilità di successo <span class="math inline">\(\theta\)</span> è pari a <span class="math inline">\(\frac{\lambda}{n}\)</span> con <span class="math inline">\(n\)</span> che tende a <span class="math inline">\(\infty\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\lim_{y \rightarrow \infty} \binom{n}{y} \theta^y (1-\theta)^{n-y} = \frac{\lambda^y}{y!}e^{-\lambda}.
\end{equation}\]</span></p>
</div>
<p>Alcune distribuzioni di Poisson sono riportate nella figura @ref(fig:examples-poisson-distrib).</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/examples-poisson-distrib-1.png" alt="Alcune distribuzioni di Poisson." width="576" />
<p class="caption">
(#fig:examples-poisson-distrib)Alcune distribuzioni di Poisson.
</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-65" class="exercise"><strong>(#exr:unlabeled-div-65) </strong></span>Supponiamo che un evento accada 300 volte all’ora e si vuole determinare la probabilità che in un minuto accadano esattamente 3 eventi.</p>
<p>Il numero medio di eventi in un minuto è pari a</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">300</span> <span class="sc">/</span> <span class="dv">60</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>lambda</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 5</span></span></code></pre></div>
<p>
Quindi la probabilità che in un minuto si abbiano 3 eventi è pari a</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>(lambda<span class="sc">^</span>y <span class="sc">/</span> <span class="fu">factorial</span>(y)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>lambda)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.1404</span></span></code></pre></div>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-66" class="exercise"><strong>(#exr:unlabeled-div-66) </strong></span>Per i dati dell’esempio precedente, si trovi la probabilità che un evento accada almeno 8 volte in un minuto.</p>
<p>La probabilità cercata è</p>
<p><span class="math display">\[
p(y \geq 8) = 1 - p (y \leq 7) = 1- \sum_{i = 0}^7 \frac{\lambda^7}{7!}e^{-\lambda},
\]</span>

con <span class="math inline">\(\lambda = 5\)</span>.</p>
<p>Svolgendo i calcoli in otteniamo:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">ppois</span>(<span class="at">q =</span> <span class="dv">7</span>, <span class="at">lambda =</span> <span class="dv">5</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.1334</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(<span class="at">q =</span> <span class="dv">7</span>, <span class="at">lambda =</span> <span class="dv">5</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.1334</span></span></code></pre></div>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-67" class="exercise"><strong>(#exr:unlabeled-div-67) </strong></span>Sapendo che un evento avviene in media 6 volte al minuto, si calcoli (a) la probabilità di osservare un numero di eventi uguale o inferiore a 3 in un minuto, e (b) la probabilità di osservare esattamente 2 eventi in 30 secondi.</p>
<ol style="list-style-type: lower-alpha">
<li>In questo caso <span class="math inline">\(\lambda = 6\)</span> e la probabilità richiesta è</li>
</ol>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(<span class="at">q =</span> <span class="dv">3</span>, <span class="at">lambda =</span> <span class="dv">6</span>, <span class="at">lower.tail =</span> <span class="cn">TRUE</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.1512</span></span></code></pre></div>
<ol start="2" style="list-style-type: lower-alpha">
<li>In questo caso <span class="math inline">\(\lambda = 6 / 2\)</span> e la probabilità richiesta è</li>
</ol>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">3</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.224</span></span></code></pre></div>
</div>
<div id="alcune-proprietà-della-variabile-di-poisson" class="section level3 hasAnchor" number="12.3.1">
<h3 class="hasAnchor"><span class="header-section-number">12.3.1</span> Alcune proprietà della variabile di Poisson<a href="#alcune-proprietà-della-variabile-di-poisson" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Il valore atteso, la moda e la varianza della variabile di Poisson sono uguali a <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>La somma <span class="math inline">\(Y_1 + \dots + Y_n\)</span> di <span class="math inline">\(n\)</span> variabili casuali indipendenti con distribuzioni di Poisson di parametri <span class="math inline">\(\lambda_{1},\dots,\lambda_{n}\)</span> segue una distribuzione di Poisson di parametro <span class="math inline">\(\lambda = \lambda_{1}+\dots+\lambda_{n}\)</span>.</p></li>
<li><p>La differenze di due variabili di Poisson non è una variabile di Poisson. Basti infatti pensare che può assumere valori negativi.</p></li>
</ul>
</div>
</div>
<div id="distribuzione-discreta-uniforme" class="section level2 hasAnchor" number="12.4">
<h2 class="hasAnchor"><span class="header-section-number">12.4</span> Distribuzione discreta uniforme<a href="#distribuzione-discreta-uniforme" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una distribuzione discreta uniforme è una distribuzione di probabilità discreta che è uniforme su un insieme, ovvero che attribuisce ad ogni elemento dell’insieme discreto e finito <span class="math inline">\(S\)</span> su cui è definita la stessa probabilità <span class="math inline">\(p\)</span> di verificarsi.</p>
<p>Consideriamo la variabile casuale <span class="math inline">\(X\)</span> con supporto <span class="math inline">\(1, 2, \dots, m\)</span>. Un esperimento casuale in cui si verifica questa distribuzione è la scelta casuale di un intero compreso tra 1 e <span class="math inline">\(m\)</span> inclusi. Sia <span class="math inline">\(X\)</span> il numero scelto. Allora</p>
<p><span class="math display">\[
P(X = x) = \frac{1}{m}, \quad x = 1, \dots, m.
\]</span></p>
<p>Il valore atteso è</p>
<p><span class="math display">\[
\E(X) = \sum_{x=1}^m x f_X(x) = \sum_{x=1}^m x \frac{1}{m} = \frac{1}{m} (1 + 2 + \dots + m) = \frac{m+1}{2},
\]</span>
dove abbiamo utilizzato l’identità <span class="math inline">\(1+2+···+m = m(m+1)/2\)</span>.</p>
<p>Per trovare la varianza, prima calcoliamo</p>
<p><span class="math display">\[
\E(X^2) = \frac{1}{m} \sum_{x=1}^m x^2,
\]</span>
e poi troviamo</p>
<p><span class="math display">\[
\V(X) = E(X^2) - \left[E(X)\right]^2.
\]</span></p>
<div id="usiamo-textsfr" class="section level3 hasAnchor" number="12.4.1">
<h3 class="hasAnchor"><span class="header-section-number">12.4.1</span> Usiamo <span class="math inline">\(\textsf{R}\)</span><a href="#usiamo-textsfr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La sintassi generale per simulare una variabile casuale uniforme discreta è <code>sample(x, size, replace = TRUE)</code>. L’argomento <code>x</code> identifica i numeri da cui campionare casualmente. Se <code>x</code> è un numero, il campionamento viene eseguito da 1 a <code>x</code>. L’argomento <code>size</code> indica quanto dovrebbe essere grande la dimensione del campione e <code>replace</code> indica se i numeri devono essere reintrodotti o meno nell’urna dopo essere stati estratti. L’opzione di default è <code>replace = FALSE</code> ma per le uniformi discrete i valori estratti devono essere sostituiti. Seguono alcuni esempi.</p>
<ul>
<li>Per lanciare un dado equilibrato 3000 volte: <code>sample(6, size = 3000, replace = TRUE)</code>;</li>
<li>per scegliere 27 numeri casuali da 30 a 70: <code>sample(30:70, size = 27, replace = TRUE)</code>;</li>
<li>per lanciare una moneta equa 1000 volte: <code>sample(c("H","T"), size = 1000, replace = TRUE)</code>.</li>
</ul>
</div>
</div>
<div id="distribuzione-beta-binomiale" class="section level2 hasAnchor" number="12.5">
<h2 class="hasAnchor"><span class="header-section-number">12.5</span> Distribuzione beta-binomiale<a href="#distribuzione-beta-binomiale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione beta-binomiale di parametri <span class="math inline">\(N\)</span>, <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> è una distribuzione discreta con una funzione di massa di probabilità uguale a</p>
<p><span class="math display">\[\begin{equation}
\mbox{BetaBinomial}(y \mid N, \alpha, \beta) = \binom{N}{y} \frac{B(y + \alpha, N-y+\beta)}{B(\alpha, \beta)},
\end{equation}\]</span></p>
<p>dove la funzione beta è <span class="math inline">\(B(u, v) = \frac{\Gamma(u)\Gamma(v)}{\Gamma(u+v)}\)</span>.</p>
<p>Senza entrare nei dettagli, ci accontentiamo di sapere che tale distribuzione è implementata nella funzione <code>dbbinom()</code> del pacchetto <code>extraDistr</code>.</p>
</div>
<div id="commenti-e-considerazioni-finali-8" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione binomiale è una distribuzione di probabilità discreta che descrive il numero di successi in un processo di Bernoulli, ovvero la variabile aleatoria <span class="math inline">\(Y = Y_1 + \dots + Y_n\)</span> che somma <span class="math inline">\(n\)</span> variabili casuali indipendenti di uguale distribuzione di Bernoulli <span class="math inline">\(\mathcal{B}(\theta)\)</span>, ognuna delle quali può fornire due soli risultati: il successo con probabilità <span class="math inline">\(\theta\)</span> e il fallimento con probabilità <span class="math inline">\(1 - \theta\)</span>.</p>
<p>La distribuzione binomiale è molto importante per le sue molte applicazioni. Nelle presenti dispense, dedicate all’analisi bayesiana, è soprattutto importante perché costituisce il fondamento del caso più semplice del cosiddetto “aggiornamento bayesiano”, ovvero il caso Beta-Binomiale. Il modello Beta-Binomiale ci fornirà infatti un esempio paradigmatico dell’approccio bayesiano all’inferenza e sarà trattato in maniera analitica. È dunque importante che le proprietà della distribuzione binomiale risultino ben chiare.</p>
<!--chapter:end:022_discr_rv_distr.Rmd-->
</div>
</div>
<div id="distr-rv-cont" class="section level1 hasAnchor" number="13">
<h1 class="hasAnchor"><span class="header-section-number">13</span> Distribuzioni di v.c. continue<a href="#distr-rv-cont" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Dopo avere introdotto con una simulazione il concetto di funzione di densità nel Capitolo @ref(chapter-intro-density-function), prendiamo ora in esame alcune delle densità di probabilità più note. La più importante di esse è sicuramente la distribuzione Normale.</p>
<div id="distribuzione-normale" class="section level2 hasAnchor" number="13.1">
<h2 class="hasAnchor"><span class="header-section-number">13.1</span> Distribuzione Normale<a href="#distribuzione-normale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Non c’è un’unica distribuzione Normale, ma ce ne sono molte. Tali distribuzioni sono anche dette “gaussiane” in onore di Carl Friedrich Gauss (uno dei più grandi matematici della storia il quale, tra le altre cose, scoprì l’utilità di tale funzione di densità per descrivere gli errori di misurazione). Adolphe Quetelet, il padre delle scienze sociali quantitative, fu il primo ad applicare tale densità alle misurazioni dell’uomo. Karl Pearson usò per primo il termine “distribuzione Normale” anche se ammise che questa espressione “ha lo svantaggio di indurre le persone a credere che le altre distribuzioni, in un senso o nell’altro, non siano normali.”</p>
<div id="limite-delle-distribuzioni-binomiali" class="section level3 hasAnchor" number="13.1.1">
<h3 class="hasAnchor"><span class="header-section-number">13.1.1</span> Limite delle distribuzioni binomiali<a href="#limite-delle-distribuzioni-binomiali" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Iniziamo con un un breve excursus storico. Nel 1733, Abraham de Moivre notò che, aumentando il numero di prove in una distribuzione binomiale, la distribuzione risultante diventava quasi simmetrica e a forma campanulare. Con 10 prove e una probabilità di successo di 0.9 in ciascuna prova, la distribuzione è chiaramente asimmetrica.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(x, N, <span class="fl">0.9</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>binomial_limit_plot <span class="ot">&lt;-</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="fl">0.2</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;y&quot;</span>) <span class="sc">+</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Binomial(y | 10, 0.9)&quot;</span>)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>binomial_limit_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-50-1.png" alt="Probabilità del numero di successi in $N = 10$ prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione $\Bin(y \mid 10, 0.9)$. Con solo dieci prove, la distribuzione è fortemente asimmetrica negativa." width="576" />
<p class="caption">
(#fig:unnamed-chunk-50)Probabilità del numero di successi in <span class="math inline">\(N = 10\)</span> prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione <span class="math inline">\(\Bin(y \mid 10, 0.9)\)</span>. Con solo dieci prove, la distribuzione è fortemente asimmetrica negativa.
</p>
</div>
<p>Ma se aumentiamo il numero di prove di un fattore di 100 a
<em>N</em> = 1000, senza modificare la probabilità di successo di 0.9, la distribuzione assume una forma campanulare quasi simmetrica. Dunque, de Moivre scoprì che, quando <em>N</em> è grande, la funzione Normale (che introdurremo qui sotto), nonostante sia la densità di v.a. continue, fornisce una buona approssimazione alla funzione di massa di probabilità binomiale.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(x, N, <span class="fl">0.9</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>binomial_limit_plot <span class="ot">&lt;-</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;y&quot;</span>) <span class="sc">+</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Binomial(y | 1000, 0.9)&quot;</span>) <span class="sc">+</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">850</span>, <span class="dv">950</span>)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>binomial_limit_plot</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/unnamed-chunk-51-1.png" alt="Probabilità del numero di successi in $N = 1000$ prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione $\Bin(y \mid 1000, 0.9)$. Con mille prove, la distribuzione è quasi simmetrica a forma campanulare." width="576" />
<p class="caption">
(#fig:unnamed-chunk-51)Probabilità del numero di successi in <span class="math inline">\(N = 1000\)</span> prove bernoulliane indipendenti, ciascuna con una probabilità di successo di 0.90. Il risultato è una distribuzione <span class="math inline">\(\Bin(y \mid 1000, 0.9)\)</span>. Con mille prove, la distribuzione è quasi simmetrica a forma campanulare.
</p>
</div>
<p>La distribuzione Normale fu scoperta da Gauss nel 1809 e, storicamente, è intimamente legata al metodo dei minimi quadrati – si veda l’Appendice @ref(gauss-normale). Il Paragrafo successivo illustra come si possa giungere alla Normale mediante una simulazione.</p>
</div>
</div>
<div id="normal-random-walk" class="section level2 hasAnchor" number="13.2">
<h2 class="hasAnchor"><span class="header-section-number">13.2</span> La Normale prodotta con una simulazione<a href="#normal-random-walk" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="citation">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span> presenta un esempio che illustra come sia possibile giungere alla distribuzione Normale mediante una simulazione. Supponiamo che vi siano mille persone tutte allineate su una linea di partenza. Quando viene dato un segnale, ciascuna persona lancia una moneta e fa un passo in avanti oppure all’indietro a seconda che sia uscita testa o croce. Supponiamo che la lunghezza di ciascun passo vari da 0 a 1 metro. Ciascuna persona lancia una moneta 16 volte e dunque compie 16 passi.</p>
<p>Alla conclusione di queste passeggiate casuali (<em>random walk</em>) non possiamo sapere con esattezza dove si troverà ciascuna persona, ma possiamo conoscere con certezza le caratteristiche della distribuzione delle mille distanze dall’origine. Per esempio, possiamo predire in maniera accurata la proporzione di persone che si sono spostate in avanti oppure all’indietro. Oppure, possiamo predire accuratamente la proporzione di persone che si troveranno ad una certa distanza dalla linea di partenza (es., a 1.5 m dall’origine).</p>
<p>Queste predizioni sono possibili perché tali distanze si distribuiscono secondo la legge Normale. È facile simulare questo processo usando . I risultati della simulazione sono riportati nella figura @ref(fig:rw-normal-4816).</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>pos <span class="ot">&lt;-</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">replicate</span>(<span class="dv">100</span>, <span class="fu">runif</span>(<span class="dv">16</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbind</span>(<span class="dv">0</span>, .) <span class="sc">%&gt;%</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">step =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(key, value, <span class="sc">-</span>step) <span class="sc">%&gt;%</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">person =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">each =</span> <span class="dv">17</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(person) <span class="sc">%&gt;%</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">position =</span> <span class="fu">cumsum</span>(value)) <span class="sc">%&gt;%</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> pos,</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> step, <span class="at">y =</span> position, <span class="at">group =</span> person)</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>), <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color =</span> person <span class="sc">&lt;</span> <span class="dv">2</span>, <span class="at">alpha =</span> person <span class="sc">&lt;</span> <span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;gray&quot;</span>, <span class="st">&quot;black&quot;</span>)) <span class="sc">+</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_alpha_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="dv">5</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Numero di passi&quot;</span>,</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">12</span>, <span class="dv">16</span>)</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Posizione&quot;</span>) <span class="sc">+</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/rw-normal-4816-1.png" alt="Passeggiata casuale di 4, 8 e 16 passi. La spezzata nera indica la media delle distanze dall'origine come funzione del numero di passi." width="576" />
<p class="caption">
(#fig:rw-normal-4816)Passeggiata casuale di 4, 8 e 16 passi. La spezzata nera indica la media delle distanze dall’origine come funzione del numero di passi.
</p>
</div>
<p>Un kernel density plot delle distanze ottenute dopo 4, 8 e 16 passi è riportato nella figura @ref(fig:rw-normal-3panels). Nel pannello di destra, al kernel density plot è stata sovrapposta una densità Normale di opportuni parametri (linea tratteggiata).</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">stat =</span> <span class="st">&quot;density&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;4 passi&quot;</span>)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">8</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">outline.type =</span> <span class="st">&quot;full&quot;</span>) <span class="sc">+</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;8 passi&quot;</span>)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>sd <span class="ot">&lt;-</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">sd =</span> <span class="fu">sd</span>(position)) <span class="sc">%&gt;%</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(sd)</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>  pos <span class="sc">%&gt;%</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(step <span class="sc">==</span> <span class="dv">16</span>) <span class="sc">%&gt;%</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> position)) <span class="sc">+</span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm,</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sd),</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="dv">2</span></span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">alpha =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;16 passi&quot;</span>,</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità&quot;</span></span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>(p1 <span class="sc">|</span> p2 <span class="sc">|</span> p3) <span class="sc">&amp;</span> <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">6</span>, <span class="dv">6</span>))</span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/rw-normal-3panels-1.png" alt="Kernel density plot dei risultati della passeggiata casuale riportata nella figura precente, dopo 4, 8 e 16 passi. Nel pannello di destra, una densità Normale di opportuni parametri è sovrapposta all'istogramma lisciato." width="576" />
<p class="caption">
(#fig:rw-normal-3panels)Kernel density plot dei risultati della passeggiata casuale riportata nella figura precente, dopo 4, 8 e 16 passi. Nel pannello di destra, una densità Normale di opportuni parametri è sovrapposta all’istogramma lisciato.
</p>
</div>
<p>Questa simulazione mostra che qualunque processo nel quale viene sommato un certo numero di valori casuali, tutti provenienti dalla medesima distribuzione, converge ad una distribuzione Normale. Non importa quale sia la forma della distribuzione di partenza: essa può essere uniforme, come nell’esempio presente, o di qualunque altro tipo. La forma della distribuzione da cui viene realizzato il campionamento determina la velocità della convergenza alla Normale. In alcuni casi la convergenza è lenta; in altri casi la convergenza è molto rapida (come nell’esempio presente).</p>
<p>Da un punto di vista formale, diciamo che una variabile casuale continua <span class="math inline">\(Y\)</span> ha una distribuzione Normale se la sua densità è</p>
<p><span class="math display">\[\begin{equation}
f(y; \mu, \sigma) = {1 \over {\sigma\sqrt{2\pi} }} \exp \left\{-\frac{(y -  \mu)^2}{2 \sigma^2} \right\},
(\#eq:normal-formula)
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(\mu \in \mathbb{R}\)</span> e <span class="math inline">\(\sigma &gt; 0\)</span> sono i parametri della distribuzione.</p>
<p>La densità normale è unimodale e simmetrica con una caratteristica forma a campana e con il punto di massima densità in corrispondenza di <span class="math inline">\(\mu\)</span>.</p>
<p>Il significato dei parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> che appaiono nella @ref(eq:normal-formula) viene chiarito dalla dimostrazione che</p>
<p><span class="math display">\[\begin{equation}
\E(X) = \mu, \qquad \Var(X) = \sigma^2.
\end{equation}\]</span></p>
<p>La rappresentazione grafica di quattro densità Normali tutte con media 0 e con deviazioni standard 0.25, 0.5, 1 e 2 è fornita nella figura @ref(fig:gaussian-plot-demo).</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/gaussian-plot-demo-1.png" alt="Alcune distribuzioni Normali." width="576" />
<p class="caption">
(#fig:gaussian-plot-demo)Alcune distribuzioni Normali.
</p>
</div>
<div id="concentrazione" class="section level3 hasAnchor" number="13.2.1">
<h3 class="hasAnchor"><span class="header-section-number">13.2.1</span> Concentrazione<a href="#concentrazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>È istruttivo osservare il grado di concentrazione della distribuzione
Normale attorno alla media:</p>
<p><span class="math display">\[\begin{align}
P(\mu - \sigma &lt; X &lt; \mu + \sigma) &amp;= P (-1 &lt; Z &lt; 1) \simeq 0.683, \notag\\
P(\mu - 2\sigma &lt; X &lt; \mu + 2\sigma) &amp;= P (-2 &lt; Z &lt; 2) \simeq 0.956, \notag\\
P(\mu - 3\sigma &lt; X &lt; \mu + 3\sigma) &amp;= P (-3 &lt; Z &lt; 3) \simeq 0.997. \notag
\end{align}\]</span></p>
<p>
Si noti come un dato la cui distanza dalla media è superiore a 3 volte la deviazione standard presenti un carattere di eccezionalità perché meno del 0.3% dei dati della distribuzione Normale presentano questa caratteristica.</p>
<p>Per indicare la distribuzione Normale si usa la notazione
<span class="math inline">\(\mathcal{N}(\mu, \sigma)\)</span>.</p>
</div>
<div id="funzione-di-ripartizione-1" class="section level3 hasAnchor" number="13.2.2">
<h3 class="hasAnchor"><span class="header-section-number">13.2.2</span> Funzione di ripartizione<a href="#funzione-di-ripartizione-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il valore della funzione di ripartizione di <span class="math inline">\(Y\)</span> nel punto <span class="math inline">\(y\)</span> è l’area sottesa alla curva di densità <span class="math inline">\(f(y)\)</span> nella semiretta <span class="math inline">\((-\infty, y]\)</span>. Non esiste alcuna funzione elementare per la funzione di ripartizione</p>
<p><span class="math display">\[\begin{equation}
F(y) = \int_{-\infty}^y {1 \over {\sigma\sqrt{2\pi} }} \exp \left\{-\frac{(y - \mu)^2}{2\sigma^2} \right\} dy,
\end{equation}\]</span></p>
<p>pertanto le probabilità <span class="math inline">\(P(Y &lt; y)\)</span> vengono calcolate mediante integrazione numerica approssimata. I valori della funzione di ripartizione di una variabile casuale Normale sono dunque forniti da un software.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-68" class="exercise"><strong>(#exr:unlabeled-div-68) </strong></span>Usiamo per calcolare la funzione di ripartizione della Normale. La funzione <code>pnorm(q, mean, sd)</code> restituisce la funzione di ripartizione della Normale con media <code>mean</code> e deviazione standard <code>sd</code>, ovvero l’area sottesa alla funzione di densità di una Normale con media <code>mean</code> e deviazione standard <code>sd</code> nell’intervallo <span class="math inline">\([-\infty, q]\)</span>.</p>
<p>Per esempio, in precedenza abbiamo detto che il 68% circa dell’area sottesa ad una Normale è compresa nell’intervallo <span class="math inline">\(\mu \pm \sigma\)</span>. Verifichiamo per la distribuzione del QI <span class="math inline">\(\sim \mathcal{N}(\mu = 100, \sigma = 15)\)</span>:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">100</span><span class="sc">+</span><span class="dv">15</span>, <span class="dv">100</span>, <span class="dv">15</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">100-15</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6827</span></span></code></pre></div>
<p>Il 95% dell’area è compresa nell’intervallo <span class="math inline">\(\mu \pm 1.96 \cdot\sigma\)</span>:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">100</span> <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> <span class="dv">15</span>, <span class="dv">100</span>, <span class="dv">15</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">100</span> <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> <span class="dv">15</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.95</span></span></code></pre></div>
<p>Quasi tutta la distribuzione è compresa nell’intervallo <span class="math inline">\(\mu \pm 3 \cdot\sigma\)</span>:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">100</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> <span class="dv">15</span>, <span class="dv">100</span>, <span class="dv">15</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">100</span> <span class="sc">-</span> <span class="dv">3</span> <span class="sc">*</span> <span class="dv">15</span>, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.9973</span></span></code></pre></div>
</div>
</div>
<div id="distribuzione-normale-standard" class="section level3 hasAnchor" number="13.2.3">
<h3 class="hasAnchor"><span class="header-section-number">13.2.3</span> Distribuzione Normale standard<a href="#distribuzione-normale-standard" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La distribuzione Normale di parametri <span class="math inline">\(\mu = 0\)</span> e <span class="math inline">\(\sigma = 1\)</span> viene detta <em>distribuzione Normale standard</em>. La famiglia Normale è l’insieme avente come elementi tutte le distribuzioni Normali con parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> diversi. Tutte le distribuzioni Normali si ottengono dalla Normale standard mediante una trasformazione lineare: se <span class="math inline">\(Y \sim \mathcal{N}(\mu_Y, \sigma_Y)\)</span> allora</p>
<p><span class="math display">\[\begin{equation}
X = a + b Y \sim \mathcal{N}(\mu_X = a+b \mu_Y, \sigma_X = \left|b\right|\sigma_Y).
\end{equation}\]</span></p>
<p>L’area sottesa alla curva di densità di <span class="math inline">\(\mathcal{N}(\mu, \sigma)\)</span> nella semiretta <span class="math inline">\((-\infty, y]\)</span> è uguale all’area sottesa alla densità Normale standard nella semiretta <span class="math inline">\((-\infty, z]\)</span>, in cui <span class="math inline">\(z = (y -\mu_Y )/\sigma_Y\)</span> è il punteggio standard di <span class="math inline">\(Y\)</span>. Per la simmetria della distribuzione, l’area sottesa nella semiretta <span class="math inline">\([1, \infty)\)</span> è uguale all’area sottesa nella semiretta <span class="math inline">\((-\infty, 1]\)</span> e quest’ultima coincide con <span class="math inline">\(F(-1)\)</span>. Analogamente, l’area sottesa nell’intervallo <span class="math inline">\([y_a, y_b]\)</span>, con <span class="math inline">\(y_a &lt; y_b\)</span>, è pari a <span class="math inline">\(F(z_b) - F(z_a)\)</span>, dove <span class="math inline">\(z_a\)</span> e <span class="math inline">\(z_b\)</span> sono i punteggi standard di <span class="math inline">\(y_a\)</span> e <span class="math inline">\(y_b\)</span>.</p>
<p>Si ha anche il problema inverso rispetto a quello del calcolo delle aree: dato un numero <span class="math inline">\(0 \leq p \leq 1\)</span>, il problema è quello di determinare un numero <span class="math inline">\(z \in \mathbb{R}\)</span> tale che <span class="math inline">\(P(Z &lt; z) = p\)</span>. Il valore <span class="math inline">\(z\)</span> cercato è detto <em>quantile</em> di ordine <span class="math inline">\(p\)</span> della Normale standard e può essere trovato mediante un software.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-69" class="exercise"><strong>(#exr:unlabeled-div-69) </strong></span>Supponiamo che l’altezza degli individui adulti segua la distribuzione Normale di media <span class="math inline">\(\mu = 1.7\)</span> m e deviazione standard <span class="math inline">\(\sigma = 0.1\)</span> m. Vogliamo sapere la proporzione di individui adulti con un’altezza compresa tra <span class="math inline">\(1.7\)</span> e <span class="math inline">\(1.8\)</span> m.</p>
<p>Il problema ci chiede di trovare l’area sottesa alla distribuzione <span class="math inline">\(\mathcal{N}(\mu = 1.7, \sigma = 0.1)\)</span> nell’intervallo <span class="math inline">\([1.7, 1.8]\)</span>:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="fl">1.4</span>, <span class="fl">2.0</span>, <span class="at">length.out =</span> <span class="dv">100</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="fl">1.7</span>, <span class="at">sd =</span> <span class="fl">0.1</span>))</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">fill =</span> <span class="st">&quot;sky blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gghighlight</span>(x <span class="sc">&lt;</span> <span class="fl">1.8</span> <span class="sc">&amp;</span> x <span class="sc">&gt;</span> <span class="fl">1.7</span>) <span class="sc">+</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Altezza&quot;</span>,</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità&quot;</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-55-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>
La risposta si trova utilizzando la funzione di
ripartizione <span class="math inline">\(F(X)\)</span> della legge <span class="math inline">\(\mathcal{N}(1.7, 0.1)\)</span> in
corrispondenza dei due valori forniti dal problema:
<span class="math inline">\(F(X = 1.8) - F(X = 1.7)\)</span>. Utilizzando la seguente istruzione</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.8</span>, <span class="fl">1.7</span>, <span class="fl">0.1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fl">1.7</span>, <span class="fl">1.7</span>, <span class="fl">0.1</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.3413</span></span></code></pre></div>
<p>otteniamo il <span class="math inline">\(31.43\%\)</span>.</p>
<p>In maniera equivalente, possiamo standardizzare i valori che delimitano l’intervallo considerato e utilizzare la funzione di ripartizione della normale standardizzata. I limiti inferiore e superiore dell’intervallo sono</p>
<p><span class="math display">\[
z_{\text{inf}} = \frac{1.7 - 1.7}{0.1} = 0, \quad z_{\text{sup}} = \frac{1.8 - 1.7}{0.1} = 1.0,
\]</span></p>
<p>quindi otteniamo</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.0</span>, <span class="dv">0</span>, <span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.3413</span></span></code></pre></div>
<p>Il modo più semplice per risolvere questo problema resta comunque quello di rendersi conto che la probabilità richiesta non è altro che la metà dell’area sottesa dalle distribuzioni Normali nell’intervallo <span class="math inline">\([\mu - \sigma, \mu + \sigma]\)</span>, ovvero <span class="math inline">\(0.683/2\)</span>.</p>
</div>
<div id="funzione-di-ripartizione-della-normale-standard-e-funzione-logistica" class="section level4 hasAnchor" number="13.2.3.1">
<h4 class="hasAnchor"><span class="header-section-number">13.2.3.1</span> Funzione di ripartizione della normale standard e funzione logistica<a href="#funzione-di-ripartizione-della-normale-standard-e-funzione-logistica" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Si noti che la funzione logistica (in blu), pur essendo del tutto diversa dalla Normale dal punto di vista formale, assomiglia molto alla Normale standard quando le due cdf hanno la stessa varianza.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> pnorm) <span class="sc">+</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> plogis,</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">scale =</span> <span class="fl">0.56</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-58-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="teorema-del-limite-centrale" class="section level2 hasAnchor" number="13.3">
<h2 class="hasAnchor"><span class="header-section-number">13.3</span> Teorema del limite centrale<a href="#teorema-del-limite-centrale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Laplace dimostrò il teorema del limite centrale (TLC) nel 1812. Il TLC ci dice che se prendiamo una sequenza di variabili casuali indipendenti e le sommiamo, tale somma tende a distribuirisi come una Normale. Il TLC specifica inoltre, sulla base dei valori attesi e delle varianze delle v.c. che vengono sommate, quali saranno i parametri della distribuzione Normale così ottenuta.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-70" class="theorem"><strong>(#thm:unlabeled-div-70) </strong></span>Si supponga che <span class="math inline">\(Y = Y_1, Y_2, \ldots, Y_N\)</span> sia una sequenza di v.a. i.i.d. con <span class="math inline">\(\E(Y_n) = \mu\)</span> e <span class="math inline">\(\SD(Y_n) = \sigma\)</span>. Si definisca una nuova v.c. come la media di <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
Z = \frac{1}{N} \sum_{n=1}^N Y_n.
\]</span></p>
<p>Con <span class="math inline">\(N \rightarrow \infty\)</span>, <span class="math inline">\(Z\)</span> tenderà ad una Normale con lo stesso valore atteso di <span class="math inline">\(Y_n\)</span> e una deviazione standard che sarà più piccola della deviazione standard originaria di un fattore pari a <span class="math inline">\(\sqrt{\frac{1}{\sqrt{N}}}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
p_Z(z) \rightarrow \mathcal{N}\left(z \ \Bigg| \ \mu, \, \frac{1}{\sqrt{N}} \cdot \sigma \right).
\end{equation}\]</span></p>
</div>
<p>Il TLC può essere generalizzato a variabili che non hanno la stessa distribuzione purché siano indipendenti e abbiano aspettative e varianze finite.</p>
<p>Molti fenomeni naturali, come l’altezza dell’uomo adulto di entrambi i sessi, sono il risultato di una serie di effetti additivi relativamente piccoli, la cui combinazione porta alla normalità, indipendentemente da come gli effetti additivi sono distribuiti. In pratica, questo è il motivo per cui la distribuzione normale ha senso come rappresentazione di molti fenomeni naturali.</p>
</div>
<div id="distribuzione-chi-quadrato" class="section level2 hasAnchor" number="13.4">
<h2 class="hasAnchor"><span class="header-section-number">13.4</span> Distribuzione Chi-quadrato<a href="#distribuzione-chi-quadrato" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dalla Normale deriva la distribuzione <span class="math inline">\(\chi^2\)</span>. La distribuzione <span class="math inline">\(\chi^2_{~k}\)</span> con <span class="math inline">\(k\)</span> gradi di libertà descrive la variabile casuale</p>
<p><span class="math display">\[
Z_1^2 + Z_2^2 + \dots + Z_k^2,
\]</span></p>
<p>dove <span class="math inline">\(Z_1, Z_2, \dots, Z_k\)</span> sono variabili casuali i.i.d. con distribuzione Normale standard <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. La variabile casuale chi-quadrato dipende dal parametro intero positivo <span class="math inline">\(\nu = k\)</span> che ne identifica il numero di gradi di libertà. La densità di probabilità di <span class="math inline">\(\chi^2_{~\nu}\)</span> è</p>
<p><span class="math display">\[
f(x) = C_{\nu} x^{\nu/2-1} \exp (-x/2), \qquad \text{se } x &gt; 0,
\]</span></p>
<p>dove <span class="math inline">\(C_{\nu}\)</span> è una costante positiva.</p>
<p>La figura @ref(fig:alcune-chi-quadrato) mostra alcune distribuzioni Chi-quadrato variando il parametro <span class="math inline">\(\nu\)</span>.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/alcune-chi-quadrato-1.png" alt="Alcune distribuzioni Chi-quadrato." width="576" />
<p class="caption">
(#fig:alcune-chi-quadrato)Alcune distribuzioni Chi-quadrato.
</p>
</div>
<div id="proprietà-2" class="section level3 hasAnchor" number="13.4.1">
<h3 class="hasAnchor"><span class="header-section-number">13.4.1</span> Proprietà<a href="#proprietà-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>La distribuzione di densità <span class="math inline">\(\chi^2_{~\nu}\)</span> è asimmetrica.</p></li>
<li><p>Il valore atteso di una variabile <span class="math inline">\(\chi^2_{~\nu}\)</span> è uguale a <span class="math inline">\(\nu\)</span>.</p></li>
<li><p>La varianza di una variabile <span class="math inline">\(\chi^2_{~\nu}\)</span> è uguale a <span class="math inline">\(2\nu\)</span>.</p></li>
<li><p>Per <span class="math inline">\(k \rightarrow \infty\)</span>, la <span class="math inline">\(\chi^2_{~\nu} \rightarrow \mathcal{N}\)</span>.</p></li>
<li><p>Se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sono due variabili casuali chi-quadrato indipendenti con <span class="math inline">\(\nu_1\)</span> e <span class="math inline">\(\nu_2\)</span> gradi di libertà, ne segue che <span class="math inline">\(X + Y \sim \chi^2_m\)</span>, con <span class="math inline">\(m = \nu_1 + \nu_2\)</span>. Tale principio si estende a qualunque numero finito di variabili casuali chi-quadrato indipendenti.</p></li>
</ul>
<div class="exercise">
<p><span id="exr:unlabeled-div-71" class="exercise"><strong>(#exr:unlabeled-div-71) </strong></span>Usiamo <span class="math inline">\(\R\)</span> per disegnare la densità chi-quadrato con 3 gradi di libertà dividendo l’area sottesa alla curva di densità in due parti uguali.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">15.0</span>, <span class="at">length.out =</span> <span class="dv">100</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">dchisq</span>(x, <span class="dv">3</span>))</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">fill =</span> <span class="st">&quot;sky blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gghighlight</span>(x <span class="sc">&lt;</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;V.a. chi-quadrato con 3 gradi di libertà&quot;</span>,</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità&quot;</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-59-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="distribuzione-t-di-student" class="section level2 hasAnchor" number="13.5">
<h2 class="hasAnchor"><span class="header-section-number">13.5</span> Distribuzione <span class="math inline">\(t\)</span> di Student<a href="#distribuzione-t-di-student" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dalle distribuzioni Normale e Chi quadrato deriva un’altra distribuzione molto nota, la <span class="math inline">\(t\)</span> di Student. Se <span class="math inline">\(Z \sim \mathcal{N}\)</span> e <span class="math inline">\(W \sim \chi^2_{~\nu}\)</span> sono due variabili casuali indipendenti, allora il rapporto</p>
<p><span class="math display">\[\begin{equation}
T = \frac{Z}{\Big( \frac{W}{\nu}\Big)^{\frac{1}{2}}}
\end{equation}\]</span></p>
<p>definisce la distribuzione <span class="math inline">\(t\)</span> di Student con <span class="math inline">\(\nu\)</span> gradi di libertà. Si usa scrivere <span class="math inline">\(T \sim t_{\nu}\)</span>. L’andamento della distribuzione <span class="math inline">\(t\)</span> di Student è simile a quello della distribuzione Normale, ma ha una maggiore dispersione (ha le code più pesanti di una Normale, ovvero ha una varianza maggiore di 1).</p>
<p>La figura @ref(fig:alcune-t-student) mostra alcune distribuzioni <span class="math inline">\(t\)</span> di Student variando il parametro <span class="math inline">\(\nu\)</span>.</p>
<div class="figure" style="text-align: center">
<img src="ds4psy_files/figure-html/alcune-t-student-1.png" alt="Alcune distribuzioni $t$ di Student." width="576" />
<p class="caption">
(#fig:alcune-t-student)Alcune distribuzioni <span class="math inline">\(t\)</span> di Student.
</p>
</div>
<div id="proprietà-3" class="section level3 hasAnchor" number="13.5.1">
<h3 class="hasAnchor"><span class="header-section-number">13.5.1</span> Proprietà<a href="#proprietà-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La variabile casuale <span class="math inline">\(t\)</span> di Student soddisfa le seguenti proprietà:</p>
<ol style="list-style-type: decimal">
<li><p>Per <span class="math inline">\(\nu \rightarrow \infty\)</span>, <span class="math inline">\(t_{\nu}\)</span> tende alla normale standard
<span class="math inline">\(\mathcal{N}(0, 1)\)</span>.</p></li>
<li><p>La densità della <span class="math inline">\(t_{\nu}\)</span> è una funzione simmetrica con valore
atteso nullo.</p></li>
<li><p>Per <span class="math inline">\(\nu &gt; 2\)</span>, la varianza della <span class="math inline">\(t_{\nu}\)</span> vale <span class="math inline">\(\nu/(\nu - 2)\)</span>;
pertanto è sempre maggiore di 1 e tende a 1 per
<span class="math inline">\(\nu \rightarrow \infty\)</span>.</p></li>
</ol>
</div>
</div>
<div id="funzione-beta" class="section level2 hasAnchor" number="13.6">
<h2 class="hasAnchor"><span class="header-section-number">13.6</span> Funzione beta<a href="#funzione-beta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La funzione beta di Eulero è una funzione matematica, <em>non</em> una densità di probabilità. La menzioniamo qui perché viene utilizzata nella distribuzione Beta. La funzione beta si può scrivere in molti modi diversi; per i nostri scopi la scriveremo così:</p>
<p><span class="math display">\[\begin{equation}
B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\,,
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(\Gamma(x)\)</span> è la funzione Gamma, ovvero il fattoriale discendente, cioè</p>
<p><span class="math display">\[\begin{equation}
(x-1)(x-2)\ldots (x-n+1)\notag\,.
\end{equation}\]</span></p>
<p>Per esempio, posti <span class="math inline">\(\alpha = 3\)</span> e <span class="math inline">\(\beta = 9\)</span>, la funzione beta assume il valore</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="fu">beta</span>(alpha, beta)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.00202</span></span></code></pre></div>
<p>Per chiarire, lo stesso risultato si ottiene con</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>((<span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">8</span> <span class="sc">*</span> <span class="dv">7</span> <span class="sc">*</span> <span class="dv">6</span> <span class="sc">*</span> <span class="dv">5</span> <span class="sc">*</span> <span class="dv">4</span> <span class="sc">*</span> <span class="dv">3</span> <span class="sc">*</span> <span class="dv">2</span>)) <span class="sc">/</span> </span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">11</span> <span class="sc">*</span> <span class="dv">10</span> <span class="sc">*</span> <span class="dv">9</span> <span class="sc">*</span> <span class="dv">8</span> <span class="sc">*</span> <span class="dv">7</span> <span class="sc">*</span> <span class="dv">6</span> <span class="sc">*</span> <span class="dv">5</span> <span class="sc">*</span> <span class="dv">4</span> <span class="sc">*</span> <span class="dv">3</span> <span class="sc">*</span> <span class="dv">2</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.00202</span></span></code></pre></div>
<p>ovvero</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gamma</span>(alpha) <span class="sc">*</span> <span class="fu">gamma</span>(beta) <span class="sc">/</span> <span class="fu">gamma</span>(alpha <span class="sc">+</span> beta)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.00202</span></span></code></pre></div>
</div>
<div id="distribuzione-beta" class="section level2 hasAnchor" number="13.7">
<h2 class="hasAnchor"><span class="header-section-number">13.7</span> Distribuzione Beta<a href="#distribuzione-beta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una distribuzione che viene usata per modellare percentuali e proporzioni è la distribuzione Beta in quanto è definita sull’intervallo <span class="math inline">\((0; 1)\)</span> – ma non include i valori 0 o 1. Una definizione formale è la seguente.</p>
<div class="definition">
<p><span id="def:unlabeled-div-72" class="definition"><strong>(#def:unlabeled-div-72) </strong></span>Sia <span class="math inline">\(\pi\)</span> una variabile casuale che può assumere qualsiasi valore compreso tra 0 e 1, cioè <span class="math inline">\(\pi \in [0, 1]\)</span>. Diremo che <span class="math inline">\(\pi\)</span> segue la distribuzione Beta di parametri <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\pi \sim \text{Beta}(\alpha, \beta)\)</span>, se la sua densità è</p>
<p><span class="math display">\[\begin{align}
\text{Beta}(\pi \mid \alpha, \beta) &amp;= \frac{1}{B(\alpha, \beta)}\pi^{\alpha-1} (1-\pi)^{\beta-1}\notag\\
&amp;=  \frac{\Gamma(\alpha+ \beta)}{\Gamma(\alpha)\Gamma(\beta)}\pi^{\alpha-1} (1-\pi)^{\beta-1} \quad \text{per } \pi \in [0, 1]\,,
(\#eq:beta-distr-formula)
\end{align}\]</span></p>
<p>laddove <span class="math inline">\(B(\alpha, \beta)\)</span> è la funzione beta.</p>
</div>
<p>I termini <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> sono i parametri della distribuzione Beta e devono essere entrambi positivi. Tali parametri possono essere interpretati come l’espressione delle nostre credenze a priori relative ad una sequenza di prove Bernoulliane Il parametro <span class="math inline">\(\alpha\)</span> rappresenta il numero di “successi” e il parametro <span class="math inline">\(\beta\)</span> il numero di “insuccessi”:</p>
<p><span class="math display">\[\begin{equation}
\frac{\text{Numero di successi}}{\text{Numero di successi} + \text{Numero di insuccessi}} = \frac{\alpha}{\alpha + \beta}\notag\,.
\end{equation}\]</span></p>
<p>Il rapporto <span class="math inline">\(\frac{1}{B(\alpha, \beta)} = \frac{\Gamma(\alpha+b)}{\Gamma(\alpha)\Gamma(\beta)}\)</span> è una costante di normalizzazione:</p>
<p><span class="math display">\[\begin{equation}
\int_0^1 \pi^{\alpha-1} (1-\pi)^{\beta-1} = \frac{\Gamma(\alpha+b)}{\Gamma(\alpha)\Gamma(\beta)}\,.
\end{equation}\]</span></p>
<p>Ad esempio, con <span class="math inline">\(\alpha = 3\)</span> e <span class="math inline">\(\beta = 9\)</span> abbiamo</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>integrand <span class="ot">&lt;-</span> <span class="cf">function</span>(p) {p<span class="sc">^</span>{a <span class="sc">-</span> <span class="dv">1</span>}<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> p)<span class="sc">^</span>{b <span class="sc">-</span> <span class="dv">1</span>}}</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(integrand, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.00202 with absolute error &lt; 2.2e-17</span></span></code></pre></div>
<p>ovvero</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> (<span class="fu">gamma</span>(a <span class="sc">+</span> b) <span class="sc">/</span> (<span class="fu">gamma</span>(a) <span class="sc">*</span> <span class="fu">gamma</span>(b)))</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.00202</span></span></code></pre></div>
<p>ovvero</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">beta</span>(alpha, beta)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.00202</span></span></code></pre></div>
<p>Il valore atteso, la moda e la varianza di una distribuzione Beta sono dati dalle seguenti equazioni:</p>
<p><span class="math display">\[\begin{equation}
\E(\pi) = \frac{\alpha}{\alpha+\beta}\,,
(\#eq:beta-mean)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\Mo(\pi) = \frac{\alpha-1}{\alpha+\beta-2}\,,
(\#eq:beta-mode)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\Var(\pi) = \frac{\alpha \beta}{(\alpha+\beta)^2 (\alpha+\beta+1)}\,.
(\#eq:beta-var)
\end{equation}\]</span></p>
<p>Al variare di <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> si ottengono molte distribuzioni di forma diversa; un’illustrazione è fornita dalla seguente <a href="https://en.wikipedia.org/wiki/File:PDF_of_the_Beta_distribution.gif">GIF animata</a>. Si può ottenere una rappresentazione grafica della distribuzione <span class="math inline">\(\mbox{Beta}(\pi \mid \alpha, \beta)\)</span> con la funzione <code>plot_beta()</code> del pacchetto <code>bayesrules</code>. Per esempio:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>bayesrules<span class="sc">::</span><span class="fu">plot_beta</span>(<span class="at">alpha =</span> <span class="dv">3</span>, <span class="at">beta =</span> <span class="dv">9</span>)</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-66-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>La funzione <code>bayesrules::summarize_beta()</code> ci restituisce la media, moda e varianza della distribuzione Beta. Per esempio:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>bayesrules<span class="sc">::</span><span class="fu">summarize_beta</span>(<span class="at">alpha =</span> <span class="dv">3</span>, <span class="at">beta =</span> <span class="dv">9</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   mean mode     var     sd</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 0.25  0.2 0.01442 0.1201</span></span></code></pre></div>
<div class="remark">
<p><span id="unlabeled-div-73" class="remark"><em>Remark</em>. </span>Attenzione alle parole: in questo contesto, il termine “beta” viene utilizzato con tre significati diversi:</p>
<ul>
<li>la distribuzione di densità Beta,</li>
<li>la funzione matematica beta,</li>
<li>il parametro <span class="math inline">\(\beta\)</span>.</li>
</ul>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-74" class="exercise"><strong>(#exr:unlabeled-div-74) </strong></span>Nel disturbo depressivo la recidiva è definita come la comparsa di un nuovo episodio depressivo che si manifesta dopo un prolungato periodo di recupero (6-12 mesi) con stato di eutimia (umore relativamente normale). Supponiamo che una serie di studi mostri una comparsa di recidiva in una proporzione che va dal 20% al 60% dei casi, con una media del 40% <span class="citation">(per una recente discussione, si veda <a href="#ref-nuggerud2020analysis" role="doc-biblioref">Nuggerud-Galeas et al. 2020</a>)</span>. Sulla base di queste ipotetiche informazioni, è possibile usare la distribuzione Beta per rappresentare le nostre credenze a priori relativamente alla probabilità di recidiva. Per fare questo dobbiamo trovare i parametri della distribuzione Beta tali per cui la massa della densità sia compresa tra 0.2 e 0.6, con la media in corrispondenza di 0.4. Procedendo per tentativi ed errori, ed usando la funzione <code>bayesrules::plot_beta()</code>, un risultato possibile è <span class="math inline">\(\Beta(16, 24)\)</span>.</p>
<p>La funzione <code>find_pars()</code> prende in input la media e <span class="math inline">\(\alpha + \beta\)</span>, ritorna i valori dei parametri:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>find_pars <span class="ot">&lt;-</span> <span class="cf">function</span>(ev, n) {</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>  a <span class="ot">&lt;-</span> ev <span class="sc">*</span> n</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">&lt;-</span> n <span class="sc">-</span> a</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">round</span>(a), <span class="fu">round</span>(b)))</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>pars <span class="ot">&lt;-</span> <span class="fu">find_pars</span>(.<span class="dv">4</span>, <span class="dv">40</span>)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>pars</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 16 24</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>bayesrules<span class="sc">::</span><span class="fu">plot_beta</span>(pars[<span class="dv">1</span>], pars[<span class="dv">2</span>])</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-68-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Verifichiamo il valore della media della distribuzione:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="dv">16</span> <span class="sc">/</span> (<span class="dv">16</span> <span class="sc">+</span> <span class="dv">24</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4</span></span></code></pre></div>
<p>La moda è</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">16</span> <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (<span class="dv">16</span> <span class="sc">+</span> <span class="dv">24</span> <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.3947</span></span></code></pre></div>
<p>La deviazione standard della distribuzione è uguale a circa 8 punti percentuali:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>((<span class="dv">16</span> <span class="sc">*</span> <span class="dv">24</span>) <span class="sc">/</span> ((<span class="dv">16</span> <span class="sc">+</span> <span class="dv">24</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> (<span class="dv">16</span> <span class="sc">+</span> <span class="dv">24</span> <span class="sc">+</span> <span class="dv">1</span>)))</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.07651</span></span></code></pre></div>
<p>Gli stessi risultati si ottengono usando la funzione <code>bayesrules::summarize_beta()</code>:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>bayesrules<span class="sc">::</span><span class="fu">summarize_beta</span>(<span class="at">alpha =</span> <span class="dv">16</span>, <span class="at">beta =</span> <span class="dv">24</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   mean   mode      var      sd</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1  0.4 0.3947 0.005854 0.07651</span></span></code></pre></div>
<p>Possiamo concludere dicendo che, se utilizziamo la distribuzione <span class="math inline">\(\mbox{Beta}(16, 24)\)</span> per rappresentare le nostre credenze (a priori) rispetto la possibilità di recidiva, ciò significa che pensiamo che la nostra incertezza sia quantificabile nei termini di una deviazione standard di circa 8 punti percentuali rispetto a tutti i valori possibili di recidiva, per i quali il valore più verosimile (ovvero, la media della distribuzione) è 0.40.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-75" class="exercise"><strong>(#exr:unlabeled-div-75) </strong></span>Poniamoci ora il problema di verificare la nostra comprensione delle funzioni <span class="math inline">\(\R\)</span> che possono essere usate per la funzione Beta. Continuiamo con l’esercizio precedente e utilizziamo i seguenti parametri per la distribuzione Beta:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">16</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">24</span></span></code></pre></div>
<p>La media di una <span class="math inline">\(\mbox{Beta}(16, 24)\)</span> è</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="sc">/</span> (alpha <span class="sc">+</span> beta)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4</span></span></code></pre></div>
<p>In corrispondenza della media la densità della funzione è</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dbeta</span>(pi, alpha, beta)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<p>ovvero</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gamma</span>(alpha <span class="sc">+</span> beta) <span class="sc">/</span> (<span class="fu">gamma</span>(alpha) <span class="sc">*</span> <span class="fu">gamma</span>(beta)) <span class="sc">*</span> </span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>  pi<span class="sc">^</span>(alpha <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> pi)<span class="sc">^</span>(beta <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -6.995e+26</span></span></code></pre></div>
<p>Usando la funzione <code>dbeta()</code> possiamo costruire un grafico della funzione <span class="math inline">\(\mbox{Beta}(16, 24)\)</span> nel modo seguente:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="fl">1e4</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(x) <span class="sc">%&gt;%</span> </span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, <span class="fu">dbeta</span>(x, alpha, beta))) <span class="sc">+</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Probabilità di recidiva&quot;</span>,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Densità Beta(16, 24)&quot;</span></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-77-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="distribuzione-di-cauchy" class="section level2 hasAnchor" number="13.8">
<h2 class="hasAnchor"><span class="header-section-number">13.8</span> Distribuzione di Cauchy<a href="#distribuzione-di-cauchy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione di Cauchy è un caso speciale della distribuzione di <span class="math inline">\(t\)</span> di Student con 1 grado di libertà. È definita da una densità di probabilità che corrisponde alla funzione, dipendente da due parametri <span class="math inline">\(\theta\)</span> e <span class="math inline">\(d\)</span> (con la condizione <span class="math inline">\(d &gt; 0\)</span>),</p>
<p><span class="math display">\[\begin{equation}
f(x; \theta, d) = \frac{1}{\pi d} \frac{1}{1 + \left(\frac{x - \theta}{d} \right)^2},
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(\theta\)</span> è la mediana della distribuzione e <span class="math inline">\(d\)</span> ne misura la larghezza a metà altezza.</p>
</div>
<div id="distribuzione-log-normale" class="section level2 hasAnchor" number="13.9">
<h2 class="hasAnchor"><span class="header-section-number">13.9</span> Distribuzione log-normale<a href="#distribuzione-log-normale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sia <span class="math inline">\(y\)</span> una variabile casuale avente distribuzione normale con media <span class="math inline">\(\mu\)</span> e varianza <span class="math inline">\(\sigma^2\)</span>. Definiamo poi una nuova variabile casuale <span class="math inline">\(x\)</span> attraverso la relazione</p>
<p><span class="math display">\[
x = e^y \quad \Longleftrightarrow \quad y = \log x.
\]</span>
Il dominio di definizione della <span class="math inline">\(x\)</span> è il semiasse <span class="math inline">\(x &gt; 0\)</span> e la densità di probabilità <span class="math inline">\(f(x)\)</span> è data da</p>
<p><span class="math display">\[\begin{equation}
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \frac{1}{x} \exp \left\{-\frac{(\log x -  \mu)^2}{2 \sigma^2} \right\}.
\end{equation}\]</span></p>
<p>Questa funzione di densità si chiama log-normale.</p>
<p>Il valore atteso e la varianza di una distribuzione log-normale sono dati dalle seguenti equazioni:</p>
<p><span class="math display">\[\begin{equation}
\E(x) = \exp \left\{\mu + \frac{\sigma^2}{2} \right\}.
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\Var(x) = \exp \left\{2 \mu + \sigma^2 \right\} \left(\exp \left\{\sigma^2 \right\}  -1\right).
\end{equation}\]</span></p>
<p>Si può dimostrare che il prodotto di variabili casuali log-normali ed indipendenti segue una distribuzione log-normale.</p>
</div>
<div id="distribuzione-di-pareto" class="section level2 hasAnchor" number="13.10">
<h2 class="hasAnchor"><span class="header-section-number">13.10</span> Distribuzione di Pareto<a href="#distribuzione-di-pareto" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La distribuzione paretiana (o distribuzione di Pareto) è una distribuzione di probabilità continua e così chiamata in onore di Vilfredo Pareto. La distribuzione di Pareto è una distribuzione di probabilità con legge di potenza utilizzata nella descrizione di fenomeni sociali e molti altri tipi di fenomeni osservabili. Originariamente applicata per descrivere la distribuzione del reddito in una società, adattandosi alla tendenza che una grande porzione di ricchezza è detenuta da una piccola frazione della popolazione, la distribuzione di Pareto è diventata colloquialmente nota e indicata come il principio di Pareto, o “regola 80-20”. Questa regola afferma che, ad esempio, l’80% della ricchezza di una società è detenuto dal 20% della sua popolazione. Viene spesso applicata nello studio della distribuzione del reddito, della dimensione dell’impresa, della dimensione di una popolazione e nelle fluttuazioni del prezzo delle azioni.</p>
<p>La densità di una distribuzione di Pareto è</p>
<p><span class="math display">\[
f(x)=(x_m/x)^\alpha,
\]</span></p>
<p>dove <span class="math inline">\(x_m\)</span> (parametro di scala) è il minimo (necessariamente positivo) valore possibile di <span class="math inline">\(X\)</span> e <span class="math inline">\(\alpha\)</span> è un parametro di forma.</p>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-78-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>
La distribuzione di Pareto ha una asimmetria positiva. Il supporto della distribuzione di Pareto è la retta reale positiva. Tutti i valori devono essere maggiori del parametro di scala <span class="math inline">\(x_m\)</span>, che è in realtà un parametro di soglia.</p>
</div>
<div id="commenti-e-considerazioni-finali-9" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Commenti e considerazioni finali<a href="#commenti-e-considerazioni-finali-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In questa dispensa le densità continue che useremo più spesso sono la distribuzione gaussiana e la distribuzione Beta. Faremo un uso limitato della distribuzione <span class="math inline">\(t\)</span> di Student e della distribuzione di Cauchy. Le altre distribuzioni qui descritte sono stato presentate solo per completezza.</p>
<!--chapter:end:023_cont_rv_distr.Rmd-->
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-albert2019probability" class="csl-entry">
Albert, Jim, and Jingchen Hu. 2019. <em>Probability and Bayesian Modeling</em>. Chapman; Hall/CRC.
</div>
<div id="ref-definetti1931prob" class="csl-entry">
Finetti, Bruno de. 1931. <span>“Probabilismo.”</span> <em>Logos</em>, 163–219.
</div>
<div id="ref-horn2021underestimating" class="csl-entry">
Horn, Samantha, and George Loewenstein. 2021. <span>“Underestimating Learning by Doing.”</span> <em>Available at SSRN 3941441</em>.
</div>
<div id="ref-lazic2020determining" class="csl-entry">
Lazic, Stanley E, Elizaveta Semenova, and Dominic P Williams. 2020. <span>“Determining Organ Weight Toxicity with Bayesian Causal Models: Improving on the Analysis of Relative Organ Weights.”</span> <em>Scientific Reports</em> 10 (1): 1–12.
</div>
<div id="ref-McElreath_rethinking" class="csl-entry">
McElreath, Richard. 2020. <em>Statistical Rethinking: <span>A</span> <span>Bayesian</span> Course with Examples in <span>R</span> and <span>Stan</span></em>. 2nd Edition. Boca Raton, Florida: CRC Press.
</div>
<div id="ref-nuggerud2020analysis" class="csl-entry">
Nuggerud-Galeas, Shysset, Loreto Sáez-Benito Suescun, Nuria Berenguer Torrijo, Ana Sáez-Benito Suescun, Alejandra Aguilar-Latorre, Rosa Magallón Botaya, and Bárbara Oliván Blázquez. 2020. <span>“Analysis of Depressive Episodes, Their Recurrence and Pharmacologic Treatment in Primary Care Patients: A Retrospective Descriptive Study.”</span> <em>Plos One</em> 15 (5): e0233454.
</div>
<div id="ref-stevens46" class="csl-entry">
Stevens, Stanley Smith. 1946. <span>“On the Theory of Scales of Measurement.”</span> <em>Science</em> 103 (2684): 677–80.
</div>
<div id="ref-tufte_visual_display" class="csl-entry">
Tufte, Edward R. 2001. <em>The Visual Display of Quantitative Information</em>. Graphics press Cheshire, CT.
</div>
<div id="ref-zetschefuture2019" class="csl-entry">
Zetsche, Ulrike, Paul-Christian Bürkner, and Babette Renneberg. 2019. <span>“Future Expectations in Clinical Depression: <span>Biased</span> or Realistic?”</span> <em>Journal of Abnormal Psychology</em> 128 (7): 678–88.
</div>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Ricordatevi inoltre che gli individui tendono a sottostimare la propria capacità di apprendere <span class="citation">(<a href="#ref-horn2021underestimating" role="doc-biblioref">Horn and Loewenstein 2021</a>)</span>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>In questo e nei successivi Paragrafi di questo Capitolo introduco gli obiettivi della <em>data science</em> utilizzando una serie di concetti che saranno chiariti solo in seguito. Questa breve panoramica risulterà dunque solo in parte comprensibile ad una prima lettura e serve solo per definire la <em>big picture</em> dei temi trattati in questo insegnamento. Il significato dei termini qui utilizzati sarà chiarito nei Capitoli successivi.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Si veda l’Appendice @ref(es-pratico-zetsche).<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>In una sezione successiva di questo capitolo discuteremo i principi che, secondo Edward Tufte, devono guidare la Data Science. Parlando delle rappresentazioni grafiche dei dati, Edward Tufte ci dice che la prima cosa da fare è “mostrare i dati”. Questa può sembrare una tautologia, considerato che questo è lo scopo della statistica descrittiva: trasformare i dati attraverso vari indici riassuntivi o rappresentazioni grafiche, in modo tale da renderli <em>comprensibili</em>. Tuttavia, spesso le tecniche statistiche vengono usate per <em>nascondere</em> e non per <em>mostrare</em> i dati. L’uso delle frequenze relative offre un chiaro esempio di questo. Di questi tempi capita spesso di incontrare, sulla stampa, notizie a proposito un nuovo farmaco che, in una prova clinica, ha mostrato risultati incoraggianti che suggeriscono la sua efficacia come possibile trattamento del COVID-19. Alle volte i risultati della sperimentazione clinica sono riportati nei termini di una <em>frequenza relativa</em>. Ad esempio, potremmo leggere che l’uso del farmaco ha portato ad una riduzione del 21% dei ricoveri o dei decessi. Sembra tanto. Ma è necessario guardare i dati! Ovvero, molto spesso, quello che <em>non</em> viene riportato dai comunicati stampa. Infatti, una riduzione del 21% può corrispondere ad un cambiamento dal 5% al 4%. E una riduzione del 44% può corrispondere ad una differenza di 10 contro 18, o di 5 contro 9, o di 15 contro 27. In altri termini, una proporzione, anche grande, può corrispondere ad una differenza <em>assoluta</em> piuttosto piccola: un piccolo passo in avanti, ma non ad un balzo! Per questa ragione, per capire cosa i dati significano, è necessario guardare i dati da diversi punti di vista, utilizzando diverse statistiche descrittive, senza limitarci alla statistica descrittiva che racconta la storia che piace di più. Perché la scelta della statistica descrittiva da utilizzare per riassumere i dati dipende dagli scopi di chi esegue l’analisi statisica: il nostro scopo è quelloi di capire se il farmaco funziona; lo scopo delle compagnie farmaceutiche è quello di vendere il farmaco. Sono obiettivi molto diversi.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Il termine <em>standard deviation</em> è stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca <span class="math inline">\(\sigma\)</span> che lo rappresenta. Il termine italiano “deviazione standard” ne è la traduzione più utilizzata nel linguaggio comune; il termine dell’<a href="https://it.wikipedia.org/wiki/Ente_nazionale_italiano_di_unificazione">Ente Nazionale Italiano di Unificazione</a> è tuttavia “scarto tipo”, definito come la radice quadrata positiva della varianza.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Per un ripasso delle nozioni di base della teoria degli insiemi, si veda l’Appendice @ref(insiemistica).<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Georg Cantor dimostrò che era impossibile mappare uno a uno i reali negli interi, dimostrando così che l’insieme dei reali è non numerabile.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Per comodità, possiamo assumere che i valori impossibili di <span class="math inline">\(\theta\)</span> abbiano una densità uguale a zero.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Per quel che riguarda la notazione dell’integrale, ovvero <span class="math inline">\(\int_x \,\operatorname {d}\!x\)</span>, rimando alla discussione di S.P. Thompson: <a href="https://calculusmadeeasy.org/1.html" class="uri">https://calculusmadeeasy.org/1.html</a><a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Data una variabile casuale <span class="math inline">\(Y\)</span> con valore atteso <span class="math inline">\(\E(Y)\)</span>, le “distanze” tra i valori di <span class="math inline">\(Y\)</span> e il valore atteso <span class="math inline">\(\E(Y)\)</span> definiscono la variabile casuale <span class="math inline">\(Y - \E(Y)\)</span> chiamata <em>scarto</em>, oppure <em>deviazione</em> oppure <em>variabile casuale centrata</em>. La variabile <span class="math inline">\(Y - \E(Y)\)</span> equivale ad una traslazione di sistema di riferimento che porta il valore atteso nell’origine degli assi. Si può dimostrare facilmente che il valore atteso della variabile scarto <span class="math inline">\(Y - \E(Y)\)</span> vale zero, dunque la media di tale variabile non può essere usata per quantificare la “dispersione” dei valori di <span class="math inline">\(Y\)</span> relativamente al suo valore medio. Occorre rendere sempre positivi i valori di <span class="math inline">\(Y - \E(Y)\)</span> e tale risultato viene ottenuto considerando la variabile casuale <span class="math inline">\(\left(Y - \E(Y)\right)^2\)</span>.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Viene detta <em>scambiabilità</em> la proprietà per cui l’ordine con cui compiamo le osservazioni è irrilevante per l’assegnazione delle probabilità.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>La derivazione della formula del coefficiente binomiale è fornita nell’Appendice @ref(derivazione-coef-binom).<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>L’eguaglianza di <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> è solo una peculiarità di questo esempio.<a href="#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
