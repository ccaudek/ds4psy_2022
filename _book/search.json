[{"path":"index.html","id":"prefazione","chapter":"Prefazione","heading":"Prefazione","text":"Data Science per psicologi contiene il materiale delle lezioni dell’insegnamento di Psicometria B000286 (.. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Psicometria si propone di fornire agli studenti un’introduzione ’analisi dei dati psicologia. Le conoscenze/competenze che verranno sviluppate questo insegnamento sono quelle della Data science, ovvero un insieme di conoscenze/competenze che si pongono ’intersezione tra statistica (ovvero, richiedono la capacità di comprendere teoremi statistici) e informatica (ovvero, richiedono la capacità di sapere utilizzare un software).","code":""},{"path":"index.html","id":"la-psicologia-e-la-data-science","chapter":"Prefazione","heading":"La psicologia e la Data science","text":"Sembra sensato spendere due parole su un tema che è importante per gli studenti: quello indicato dal titolo di questo Capitolo. È ovvio che agli studenti di psicologia la statistica non piace. Se piacesse, forse studierebbero Data science e non psicologia; ma non lo fanno. Di conseguenza, gli studenti di psicologia si chiedono: “perché dobbiamo perdere tanto tempo studiare queste cose quando realtà quello che ci interessa è tutt’altro?” Questa è una bella domanda.C’è una ragione molto semplice che dovrebbe farci capire perché la Data science è così importante per la psicologia. Infatti, ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, certi casi, predire. questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data science psicologia: perché la Data science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.Sono sicuro che, leggendo queste righe, molti studenti sarà venuta mente la seguente domanda: perché non chiediamo qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data science? La risposta questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data science. Le tematiche della Data science non possono essere ignorate né dai ricercatori psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche professionisti al di fuori dall’università non possono fare meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data science! Basta aprire caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.Le considerazioni precedenti cercano di chiarire il seguente punto: la Data science non è qualcosa da studiare malincuore, un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data science tantissimi ambiti della loro attività professionale: particolare quando costruiscono, somministrano e interpretano test psicometrici. È dunque chiaro che possedere delle solide basi di Data science è un tassello imprescindibile del bagaglio professionale dello psicologo. questo insegnamento verrano trattati temi base della Data science e verrà adottato un punto di vista bayesiano, che corrisponde ’approccio più recente e sempre più diffuso psicologia.","code":""},{"path":"index.html","id":"come-studiare","chapter":"Prefazione","heading":"Come studiare","text":"Il giusto metodo di studio per prepararsi ’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare concetti via via che essi vengono presentati e verificare autonomia le procedure presentate lezione. Incoraggio gli studenti farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti utilizzare forum attivi su Moodle e, soprattutto, svolgere gli esercizi proposti su Moodle. problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino quel punto sono sufficienti rispetto alle richieste dell’esame.La prima fase dello studio, che è sicuramente individuale, è quella cui è necessario acquisire le conoscenze teoriche relative ai problemi che saranno presentati ’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso ci aiuta capire meglio.","code":""},{"path":"index.html","id":"sviluppare-un-metodo-di-studio-efficace","chapter":"Prefazione","heading":"Sviluppare un metodo di studio efficace","text":"Avendo insegnato molte volte passato un corso introduttivo di analisi dei dati ho notato nel corso degli anni che gli studenti con l’atteggiamento mentale che descriverò qui sotto generalmente ottengono ottimi risultati. Alcuni studenti sviluppano naturalmente questo approccio allo studio, ma altri hanno bisogno di fare uno sforzo per maturarlo. Fornisco qui sotto una breve descrizione del “metodo di studio” che, nella mia esperienza, è il più efficace per affrontare le richieste di questo insegnamento.Dedicate un tempo sufficiente al materiale di base, apparentemente facile; assicuratevi di averlo capito bene. Cercate le lacune nella vostra comprensione. Leggere presentazioni diverse dello stesso materiale (libri o articoli diversi) può fornire nuove intuizioni.Gli errori che facciamo sono nostri migliori maestri. Istintivamente cerchiamo di dimenticare subito nostri errori. Ma il miglior modo di imparare è apprendere dagli errori che commettiamo. questo senso, una soluzione corretta è meno utile di una soluzione sbagliata. Quando commettiamo un errore questo ci fornisce un’informazione importante: ci fa capire qual è il materiale di studio sul quale dobbiamo ritornare e che dobbiamo capire meglio.C’è ovviamente un aspetto “psicologico” nello studio. Quando un esercizio o problema ci sembra incomprensibile, la cosa migliore da fare è dire: “mi arrendo”, “non ho idea di cosa fare!”. Questo ci rilassa: ci siamo già arresi, quindi non abbiamo niente da perdere, non dobbiamo più preoccuparci. Ma non dobbiamo fermarci qui. Le cose “migliori” che faccio (se ci sono) le faccio quando non ho voglia di lavorare. Alle volte, quando c’è qualcosa che non fare e non ho idea di come affontare, mi dico: “oggi non ho proprio voglia di fare fatica”, non ho voglia di mettermi nello stato mentale per cui “10 minuti devo risolvere il problema perché dopo devo fare altre cose”. Però ho voglia di divertirmi con quel problema e allora mi dedico qualche aspetto “marginale” del problema, che come affrontare, oppure considero l’aspetto più difficile del problema, quello che non come risolvere, ma invece di cercare di risolverlo, guardo come altre persone hanno affrontato problemi simili, opppure lo stesso problema un altro contesto. Non mi pongo l’obiettivo “risolvi il problema 10 minuti”, ma invece quello di farmi un’idea “generale” del problema, o quello di capire un caso più specifico e più semplice del problema. Senza nessuna pressione. Infatti, quel momento ho deciso di non lavorare (ovvero, di non fare fatica). Va benissimo se “parto per la tangente”, ovvero se mi metto leggere del materiale che sembra avere poco che fare con il problema centrale (le nostre intuizioni e la nostra curiosità solitamente ci indirizzano sulla strada giusta). Quando faccio così, molto spesso trovo la soluzione del problema che mi ero posto e, paradossalmente, la trovo un tempo minore di quello che, precedenza, avevo dedicato “lavorare” al problema. Allora perché non faccio sempre così? C’è ovviamente l’aspetto dei “10 minuti” che non è sempre facile da dimenticare. Sotto pressione, possiamo solo agire maniera automatica, ovvero possiamo solo applicare qualcosa che già sappiamo fare. Ma se dobbiamo imparare qualcosa di nuovo, la pressione è un impedimento.È utile farsi da soli delle domande sugli argomenti trattati, senza limitarsi cercare di risolvere gli esercizi che vengono assegnati. Quando studio qualcosa mi viene mente: “se questo è vero, allora deve succedere quest’altra cosa”. Allora verifico se questo è vero, di solito con una simulazione. Se risultati della simulazione sono quelli che mi aspetto, allora vuol dire che ho capito. Se risultati sono diversi da quelli che mi aspettavo, allora mi rendo conto di non avere capito e ritorno indietro studiare con più attenzione la teoria che pensavo di avere capito – e ovviamente mi rendo conto che c’era un aspetto che avevo frainteso. Questo tipo di verifica è qualcosa che dobbiamo fare da soli, prima persona: nessun altro può fare questo al posto nostro.Non aspettatevi di capire tutto la prima volta che incontrate un argomento nuovo.1 È utile farsi una nota mentalmente delle lacune nella vostra comprensione e tornare su di esse seguito per carcare di colmarle. L’atteggiamento naturale, quando non capiamo dettagli di qualcosa, è quello di pensare: “non importa, ho capito maniera approssimativa questo punto, non devo preoccuparmi del resto”. Ma realtà non è vero: se la nostra comprensione è superficiale, quando il problema verrà presentato una nuova forma, non riusciremo risolverlo. Per cui dubbi che ci vengono quando studiamo qualcosa sono il nostro alleato più prezioso: ci dicono esattamente quali sono gli aspetti che dobbiamo approfondire per potere migliorare la nostra preparazione.È utile sviluppare una visione d’insieme degli argomenti trattati, capire l’obiettivo generale che si vuole raggiungere e avere chiaro il contributo che vari pezzi di informazione forniscono al raggiungimento di tale obiettivo. Questa organizzazione mentale del materiale di studio facilita la comprensione. È estremamente utile creare degli schemi di ciò che si sta studiando. Non aspettate che sia io fornirvi un riepilogo di ciò che dovete imparare: sviluppate da soli tali schemi e tali riassunti.Tutti noi dobbiamo imparare l’arte di trovare le informazioni, non solo nel caso di questo insegnamento. Quando vi trovate di fronte qualcosa che non capite, o ottenete un oscuro messaggio di errore da un software, ricordatevi: “Google friend”!\nCorrado Caudek\nMarzo 2022\n","code":""},{"path":"concetti-chiave.html","id":"concetti-chiave","chapter":"Capitolo 1 Concetti chiave","heading":"Capitolo 1 Concetti chiave","text":"La data science si pone ’intersezione tra statistica e informatica. La statistica è un insieme di metodi ugilizzati per estrarre informazioni dai dati; l’informatica implementa tali procedure un software. questo Capitolo vengono introdotti concetti fondamentali.","code":""},{"path":"concetti-chiave.html","id":"popolazioni-e-campioni","chapter":"Capitolo 1 Concetti chiave","heading":"1.1 Popolazioni e campioni","text":"Popolazione. L’analisi dei dati inizia con l’individuazione delle unità portatrici di informazioni circa il fenomeno di interesse. Si dice popolazione (o universo) l’insieme \\(\\Omega\\) delle entità capaci di fornire informazioni sul fenomeno oggetto dell’indagine statistica. Possiamo scrivere \\(\\Omega = \\{\\omega_i\\}_{=1, \\dots, n}= \\{\\omega_1, \\omega_2, \\dots, \\omega_n\\}\\), oppure \\(\\Omega = \\{\\omega_1, \\omega_2, \\dots \\}\\) nel caso di popolazioni finite o infinite, rispettivamente.L’obiettivo principale della ricerca psicologica è conoscere gli esiti psicologici e loro fattori trainanti nella popolazione. Questo è l’obiettivo delle sperimentazioni psicologiche e della maggior parte degli studi osservazionali psicologia. È quindi necessario essere molto chiari sulla popolazione cui si applicano risultati della ricerca. La popolazione può essere ben definita, ad esempio, tutte le persone che si trovavano nella città di Hiroshima al momento dei bombardamenti atomici e sono sopravvissute al primo anno, o può essere ipotetica, ad esempio, tutte le persone depresse che hanno subito o saranno sottoporsi ad un intervento di psicoterapia. Il ricercatore deve sempre essere grado di determinare se un soggetto appartiene alla popolazione oggetto di interesse.Una sottopopolazione è una popolazione sé e per sé che soddisfa proprietà ben definite. Negli esempi precedenti, potremmo essere interessati alla sottopopolazione di uomini di età inferiore ai 20 anni o di pazienti depressi sottoposti ad uno specifico intervento psicologico. Molte questioni scientifiche riguardano le differenze tra sottopopolazioni; ad esempio, confrontando gruppi con o senza psicoterapia per determinare se il trattamento è vantaggioso. modelli di regressione, introdotti nel Capitolo ?? riguardano le sottopopolazioni, quanto stimano il risultato medio per diversi gruppi (sottopopolazioni) definiti dalle covariate.Campione. Gli elementi \\(\\omega_i\\) dell’insieme \\(\\Omega\\) sono detti unità statistiche. Un sottoinsieme della popolazione, ovvero un insieme di elementi \\(\\omega_i\\), viene chiamato campione. Ciascuna unità statistica \\(\\omega_i\\) (abbreviata con u.s.) è portatrice dell’informazione che verrà rilevata mediante un’operazione di misurazione.Un campione è dunque un sottoinsieme della popolazione utilizzato per conoscere tale popolazione. differenza di una sottopopolazione definita base chiari criteri, un campione viene generalmente selezionato tramite un procedura casuale. Il campionamento casuale consente allo scienziato di trarre conclusioni sulla popolazione e, soprattutto, di quantificare l’incertezza sui risultati. campioni di un sondaggio sono esempi di campioni casuali, ma molti studi osservazionali non sono campionati casualmente. Possono essere campioni di convenienza, come coorti di studenti un unico istituto, che consistono di tutti gli studenti sottoposti ad un certo intervento psicologico quell’istituto. Indipendentemente da come vengono ottenuti campioni, il loro uso al fine di conoscere una popolazione target significa che problemi di rappresentatività sono inevitabili e devono essere affrontati.","code":""},{"path":"concetti-chiave.html","id":"variabili-e-costanti","chapter":"Capitolo 1 Concetti chiave","heading":"1.2 Variabili e costanti","text":"Definiamo variabile statistica la proprietà (o grandezza) che è\noggetto di studio nell’analisi dei dati. Una variabile è una proprietà\ndi un fenomeno che può essere espressa più valori sia numerici sia\ncategoriali. Il termine “variabile” si contrappone al termine “costante”\nche descrive una proprietà invariante di tutte le unità statistiche.Si dice modalità ciascuna delle varianti con cui una variabile\nstatistica può presentarsi. Definiamo insieme delle modalità di una\nvariabile statistica l’insieme \\(M\\) di tutte le possibili espressioni con\ncui la variabile può manifestarsi. Le modalità osservate e facenti parte\ndel campione si chiamano dati (si veda la\nTabella 1.1).Esempio 1.1  Supponiamo che il fenomeno studiato sia l’intelligenza. uno studio, la popolazione potrebbe corrispondere ’insieme di tutti gli italiani adulti. La variabile considerata potrebbe essere il punteggio del test standardizzato WAIS-IV. Le modalità di tale variabile potrebbero essere \\(112, 92, 121, \\dots\\). Tale variabile è di tipo quantitativo discreto.Esempio 1.2  Supponiamo che il fenomeno studiato sia il compito Stroop. La popolazione potrebbe corrispondere ’insieme dei bambini dai 6 agli 8 anni. La variabile considerata potrebbe essere il reciproco dei tempi di reazione secondi. Le modalità di tale variabile potrebbero essere \\(1 / 2.35, 1/ 1.49, 1/2.93, \\dots\\). La variabile è di tipo quantitativo continuo.Esempio 1.3  Supponiamo che il fenomeno studiato sia il disturbo di personalità. La popolazione potrebbe corrispondere ’insieme dei detenuti nelle carceri italiane. La variabile considerata potrebbe essere l’assessment del disturbo di personalità tramite interviste cliniche strutturate. Le modalità di tale variabile potrebbero essere Cluster , Cluster B, Cluster C descritti dal DSM-V. Tale variabile è di tipo qualitativo.","code":""},{"path":"concetti-chiave.html","id":"variabili-casuali","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.1 Variabili casuali","text":"Il termine variabile usato nella statistica è equivalente al termine variabile casuale usato nella teoria delle probabilità. Lo studio dei risultati degli interventi psicologici è lo studio delle variabili casuali che misurano questi risultati. Una variabile casuale cattura una caratteristica specifica degli individui nella popolazione e suoi valori variano tipicamente tra gli individui. Ogni variabile casuale può assumere teoria una gamma di valori sebbene, pratica, osserviamo un valore specifico per ogni individuo. Quando faremo riferiremo alle variabili casuali considerate termini generali useremo lettere maiuscole come \\(X\\) e \\(Y\\); quando faremo riferimento ai valori che una variabile casuale assume determinate circostanze useremo lettere minuscole come \\(x\\) e \\(y\\).","code":""},{"path":"concetti-chiave.html","id":"variabili-indipendenti-e-variabili-dipendenti","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.2 Variabili indipendenti e variabili dipendenti","text":"Un primo compito fondamentale qualsiasi analisi dei dati è l’identificazione delle variabili dipendenti (\\(Y\\)) e delle variabili indipendenti (\\(X\\)). Le variabili dipendenti sono anche chiamate variabili di esito o di risposta e le variabili indipendenti sono anche chiamate predittori o covariate. Ad esempio, nell’analisi di regressione, che esamineremo seguito, la domanda centrale è quella di capire come \\(Y\\) cambia al variare di \\(X\\). Più precisamente, la domanda che viene posta è: se il valore della variabile indipendente \\(X\\) cambia, qual è la conseguenza per la variabile dipendente \\(Y\\)? parole povere, le variabili indipendenti e dipendenti sono analoghe “cause” ed “effetti”, laddove le virgolette usate qui sottolineano che questa è solo un’analogia e che la determinazione delle cause può avvenire soltanto mediante l’utilizzo di un appropriato disegno sperimentale e di un’adeguata analisi statistica.Se una variabile è una variabile indipendente o dipendente dipende dalla domanda di ricerca. volte può essere difficile decidere quale variabile è dipendente e quale è indipendente, particolare quando siamo specificamente interessati ai rapporti di causa/effetto. Ad esempio, supponiamo di indagare l’associazione tra esercizio fisico e insonnia. Vi sono evidenze che l’esercizio fisico (fatto al momento giusto della giornata) può ridurre l’insonnia. Ma l’insonnia può anche ridurre la capacità di una persona di fare esercizio fisico. questo caso, dunque, non è facile capire quale sia la causa e quale l’effetto, quale sia la variabile dipendente e quale la variabile indipendente. La possibilità di identificare il ruolo delle variabili (dipendente/indipendente) dipende dalla nostra comprensione del fenomeno esame.Esempio 1.4  Uno psicologo convoca 120 studenti universitari per un test di memoria.\nPrima di iniziare l’esperimento, metà dei soggetti viene detto che si\ntratta di un compito particolarmente difficile; agli altri soggetti non\nviene data alcuna indicazione. Lo psicologo misura il punteggio nella\nprova di memoria di ciascun soggetto.questo esperimento, la variabile indipendente è l’informazione sulla difficoltà della prova. La variabile indipendente viene manipolata dallo sperimentatore assegnando soggetti (di solito maniera causale) o alla condizione (modalità) “informazione assegnata” o “informazione non data”. La\nvariabile dipendente è ciò che viene misurato nell’esperimento, ovvero\nil punteggio nella prova di memoria di ciascun soggetto.","code":""},{"path":"concetti-chiave.html","id":"la-matrice-dei-dati","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.3 La matrice dei dati","text":"Le realizzazioni delle variabili esaminate una rilevazione statistica\nvengono organizzate una matrice dei dati. Le colonne della matrice\ndei dati contengono gli insiemi dei dati individuali di ciascuna\nvariabile statistica considerata. Ogni riga della matrice contiene tutte\nle informazioni relative alla stessa unità statistica. Una generica\nmatrice dei dati ha l’aspetto seguente:\\[\nD_{m,n} =\n\\begin{pmatrix}\n  \\omega_1 & a_{1}   & b_{1}   & \\cdots & x_{1} & y_{1}\\\\\n  \\omega_2 & a_{2}   & b_{2}   & \\cdots & x_{2} & y_{2}\\\\\n  \\vdots   & \\vdots  & \\vdots  & \\ddots & \\vdots & \\vdots  \\\\\n\\omega_n  & a_{n}   & b_{n}   & \\cdots & x_{n} & y_{n}\n\\end{pmatrix}\n\\]\ndove, nel caso presente, la prima colonna contiene il\nnome delle unità statistiche, la seconda e la terza colonna si\nriferiscono due mutabili statistiche (variabili categoriali; \\(\\) e\n\\(B\\)) e ne presentano le modalità osservate nel campione mentre le ultime\ndue colonne si riferiscono due variabili statistiche (\\(X\\) e \\(Y\\)) e ne\npresentano le modalità osservate nel campione. Generalmente, tra le\nunità statistiche \\(\\omega_i\\) non esiste un ordine progressivo; l’indice\nattribuito alle unità statistiche nella matrice dei dati si riferisce\nsemplicemente alla riga che esse occupano.","code":""},{"path":"concetti-chiave.html","id":"parametri-e-modelli","chapter":"Capitolo 1 Concetti chiave","heading":"1.3 Parametri e modelli","text":"Ogni variabile casuale ha una distribuzione che descrive la probabilità che la variabile assuma qualsiasi valore un dato intervallo.2 Senza ulteriori specificazioni, una distribuzione può fare riferimento un’intera famiglia di distribuzioni. parametri, tipicamente indicati con lettere greche come \\(\\mu\\) e \\(\\alpha\\), ci permettono di specificare di quale membro della famiglia stiamo parlando. Quindi, si può parlare di una variabile casuale con una distribuzione Normale, ma se viene specificata la media \\(\\mu\\) = 100 e la varianza \\(\\sigma^2\\) = 15, viene individuata una specifica distribuzione Normale – nell’esempio, la distribuzione del quoziente di intelligenza.metodi statistici parametrici specificano la famiglia delle distribuzioni e quindi utilizzano dati per individuare, stimando parametri, una specifica distribuzione ’interno della famiglia di distribuzioni ipotizzata. Se \\(f\\) è la PDF di una variabile casuale \\(Y\\), l’interesse può concentrarsi sulla sua media e varianza. Nell’analisi di regressione, ad esempio, cerchiamo di spiegare come parametri di \\(f\\) dipendano dalle covariate \\(X\\). Nella regressione lineare classica, assumiamo che \\(Y\\) abbia una distribuzione normale con media \\(\\mu = \\E(Y)\\), e stimiamo come \\(\\E(Y)\\) dipenda da \\(X\\). Poiché molti esiti psicologici non seguono una distribuzione normale, verranno introdotte distribuzioni più appropriate per questi risultati. metodi non parametrici, invece, non specificano una famiglia di distribuzioni per \\(f\\). queste dispense faremo riferimento metodi non parametrici quando discuteremo della statistica descrittiva.Il termine modello è onnipresente statistica e nella data science. Il modello statistico include le ipotesi e le specifiche matematiche relative alla distribuzione della variabile casuale di interesse. Il modello dipende dai dati e dalla domanda di ricerca, ma raramente è unico; nella maggior parte dei casi, esiste più di un modello che potrebbe ragionevolmente usato per affrontare la stessa domanda di ricerca e avendo disposizione dati osservati. Nella previsione delle aspettative future dei pazienti depressi che discuteremo seguito (Zetsche, Bürkner, Renneberg 2019), ad esempio, la specifica del modello include l’insieme delle covariate candidate, l’espressione matematica che collega predittori con le aspettative future e qualsiasi ipotesi sulla distribuzione della variabile dipendente. La domanda di cosa costituisca un buon modello è una domanda su cui torneremo ripetutamente questo insegnamento.","code":""},{"path":"concetti-chiave.html","id":"effetto","chapter":"Capitolo 1 Concetti chiave","heading":"1.4 Effetto","text":"L’effetto è una qualche misura dei dati. Dipende dal tipo di dati e dal tipo di test statistico che si vuole utilizzare. Ad esempio, se viene lanciata una moneta 100 volte e esce testa 66 volte, l’effetto sarà 66/100. Diventa poi possibile confrontare l’effetto ottenuto con l’effetto nullo che ci si aspetterebbe da una moneta bilanciata (50/100), o con qualsiasi altro effetto che può essere scelto. La dimensione dell’effetto si riferisce alla differenza tra l’effetto misurato nei dati e l’effetto nullo (di solito un valore che ci si aspetta di ottenere base al caso soltanto).","code":""},{"path":"concetti-chiave.html","id":"stima-e-inferenza","chapter":"Capitolo 1 Concetti chiave","heading":"1.5 Stima e inferenza","text":"La stima è il processo mediante il quale il campione viene utilizzato per conoscere le proprietà di interesse della popolazione. La media campionaria è una stima naturale della media della popolazione e la mediana campionaria è una stima naturale della mediana della popolazione. Quando parliamo di stimare una proprietà della popolazione (volte indicata come parametro della popolazione) o di stimare la distribuzione di una variabile casuale, stiamo parlando dell’utilizzo dei dati osservati per conoscere le proprietà di interesse della popolazione. L’inferenza statistica è il processo mediante il quale le stime campionarie vengono utilizzate per rispondere domande di ricerca e per valutare specifiche ipotesi relative alla popolazione. Discuteremo le procedure bayesiane dell’inferenza nell’ultima parte di queste dispense.","code":""},{"path":"concetti-chiave.html","id":"metodi-e-procedure-della-psicologia","chapter":"Capitolo 1 Concetti chiave","heading":"1.6 Metodi e procedure della psicologia","text":"Un modello psicologico di un qualche aspetto del comportamento umano o della mente ha le seguenti proprietà:descrive le caratteristiche del comportamento questione,formula predizioni sulle caratteristiche future del comportamento,è sostenuto da evidenze empiriche,deve essere falsificabile (ovvero, linea di principio, deve\npotere fare delle predizioni su aspetti del fenomeno considerato che\nnon sono ancora noti e che, se venissero indagati, potrebbero\nportare rigettare il modello, se si dimostrassero incompatibili con\nesso).\nL’analisi dei dati valuta un modello psicologico utilizzando strumenti statistici.Questa dispensa è strutturata maniera tale da rispecchiare la suddivisione tra temi della misurazione, dell’analisi descrittiva e dell’inferenza. Nel prossimo Capitolo sarà affrontato il tema della misurazione e, nell’ultima parte della dispensa verrà discusso l’argomento più difficile, quello dell’inferenza. Prima di affrontare il secondo tema, l’analisi descrittiva dei dati, sarà necessario introdurre il linguaggio di programmazione statistica R (un’introduzione R è fornita Appendice). Inoltre, prima di potere discutere l’inferenza, dovranno essere introdotti concetti di base della teoria delle probabilità, quanto l’inferenza non è che l’applicazione della teoria delle probabilità ’analisi dei dati.","code":""},{"path":"chapter-misurazione.html","id":"chapter-misurazione","chapter":"Capitolo 2 La misurazione in psicologia","heading":"Capitolo 2 La misurazione in psicologia","text":"Introduco il problema della misurazione psicologia parlando dell’intelligenza. quanto psicologi, siamo abituati pensare alla misurazione dell’intelligenza, ma anche le persone che non sono psicologi sono ben familiari con la misurazione dell’intelligenza: tra le misurazioni delle caratteristiche psicologiche, infatti, la misurazione dell’intelligenza è forse la più conosciuta.test di intelligenza consistono una serie di problemi di carattere verbale, numerico o simbolico. Come ci si può aspettare, alcune persone riescono risolvere correttamente un numero maggiore di problemi di altre. Possiamo contare il numero di risposte corrette e osservare le differenze individuali nei punteggi calcolati. Scopriamo questo modo che le differenze individuali nell’abilità di risolvere tali problemi risultano sorprendentemente stabili nell’età adulta. Inoltre, diversi test di intelligenza tendono ad essere correlati positivamente: le persone che risolvono un maggior numero di problemi verbali, media, tenderanno anche risolvere correttamente un numero più grande di numerici e simbolici. Esiste quindi una notevole coerenza delle differenze osservate tra le persone, sia nel tempo sia considerando diverse procedure di test e valutazione.Avendo stabilito che ci sono differenze individuali tra le persone, è possibile esaminare le associazioni tra punteggi dei test di intelligenza e altre variabili. Possiamo indagare se le persone con punteggi più alti nei test di intelligenza, rispetto persone che ottengono punteggi più bassi, hanno più successo sul lavoro; se guadagnano di più; se votano modo diverso; o se hanno un’aspettativa di vita più alta. Possiamo esaminare le differenze nei punteggi dei test di intelligenza funzione di variabili come il genere, il gruppo etnico-razziale o lo stato socio-economico. Possiamo fare ricerche sull’associazione tra punteggi dei test di intelligenza e l’efficienza dell’elaborazione neuronale, tempi di reazione o la quantità di materia grigia ’interno della scatola cranica. Tutte queste ricerche sono state condotte e gli psicologi hanno scoperto una vasta gamma di associazioni tra le misure dell’intelligenza e altre variabili. Alcune di queste associazioni sono grandi e stabili, altre sono piccole e difficili da replicare. riferimento ’intelligenza, dunque, gli psicologi hanno condotto un enorme numero di ricerche ponendosi domande diverse. quali condizioni si verificano determinati effetti? Quali variabili mediano o moderano le relazioni tra punteggi dei test di intelligenza e altre variabili? Queste relazioni si mantengono stabili diversi gruppi di persone? Le ricerche sull’intelligenza umana sono un campo continuo sviluppo.Tuttavia, tuttavia una domanda sorge spontanea: test di intelligenza misurano davvero qualcosa e, caso affermativo, che cos’è questo qualcosa? Infatti, dopo un secolo di teoria e ricerca sui punteggi dei test di intelligenza e, generale, sui test psicologici, non sappiamo ancora con precisione cosa effettivamente questi test misurano.\nQueste considerazioni relative ai test di intelligenza ci conducono dunque alla domanda che ha motivato le precedenti considerazioni: cosa significa misurare un attributo psicologico? Questa è una domanda cui è difficile rispondere, una domanda cui è dedicata un’intera area di ricerca, quella della teoria della misurazione psicologica.Non possiamo qui entrare nel merito delle complessità formali della teoria della misurazione psicologica – questo argomento verrà approfondito nei successivi insegnamenti sulla testistica psicologica. Ci limiteremo invece presentare alcune nozioni di base su un tema centrale della teoria della misurazione psicologica: il tema delle scale delle misure psicologiche.","code":""},{"path":"chapter-misurazione.html","id":"le-scale-di-misura","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.1 Le scale di misura","text":"generale possiamo dire che la teoria della misurazione si occupa dello studio delle relazioni esistenti tra due domini: il “mondo fisico” e il “mondo psicologico”. Secondo la teoria della misurazione, la misurazione è un’attività rappresentativa, cioè è un processo di assegnazione di numeri modo tale da preservare, ’interno del dominio numerico, le relazioni qualitative che sono state osservate nel mondo empirico. La teoria della misurazione ha lo scopo di specificare le condizioni necessarie per la costruzione di una rappresentazione adeguata delle relazioni empiriche ’interno di un sistema numerico. Da una prospettiva formale, le operazioni descritte dalla teoria della misurazione possono essere concettualizzate termini di mappatura tra le relazioni esistenti ’interno di due insiemi (quello empirico e quello numerico). Il risultato di questa attività è chiamato “scala di misurazione”.Una famosa teoria delle scale di misura è stata proposta da Stevens (1946). Stevens ci fa notare che, linea di principio, le variabili psicologiche sono grado di rappresentare (preservare) con diversi gradi di accuratezza le relazioni qualitative che sono state osservate nei fenomeni psicologici. Secondo la teoria di Stevens, possiamo distinguere tra quattro scale di misura: le scale nominali (nominal scales), ordinali (ordinal scales), intervalli (interval scales), di rapporti (ratio scales). Tali scale di misura consentono operazioni aritmetiche diverse, come indicato nella tabella successiva, quanto ciasuna di esse è grado di “catturare” soltanto alcune delle proprietà dei fenomeni psicologici che intende misurare.","code":""},{"path":"chapter-misurazione.html","id":"scala-nominale","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.1.1 Scala nominale","text":"Il livello di misurazione più semplice è quello della scala nominale. Questa scala di misurazione corrisponde ad una tassonomia. simoboli o numeri che costituiscono questa scala non sono altro che nomi delle categorie che utilizziamo per classificare fenomeni psicologici. base alle misure fornite da una scala nominale, l’unica cosa che siamo grado di dire proposito di una caratteristica psicologica è se essa è uguale o ad un’altra caratteristica psicologica.La scala nominale raggruppa dunque dati categorie qualitative mutuamente esclusive (cioè nessun dato si può collocare più di una categoria).\nEsiste la sola relazione di equivalenza tra le misure delle u.s., cioè\nnella scala nominale gli elementi del campione appartenenti classi\ndiverse sono differenti, mentre tutti quelli della stessa classe sono\ntra loro equivalenti: \\(x_i = x_j\\) oppure \\(x_i \\neq x_j\\).L’unica operazione algebrica che possiamo compiere sulle modalità della scala nominale è quella di contare le u.s. che appartengono ad ogni modalità e contare il numero delle modalità (classi di equivalenza). Dunque la descrizione dei dati avviene tramite le frequenze assolute e le frequenze relative.partire da una scala nominale è possibile costruire altre scale nominali che sono equivalenti alla prima trasformando valori della scala di partenza modo tale\nda cambiare nomi delle modalità, ma lasciando però inalterata la suddivisione u.s. nelle medesime classi di equivalenza. Questo significa che prendendo una variabile misurata su scala nominale e cambiando nomi delle sue categorie otteniamo una nuova variabile esattamente corrispondente alla prima.","code":""},{"path":"chapter-misurazione.html","id":"scala-ordinale","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.1.2 Scala ordinale","text":"La scala ordinale conserva la proprietà della scala nominale di classificare ciascuna u.s. ’interno di una e una sola categoria, ma alla relazione di equivalenza tra elementi di una stessa classe aggiunge la relazione di ordinamento tra le classi di equivalenza. Essendo basata su una relazione d’ordine, una scala ordinale descrive soltanto l’ordine di rango tra le modalità, ma non ci dà alcuna informazione su quanto una modalità sia più grande di un’altra. Non ci dice, per esempio, se la distanza tra le modalità \\(\\) e \\(b\\) sia uguale, maggiore o minore della distanza tra le modalità \\(b\\) e \\(c\\).Esempio 2.1  Un esempio classico di scala ordinale è quello della scala Mohs per la\ndeterminazione della durezza dei minerali. Per stabilire la durezza dei\nminerali si usa il criterio empirico della scalfittura. Vengono\nstabiliti livelli di durezza crescente da 1 10 con riferimento dieci\nminerali: talco, gesso, calcite, fluorite, apatite, ortoclasio, quarzo,\ntopazio, corindone e diamante. Un minerale appartenente ad uno di questi\nlivelli se scalfisce quello di livello inferiore ed è scalfito da quello\ndi livello superiore.","code":""},{"path":"chapter-misurazione.html","id":"scala-ad-intervalli","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.1.3 Scala ad intervalli","text":"La scala ad intervalli include le proprietà di quella nominale e di\nquella ordinale, e più consente di misurare le distanze tra le coppie\ndi u.s. nei termini di un intervallo costante, chiamato unità di\nmisura, cui viene attribuito il valore “1”. La posizione dell’origine\ndella scala, cioè il punto zero, è scelta arbitrariamente, nel senso che\nnon indica l’assenza della quantità che si sta misurando. Avendo uno\nzero arbitrario, questa scala di misura consente valori negativi. Lo\nzero, infatti, non viene attribuito ’u.s. cui la proprietà\nmisurata risulta assente.La scala intervalli equivalenti ci consente di effettuare operazioni\nalgebriche basate sulla differenza tra numeri associati ai diversi\npunti della scala, operazioni algebriche non era possibile eseguire nel\ncaso di misure livello di scala ordinale o nominale. Il limite della\nscala ad intervalli è quello di non consentire il calcolo del rapporto\ntra coppie di misure. Possiamo dire, per esempio, che la distanza tra\n\\(\\) e \\(b\\) è la metà della distanza tra \\(c\\) e \\(d\\). Oppure che la distanza\ntra \\(\\) e \\(b\\) è uguale alla distanza tra \\(c\\) e \\(d\\). Non possiamo dire,\nperò, che \\(\\) possiede la proprietà misurata quantità doppia rispetto\n\\(b\\). Non possiamo cioè stabilire dei rapporti diretti tra le misure\nottenute. Solo per le differenze tra le modalità sono dunque permesse\ntutte le operazioni aritmetiche: le differenze possono essere tra loro\nsommate, elevate potenza oppure divise, determinando così le quantità\nche stanno alla base della statistica inferenziale.Nelle scale ad intervalli equivalenti, l’unità di misura è arbitraria,\novvero può essere cambiata attraverso una dilatazione, operazione che\nconsiste nel moltiplicare tutti valori della scala per una costante\npositiva. Poiché l’aggiunta di una costante non altera le differenze tra\nvalori della scala, è anche ammessa la traslazione, operazione che\nconsiste nel sommare una costante tutti valori della scala. Essendo\nla scala invariate rispetto alla traslazione e alla dilatazione, le\ntrasformazioni ammissibili sono le trasformazioni lineari:\\[\ny' = + , \\quad b > 0.\n\\]\nL’aspetto che rimane invariante seguito di una trasformazione lineare\nè l’uguaglianza dei rapporti fra intervalli.Esempio 2.2  Esempio di scala ad intervalli è la temperatura misurata gradi\nCelsius o Fahrenheit, ma non Kelvin. Come per la scala nominale, è\npossibile stabilire se due modalità sono uguali o diverse: 30\\(^\\circ\\)C\n\\(\\neq\\) 20\\(^\\circ\\)C. Come per la scala ordinale è possibile mettere due\nmodalità una relazione d’ordine: 30\\(^\\circ\\)C \\(>\\) 20\\(^\\circ\\)C. \naggiunta ai casi precedenti, però, è possibile definire una unità di\nmisura per cui è possibile dire che tra 30\\(^\\circ\\)C e 20\\(^\\circ\\)C c’è\nuna differenza di 30\\(^\\circ\\) - 20\\(^\\circ\\) = 10\\(^\\circ\\)C. valori di\ntemperatura, oltre poter essere ordinati secondo l’intensità del\nfenomeno, godono della proprietà che le differenze tra loro sono\ndirettamente confrontabili e quantificabili.Il limite della scala ad intervalli è quello di non consentire il\ncalcolo del rapporto tra coppie di misure. Ad esempio, una temperatura\ndi 80\\(^\\circ\\)C non è il doppio di una di 40\\(^\\circ\\)C. Se infatti\nesprimiamo le stesse temperature nei termini della scala Fahrenheit,\nallora due valori non saranno rapporto di 1 2 tra loro. Infatti,\n20\\(^\\circ\\)C = 68\\(^\\circ\\)F e 40\\(^\\circ\\)C = 104\\(^\\circ\\)F. Questo significa\nche la relazione “il doppio di” che avevamo individuato precedenza si\napplicava ai numeri della scala centigrada, ma non alla proprietà\nmisurata (cioè la temperatura). La decisione di che scala usare\n(Centigrada vs. Fahrenheit) è arbitraria. Ma questa arbitrarietà non\ndeve influenzare le inferenze che traiamo dai dati. Queste inferenze,\ninfatti, devono dirci qualcosa proposito della realtà empirica e non\npossono nessun modo essere condizionate dalle nostre scelte\narbitrarie che ci portano scegliere la scala Centigrada piuttosto che\nquella Fahrenheit.Consideriamo ora l’aspetto invariante di una trasformazione lineare, ovvero l’uguaglianza dei rapporti fra intervalli. Prendiamo esame, ad esempio, tre temperature:\n\\(20^\\circ C = 68^\\circ F\\),\n\\(15^\\circ C = 59^\\circ F\\),\n\\(10^\\circ C = 50 ^\\circ F\\).È facile rendersi conto del fatto che rapporti fra intervalli restano costanti indipendentemente dall’unità di misura che è stata scelta:\\[\n  \\frac{20^\\circ C - 10^\\circ C}{20^\\circ C - 15^\\circ C} =\n  \\frac{68^\\circ F - 50^\\circ F}{68^\\circ F-59^\\circ F} = 2.\n\\]","code":""},{"path":"chapter-misurazione.html","id":"scala-di-rapporti","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.1.4 Scala di rapporti","text":"Nella scala rapporti equivalenti la posizione dello zero non è\narbitraria, ma corrisponde ’elemento dotato di intensità nulla\nrispetto alla proprietà misurata. Una scala rapporti equivalenti si\ncostruisce associando il numero 0 ’elemento con intensità nulla;\nviene poi scelta un’unità di misura \\(u\\) e, ad ogni elemento, si assegna\nun numero \\(\\) definito come: \\[= \\frac{d}{u}\\] dove \\(d\\) rappresenta la\ndistanza dall’origine. Alle u.s. vengono dunque assegnati dei numeri\ntali per cui le differenze e rapporti tra numeri riflettono le\ndifferenze e rapporti tra le intensità della proprietà misurata.Operazioni aritmetiche sono possibili non solo sulle differenze tra \nvalori della scala (come per la scala intervalli equivalenti), ma\nanche sui valori stessi della scala. L’unica arbitrarietà riguarda\nl’unità di misura che si utilizza. L’unità di misura può cambiare, ma\nqualsiasi unità di misura si scelga, lo zero deve sempre indicare\nl’intensità nulla della proprietà considerata.Le trasformazioni ammissibili questo livello di scala sono dette\ntrasformazioni di similarità: \\[y' = , \\quad b > 0.\\] questo livello\ndi scala, seguito delle trasformazioni ammissibili, rimangono\ninvariati anche rapporti: \\[\\frac{y_i}{y_j} = \\frac{y'_i}{y'_j}.\\]","code":""},{"path":"chapter-misurazione.html","id":"gerarchia-dei-livelli-di-scala-di-misura","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.2 Gerarchia dei livelli di scala di misura","text":"Stevens (1946) parla di livelli di scala poiché quattro tipi di scala di\nmisura stanno una precisa gerarchia: la scala nominale rappresenta il\nlivello più basso della misurazione, la scala rapporti equivalenti è\ninvece il livello più alto.Passando da un livello di misurazione ad uno più alto aumenta il numero di operazioni aritmetiche che possono essere compiute sui valori della scala, come indicato nella figura seguente.Per ciò che riguarda le trasformazioni ammissibili, più il livello di\nscala è basso, più le funzioni sono generali (sono minori cioè vincoli\nper passare da una rappresentazione numerica ad un’altra equivalente).\nSalendo la gerarchia, la natura delle funzioni di trasformazione si fa\npiù restrittiva.","code":""},{"path":"chapter-misurazione.html","id":"variabili-discrete-o-continue","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.3 Variabili discrete o continue","text":"Le variabili livello di intervalli e di rapporti possono essere\ndiscrete o continue. Le variabili discrete possono assumere alcuni\nvalori ma non altri. Una volta che l’elenco di valori accettabili è\nstato specificato, non ci sono casi che cadono tra questi valori.\nLe variabili discrete di solito assumono valori interi.Quando una variabile può assumere qualsiasi valore entro un intervallo\nspecificato, allora si dice che la variabile è continua. teoria, ciò\nsignifica che frazioni e decimali possono essere utilizzati per\nraggiungere un livello di precisione qualsiasi. pratica, un certo\npunto dobbiamo arrotondare numeri, rendendo tecnicamente la variabile\ndiscreta. variabili veramente discrete, tuttavia, non è possibile\naumentare piacimento il livello di precisione della misurazione.Esempio 2.3  Il numero di biciclette possedute da una persona è una variabile discreta poiché tale variabile può assumere come modalità solo numeri interi non negativi. Frazioni di bicicletta non hanno senso.","code":""},{"path":"chapter-misurazione.html","id":"alcune-misure-sono-migliori-di-altre","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.4 Alcune misure sono migliori di altre","text":"psicologia, ciò che vogliamo misurare non è una caratteristica fisica, ma invece è un concetto teorico inosservabile, ovvero un costrutto. Un costrutto rappresenta il risultato di una fondata riflessione scientifica, non è per definizione accessibile ’osservazione diretta, ma viene inferito dall’osservazione di opportuni indicatori (Sartori, 2005). Ad esempio, supponiamo che un docente voglia valutare quanto bene uno studente comprenda la distinzione tra le quattro diverse scale di misura che sono state descritte sopra. Il docente potrebbe predisporre un test costituito da un insieme di domande e potrebbe contare quante domande lo studente risponde correttamente. Questo\ntest, però, può o può non essere una buona misura del costrutto relativo\nalla conoscenza effettiva delle quattro scale di misura. Per esempio, se\nil docente scrive le domande del test modo ambiguo o se usa una\nlinguaggio troppo tecnico che lo studente non conosce, allora \nrisultati del test potrebbero suggerire che lo studente non conosce la\nmateria questione anche se realtà questo non è vero. D’altra\nparte, se il docente prepara un test scelta multipla con risposte\nerrate molto ovvie, allora lo studente può ottenere dei buoni risultati\nal test anche senza essere grado di comprendere adeguatamente le\nproprietà delle quattro scale di misura. generale non è possibile misurare un costrutto senza una certa quantità di errore. Poniamoci dunque il problema di determinare che modo una misurazione possa dirsi adeguata.","code":""},{"path":"chapter-misurazione.html","id":"tipologie-di-errori","chapter":"Capitolo 2 La misurazione in psicologia","heading":"2.4.1 Tipologie di errori","text":"L’errore è, per definizione, la differenza tra il valore vero e il\nvalore misurato della grandezza esame. Gli errori sono classificati\ncome sistematici (o determinati) e casuali (o indeterminati). Gli errori\ncasuali sono fluttuazioni, eccesso o difetto rispetto al valore\nreale, delle singole determinazioni e sono dovuti alle molte variabili\nincontrollabili che influenzano ogni misura psicologica. Gli errori\nsistematici, invece, influiscono sulla misurazione sempre nello stesso\nsenso e, solitamente, per una stessa quantità (possono essere additivi o\nproporzionali).Le differenze tra le due tipologie di errori, sistematici e casuali,\nintroducono concetti di accuratezza e di precisione della misura. Una\nmisura viene definita:accurata, quando vi è un accordo tra la misura effettuata ed il\nvalore reale;precisa quando, ripetendo più volte la misura, risultati\nottenuti sono concordanti, cioè differiscono maniera irrilevante\ntra loro.La metafora del tiro bersaglio illustra la relazione tra precisione e accuratezza.\nFIGURA 2.1: Metafora del tiro al bersaglio.\nPer tenere sotto controllo l’incidenza degli errori, sono stati\nintrodotti psicologia concetti di attendibilità e validità.Uno strumento si dice attendibile quando valuta modo coerente e\nstabile la stessa variabile: risultati ottenuti si mantengono costanti\ndopo ripetute somministrazione ed assenza di variazioni psicologiche\ne fisiche dei soggetti sottoposti al test o cambiamenti dell’ambiente \ncui ha luogo la somministrazione.L’attendibilità di uno strumento, però, non è sufficiente: primo luogo uno\nstrumento di misura deve essere valido, laddove la validità rappresenta\nil grado cui uno strumento misura effettivamente ciò che dovrebbe\nmisurare. genere, si fa riferimento ad almeno quattro tipi di\nvalidità.La validità di costrutto riguarda il grado cui un test misura\nciò per cui è stato costruito. Essa si suddivide : validità\nconvergente e validità divergente. La validità convergente fa\nriferimento alla concordanza tra uno strumento e un altro che misura\nlo stesso costrutto. La validità divergente, al contrario, valuta il\ngrado di discriminazione tra strumenti che misurano costrutti\ndifferenti. Senza validità di costrutto le altre forme di validità\nnon hanno senso.base alla validità di contenuto, un test fornisce una misura\nvalida di un attributo psicologico se il dominio dell’attributo è\nrappresentato maniera adeguata dagli item del test. Un requisito\ndi base della validità di contenuto è la rilevanza e la\nrappresentatività del contenuto degli item riferimento\n’attributo che il test intende misurare.La validità di criterio valuta il grado di concordanza tra \nrisultati dello strumento considerato e risultati ottenuti da\naltri strumenti che misurano lo stesso costrutto, o tra risultati\ndello strumento considerato e un criterio esterno. Nella validità\nconcorrente, costrutto e criterio vengono misurati contestualmente,\nconsentendo un confronto immediato. Nella validità predittiva, il\ncostrutto viene misurato prima e il criterio un momento\nsuccessivo, consentendo la valutazione della capacità dello\nstrumento di predire un evento futuro.Infine, la validità di facciata fa riferimento al grado cui il\ntest appare valido ai soggetti cui esso è diretto. La validità di\nfacciata è importante ambiti particolari, quali ad esempio la\nselezione del personale per una determinata occupazione. questo\ncaso è ovviamente importante che chi si sottopone al test ritenga\nche il test vada misurare quegli aspetti che sono importanti per\nle mansioni lavorative che dovranno essere svolte, piuttosto che\naltre cose. generale, la validità di facciata non è utile, tranne\ncasi particolari.","code":""},{"path":"chapter-misurazione.html","id":"commenti-e-considerazioni-finali","chapter":"Capitolo 2 La misurazione in psicologia","heading":"Commenti e considerazioni finali","text":"Una domanda che uno psicologo spesso si pone è: “sulla base delle\nevidenze osservate, possiamo concludere dicendo che l’intervento\npsicologico è efficace nel trattamento e nella cura del disturbo?” Le\nconsiderazioni svolte questo capitolo dovrebbero farci capire che,\nprima di cercare di rispondere questa domanda con l’analisi statistica\ndei dati, devono essere affrontati problemi della validità e\ndell’attendibilità delle misure (oltre stabilire l’appropriato livello\ndi scala di misura delle osservazioni). L’attendibilità è un\nprerequisito della validità. Se gli errori di misurazione sono troppo\ngrandi, dati sono inutili. Inoltre, uno strumento di misurazione può\nessere preciso ma non valido. La validità e l’attendibilità delle\nmisurazioni sono dunque entrambe necessarie.generale, l’attendibilità e la validità delle misure devono essere\nvalutate per capire se dati raccolti da un ricercatore siano adeguati\n(1) per fornire una risposta alla domanda della ricerca, e (2) per\ngiungere alla conclusione proposta dal ricercatore alla luce dei\nrisultati dell’analisi statistica che è stata eseguita. È chiaro che le\ninformazioni fornite questo capitolo si limitano scalfire la\nsuperficie di questi problemi. concetti qui introdotti, però, devono\nsempre essere tenuti mente e costituiscono il fondamento di quanto\nverrà esposto nei capitoli successivi.","code":""},{"path":"ch:freq-distr.html","id":"ch:freq-distr","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"Capitolo 3 Variabili e distribuzioni di frequenza","text":"Le analisi esplorative dei dati e la statistica descrittiva costituiscono la prima fase dell’analisi dei dati psicologici. Consentono di capire come dati sono distribuiti, ci aiutano ad individuare le osservazioni anomale e gli errori di tabulazione. Consentono di riassumere le distribuzioni dei dati mediante indici sintetici. Consentono di visualizzare e di studiare le relazioni tra le variabili. questo Capitolo, dopo avere presentato gli obiettivi dell’analisi esplorative dei dati, discuteremo il problema della descrizione numerica e della rappresentazione grafica delle distribuzioni di frequenza.","code":""},{"path":"ch:freq-distr.html","id":"chapter-descript","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.1 Introduzione all’esplorazione dei dati","text":"Le analisi esplorative dei dati sono indispensabili per condurre modo corretto una qualsiasi analisi statistica, dal livello base quello avanzato. Si parla di analisi descrittiva se l’obiettivo è quello di descrivere le caratteristiche di un campione. Si parla di analisi esplorativa dei dati (Exploratory Data Analysis o EDA) se l’obiettivo è quello di esplorare dati alla ricerca di nuove informazioni e relazioni tra variabili. Questa distinzione, seppur importante livello teorico, nella pratica è più fumosa perché spesso entrambe le situazioni si verificano contemporaneamente nella stessa indagine statistica e le metodologie di analisi che si utilizzano sono molto simili.Né il calcolo delle statistiche descrittive né l’analisi esplorativa dei dati possono essere condotte senza utilizzare un software. Le descrizioni dei concetti di base della EDA saranno dunque fornite di pari passo alla spiegazione di come le quantità discusse possono essere calcolate pratica utilizzando .","code":""},{"path":"ch:freq-distr.html","id":"un-excursus-storico","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.2 Un excursus storico","text":"Nel 1907 Francis Galton, cugino di Charles Darwin, matematico e\nstatistico autodidatta, geografo, esploratore, teorico della\ndattiloscopia (ovvero, dell’uso delle impronte digitali fini\nidentificativi) e dell’eugenetica, scrisse una lettera alla rivista\nscientifica Nature sulla sua visita alla Fat Stock Poultry\nExhibition di Plymouth. Lì vide alcuni membri del pubblico partecipare\nad un gioco il cui scopo era quello di indovinare il peso della carcassa\ndi un grande bue che era appena stato scuoiato. Galton si procurò 787\ndei biglietti che erano stati compilati dal pubblico e considerò il\nvalore medio di 547 kg come la “scelta democratica” dei partecipanti, \nquanto “ogni altra stima era stata giudicata troppo alta o troppo bassa\ndalla maggioranza dei votanti”. Il punto interessante è che il peso\ncorretto di 543 kg si dimostrò essere molto simile alla “scelta\ndemocratica” basata sulle stime dei 787 partecipanti. Galton intitolò la\nsua lettera Nature Vox Populi (voce del popolo), ma questo processo\ndecisionale è ora meglio conosciuto come la “saggezza delle folle”\n(wisdom crowds). Possiamo dire che, nel suo articolo del 1907,\nGalton effettuò quello che ora chiamiamo un riepilogo dei dati, ovvero\ncalcolò un indice sintetico partire da un insieme di dati. questo\ncapitolo esamineremo le tecniche che sono state sviluppate nel secolo\nsuccessivo per riassumere le grandi masse di dati con cui sempre più\nspesso ci dobbiamo confrontare. Vedremo come calcolare e interpretare\ngli indici di posizione e di dispersione, discuteremo le distribuzioni\ndi frequenze e le relazioni tra variabili. Vedremo inoltre quali sono le\ntecniche di visualizzazione che ci consentono di rappresentare questi\nsommari dei dati mediante dei grafici. Ma prima di entrare nei dettagli, prendiamoci un momento per capire perché abbiamo bisogno della statistica e, per ciò che stiamo discutendo qui, della statistica descrittiva.generale, che cos’è la statistica? Ci sono molte definizioni. Fondamentalmente, la statistica è un insieme di tecniche che ci consentono di dare un senso al mondo attraverso dati. Ciò avviene tramite il processo di analisi statistica. L’analisi statistica traduce le domande che abbiamo proposito del mondo modelli matematici, utilizza dati per scegliere modelli matematici che sono apppropriati per descrivere il mondo e, infine, applica tali modelli per trovare una risposta alle domande che ci siamo posti. La statistica consente quindi di collegare le nostre domande proposito del mondo ai dati, di utilizzare dati per trovare le risposte alle domande che ci siamo posti e di valutare l’impatto delle risposte che abbiamo trovato.","code":""},{"path":"ch:freq-distr.html","id":"riassumere-i-dati","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.3 Riassumere i dati","text":"Iniziamo porci una domanda. Quando riassumiamo dati, necessariamente buttiamo via delle informazioni; ma è una buona idea procedere questo modo? Non sarebbe meglio conservare le informazioni specifiche di ciascun soggetto che partecipa ad un esperimento psicologico, al di là di ciò che viene trasmesso dagli indici riassuntivi della statistica descrittiva? Che dire delle informazioni che descrivono come sono stati raccolti dati, come l’ora del giorno o l’umore del partecipante? Tutte queste informazioni vengono perdute quando riassumiamo dati. La risposta alla domanda che ci siamo posti è che, generale, non è una buona idea conservare tutti dettagli di ciò che sappiamo. È molto più utile riassumere le informazioni perché la semplificazione risultante consente processi di generalizzazione.un contesto letterario, l’importanza della generalizzazione è stata\nsottolineata da Jorge Luis Borges nel suo racconto “Funes o della\nmemoria”, che descrive un individuo che perde la capacità di\ndimenticare. Borges si concentra sulla relazione tra generalizzazione e\npensiero: “Pensare è dimenticare una differenza, generalizzare, astrarre. Nel mondo troppo pieno di Funes, c’erano solo dettagli.”Come possiamo ben capire, la vita di Funes non è facile. Se facciamo\nriferimento alla psicologia possiamo dire che gli psicologi hanno\nstudiato lungo l’utilità della generalizzazione per il pensiero. Un\nesempio è fornito dal fenomeno della formazione dei concetti e lo\npsicologo che viene mente questo proposito è sicuramente Eleanor\nRosch, la quale ha studiato principi di base della categorizzazione. \nconcetti ci forniscono uno strumento potente per organizzare le\nconoscenze. Noi siamo grado di riconoscere facilmente diversi\nesemplare di un concetto – per esempio, “gli uccelli” – anche se \nsingoli esemplari che fanno parte di una categoria sono molto diversi\ntra loro (l’aquila, il gabbiano, il pettirosso). L’uso dei concetti, cioè\nla generalizzazione, è utile perché ci consente di fare previsioni sulle\nproprietà dei singoli esemplari che appartengono ad una categoria, anche\nse non abbiamo mai avuto esperienza diretta con essi – per esempio,\npossiamo fare la predizione che tutti gli uccelli possono volare e\nmangiare vermi, ma non possono guidare un’automobile o parlare \ninglese. Queste previsioni non sono sempre corrette, ma sono utili.Le statistiche descrittive, un certo senso, ci fornisco l’analogo dei\n“prototipi” che, secondo Eleanor Rosch, stanno alla base del processo\npsicologico di creazione dei concetti. Un prototipo è l’esemplare più\nrappresentativo di una categoria. maniera simile, una statistica\ndescrittiva come la media, ad esempio, potrebbe essere intesa come\nl’osservazione “tipica”.La statistica descrittiva ci fornisce gli strumenti per riassumere \ndati che abbiamo disposizione una forma visiva o numerica. Le\nrappresentazioni grafiche più usate della statistica descrittiva sono\ngli istogrammi, diagrammi dispersione o box-plot, e gli indici\nsintetici più comuni sono la media, la mediana, la varianza e la\ndeviazione standard.","code":""},{"path":"ch:freq-distr.html","id":"i-dati-grezzi","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.4 I dati grezzi","text":"Per introdurre principali strumenti della statistica descrittiva considereremo qui dati raccolti da Zetsche, Bürkner, Renneberg (2019). Questi ricercatori hanno studiato le aspettative negative quale meccanismo chiave nel mantenimento e nella reiterazione della depressione. Nello studio, Zetsche, Bürkner, Renneberg (2019) si sono chiesti se individui depressi maturino delle aspettative accurate sul loro umore futuro, oppure se tali aspettative sono distorte negativamente.3. uno studio viene esaminato un campione costituito da 30 soggetti con almeno un episodio depressivo maggiore e da 37 controlli sani. Gli autori hanno misurato il livello depressivo con il Beck Depression Inventory (BDI-II). Questi sono dati che considereremo qui.Esercizio 3.1  Qual è la la gravità della depressione riportata dai soggetti nel campione esaminato da Zetsche, Bürkner, Renneberg (2019)?Per rispondere questa domanda, iniziamo leggere \\(\\R\\) dati, assumendo che il file data.mood.csv si trovi nella cartella data contenuta nella working directory.C’è un solo valore BDI-II per ciascun soggetto ma tale valore viene ripetuto tante volte quante volte sono le righe del data.frame associate ad ogni soggetto (ciascuna riga corrispondente ad una prova diversa). È dunque necessario trasformare il data.frame modo tale da avere un’unica riga per ciascun soggetto, ovvero un unico valore BDI-II per soggetto.Ci sono dunque 66 soggetti quali hanno ottenuto valori sulla scala del BDI-II stampati di seguito. Per semplicità, li presentiamo ordinati dal più piccolo al più grande.","code":"\nlibrary(\"rio\")\ndf <- rio::import(\n  here(\"data\", \"data.mood.csv\"),\n  header = TRUE\n)\nbysubj <- df %>%\n  group_by(esm_id) %>%\n  summarise(\n    bdi = mean(bdi)\n  ) %>%\n  na.omit()\nsort(bysubj$bdi)\n#>  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n#> [18]  1  1  1  1  1  1  1  1  2  2  2  2  3  3  3  5  7\n#> [35]  9 12 19 22 22 24 25 25 26 26 26 27 27 28 28 30 30\n#> [52] 30 31 31 33 33 34 35 35 35 36 39 41 43 43 44"},{"path":"ch:freq-distr.html","id":"distribuzioni-di-frequenze","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.5 Distribuzioni di frequenze","text":"È chiaro che dati grezzi sono di difficile lettura. Poniamoci dunque il problema di creare una rappresentazione sintetica e comprensibile di questo insieme di valori. Uno dei modi che ci consentono di effettuare una sintesi dei dati è quello di generare una distribuzione di frequenze.Definizione 3.1  Una distribuzione di frequenze è un riepilogo del conteggio della frequenza con cui le modalità osservate un insieme di dati si verificano un intervallo di valori.Per creare una distribuzione di frequenze possiamo procedere effettuando una partizione delle modalità della variabile di interesse \\(m\\) classi (denotate con \\(\\Delta_i\\)) tra loro disgiunte. tale partizione, la classe \\(\\)-esima coincide con un intervallo di valori aperto destra \\([a_i, b_i)\\) o aperto sinistra \\((a_i, b_i]\\). Ad ogni classe \\(\\Delta_i\\) avente \\(a_i\\) e \\(b_i\\) come limite inferiore e superiore associamo l’ampiezza \\(b_i - a_i\\) (non necessariamente uguale per ogni\nclasse) e il valore centrale \\(\\bar{x}_i\\). La scelta delle classi è arbitraria, ma è buona norma non definire classi con un numero troppo piccolo (< 5) di osservazioni. Poiché ogni elemento dell’insieme \\(\\{x_i\\}_{=1}^n\\) appartiene ad una ed una sola classe \\(\\Delta_i\\), possiamo calcolare le quantità elencate di seguito.La frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).\nProprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).\nLa frequenza assoluta \\(n_i\\) di ciascuna classe, ovvero il numero di osservazioni che ricadono nella classe \\(\\Delta_i\\).Proprietà: \\(n_1 + n_2 + \\dots + n_m = n\\).La frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe.\nProprietà: \\(f_1+f_2+\\dots+f_m =1\\).\nLa frequenza relativa \\(f_i = n_i/n\\) di ciascuna classe.Proprietà: \\(f_1+f_2+\\dots+f_m =1\\).La frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(\\)-esima compresa: \\(N_i = \\sum_{=1}^m n_i.\\)La frequenza cumulata \\(N_i\\), ovvero il numero totale delle osservazioni che ricadono nelle classi fino alla \\(\\)-esima compresa: \\(N_i = \\sum_{=1}^m n_i.\\)La frequenza cumulata relativa \\(F_i\\), ovvero\n\\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{=1}^m f_i.\\)La frequenza cumulata relativa \\(F_i\\), ovvero\n\\(F_i = f_1+f_2+\\dots+f_m = \\frac{N_i}{n} = \\frac{1}{n} \\sum_{=1}^m f_i.\\)Esercizio 3.2  Si calcoli la distribuzione di frequenza assoluta e la distribuzione di frequenza relativa per valori del BDI-II del campione clinico di Zetsche, Bürkner, Renneberg (2019).Per costruire una distribuzione di frequenza è innanzitutto necessario scegliere gli intervalli delle classi. Facendo riferimento ai cut-usati per l’interpretazione del BDI-II, definiamo seguenti intervalli aperti destra:depressione minima: [0, 13.5),depressione lieve: [13.5, 19.5),depressione moderata: [19.5, 28.5),depressione severa: [28.5, 63).Esaminando dati, possiamo notare che 36 soggetti cadono nella prima classe, uno nella seconda classe, e così via. La distribuzione di frequenza della variabile bdi2 è riportata nella tabella seguente. Questa distribuzione di frequenza ci aiuta capire meglio cosa sta succedendo. Se consideriamo la frequenza relativa, ad esempio, possiamo notare che ci sono due valori maggiormente ricorrenti e tali valori corrispondono alle due classi più estreme. Questo ha senso nel caso presente, quanto il campione esaminato da Zetsche, Bürkner, Renneberg (2019) includeva due gruppi di soggetti: soggetti sani (con valori BDI-II bassi) e soggetti depressi (con valori BDI-II alti).4 una distribuzione di frequenza tali valori tipici vanno sotto il nome di mode della distribuzione.Poniamoci ora il problema di costruire la tabella precedente utilizzando . Usando la funzione cut(), dividiamo il campo di variazione (ovvero, la differenza tra il valore massimo di una distribuzione ed il valore minimo) di una variabile continua x intervalli e codifica ciascun valore x nei termini dell’intervallo cui appartiene. Così facendo otteniamo:\nPossiamo ora usare la funzione table() la quale ritorna un elenco che associa la frequenza assoluta ciascuna modalità della variabile – ovvero, ritorna la distribuzione di frequenza assoluta.\nLa distribuzione di frequenza relativa si ottiene dividendo ciascuna frequenza assoluta per il numero totale di osservazioni:","code":"\nbysubj$bdi_level <- cut(\n  bysubj$bdi,\n  breaks = c(0, 13.5, 19.5, 28.5, 63),\n  include.lowest = TRUE,\n  labels = c(\n    \"minimal\", \"mild\", \"moderate\", \"severe\"\n  )\n)\n\nbysubj$bdi_level\n#>  [1] moderate severe   severe   moderate severe  \n#>  [6] severe   severe   severe   moderate severe  \n#> [11] moderate mild     severe   minimal  minimal \n#> [16] minimal  severe   moderate minimal  minimal \n#> [21] minimal  minimal  minimal  moderate minimal \n#> [26] minimal  minimal  minimal  minimal  minimal \n#> [31] minimal  severe   minimal  minimal  severe  \n#> [36] minimal  moderate minimal  minimal  minimal \n#> [41] severe   minimal  minimal  severe   severe  \n#> [46] moderate severe   severe   minimal  moderate\n#> [51] minimal  moderate severe   moderate moderate\n#> [56] minimal  minimal  minimal  minimal  minimal \n#> [61] minimal  minimal  minimal  minimal  minimal \n#> [66] minimal \n#> Levels: minimal mild moderate severe\ntable(bysubj$bdi_level)\n#> \n#>  minimal     mild moderate   severe \n#>       36        1       12       17\ntable(bysubj$bdi_level) / sum(table(bysubj$bdi_level))\n#> \n#>  minimal     mild moderate   severe \n#>  0.54545  0.01515  0.18182  0.25758"},{"path":"ch:freq-distr.html","id":"istogramma","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.6 Istogramma","text":"dati che sono stati sintetizzati una distribuzione di frequenze\npossono essere rappresentati graficamente un istogramma.\nUn istogramma si costruisce riportando sulle ascisse limiti delle\nclassi \\(\\Delta_i\\) e sulle ordinate valori della funzione costante \ntratti\\[\n\\varphi_n(x)= \\frac{f_i}{b_i-a_i}, \\quad x\\\\Delta_i,\\, =1, \\dots, m\n\\]\nche misura la densità della frequenza relativa della variabile \\(X\\)\nnella classe \\(\\Delta_i\\), ovvero il rapporto fra la frequenza relativa\n\\(f_i\\) e l’ampiezza (\\(b_i - a_i\\)) della classe. questo modo il\nrettangolo dell’istogramma associato alla classe \\(\\Delta_i\\) avrà un’area\nproporzionale alla frequenza relativa \\(f_i\\). Si noti che l’area totale\ndell’istogramma delle frequenze relative è data della somma delle aree\ndei singoli rettangoli e quindi vale 1.0.Esercizio 3.3  Si utilizzi \\(\\R\\) per costruire un istogramma per valori BDI-II riportati da Zetsche, Bürkner, Renneberg (2019).Con quattro intervalli individuati dai cut-del BDI-II otteniamo la\nrappresentazione riportata nella figura 3.1. Per chiarezza, precisiamo che ggplot() utilizza intervalli aperti destra. Nel caso della prima barra dell’istogramma, l’ampiezza dell’intervallo è pari 13.5 e l’area della barra (ovvero, la frequenza relativa) è uguale 36/66. Dunque l’altezza della barra è uguale \\((36 / 66) / 13.5 = 0.040\\). Lo stesso procedimento si applica per il calcolo dell’altezza degli altri rettangoli.\nFIGURA 3.1: Istogramma per valori BDI-II riportati da Zetsche et al. (2019).\nAnche se nel caso presente è sensato usare ampiezze diverse per gli intervalli delle classi, generale gli istogrammi si costruiscono utilizzando intervalli riportati sulle ascisse con un’ampiezza uguale. Questo è il caso dell’istogramma della figura 3.2.\nFIGURA 3.2: Una rappresentazione più comune per l’istogramma dei valori BDI-II nella quale gli intervalli delle classi hanno ampiezze uguali.\n","code":"\nbysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1)\n    # il valore BDI-II massimo è 44\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1)\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\nbysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  scale_x_continuous(\n    breaks = c(0.00, 7.35, 14.70, 22.05, 29.40, 36.75, 44.10)\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequanza\"\n  )"},{"path":"ch:freq-distr.html","id":"kernel-density-plot","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.7 Kernel density plot","text":"Il confronto tra le figure 3.1 e 3.2 rende chiaro il limite dell’istogramma: il profilo dell’istogramma è arbitrario, quanto dipende dal numero e dall’ampiezza delle classi. Questo rende difficile l’interpretazione.Il problema precedente può essere alleviato utilizzando una rappresentazione alternativa della distribuzione di frequenza, ovvero la stima della densità della frequenza dei dati (detta anche stima kernel di densità). Un modo semplice per pensare tale rappresentazione, che inglese va sotto il nome di kernel density plot (cioè grafici basati sulla stima kernel di densità), è quello di immaginare un grande campione di dati, modo che diventi possibile definire un enorme numero di classi di equivalenza di ampiezza molto piccola, le quali non risultino vuote. tali circostanze, la funzione di densità empirica non è altro che il profilo lisciato dell’istogramma. La stessa idea si applica anche quando il campione è piccolo. tali circostanze, invece di raccogliere le osservazioni barre come negli istogrammi, lo stimatore di densità kernel colloca una piccola “gobba” (bump), determinata da un fattore \\(K\\) (kernel) e da un parametro \\(h\\) di smussamento detto ampiezza di banda (bandwidth), corrispondenza di ogni osservazione, quindi somma le gobbe risultanti generando una curva smussata.L’interpretazione che possiamo attribuire al kernel density plot è simile quella che viene assegnata agli istogrammi: l’area sottesa al kernel density plot un certo intervallo rappresenta la proporzione di casi della distribuzione che hanno valori compresi quell’intervallo.Esercizio 3.4  ’istogramma dei valori BDI-II di Zetsche, Bürkner, Renneberg (2019) si sovrapponga un kernel density plot.\nFIGURA 3.3: Kernel density plot e corrispondente istogramma per valori BDI-II.\n","code":"\nbysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  geom_density(\n    aes(x = bdi),\n    adjust = 0.5,\n    size = 0.8,\n    # fill = colors[2],\n    alpha = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )"},{"path":"ch:freq-distr.html","id":"forma-di-una-distribuzione","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.8 Forma di una distribuzione","text":"generale, la forma di una distribuzione descrive come dati si distribuiscono intorno ai valori centrali. Distinguiamo tra distribuzioni simmetriche e asimmetriche, e tra distribuzioni unimodali o multimodali. Un’illustrazione grafica è fornita nella figura 3.4. Nel pannello 1 la distribuzione è unimodale con asimmetria negativa; nel pannello 2 la distribuzione è unimodale con asimmetria positiva; nel pannello 3 la distribuzione è simmetrica e unimodale; nel pannello 4 la distribuzione è bimodale.\nFIGURA 3.4: 1: Asimmetria negativa. 2: Asimmetria positiva. 3: Distribuzione unimodale. 4: Distribuzione bimodale.\nEsercizio 3.5  Il kernel density plot della figura 3.3 indica che la distribuzione dei valori del BDI-II nel campione di Zetsche, Bürkner, Renneberg (2019) è bimodale. Ciò indica che le osservazioni della distribuzione si addensano due cluster ben distinti: un gruppo di osservazioni tende ad avere valori BDI-II bassi, mentre l’altro gruppo tende ad avere BDI-II alti. Questi due cluster di osservazioni corrispondono al gruppo di controllo e al gruppo clinico nel campione di dati esaminato da Zetsche, Bürkner, Renneberg (2019).","code":""},{"path":"ch:freq-distr.html","id":"indici-di-posizione","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.9 Indici di posizione","text":"Nuovamente, se preferite un’introduzione “soft” alla nozione di “tendenza centrale” di una distribuzione statistica, vi rimando nuovamentew al link che ho già suggerito precedenza.","code":""},{"path":"ch:freq-distr.html","id":"quantili","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.9.1 Quantili","text":"La descrizione della distribuzione dei valori BDI-II di\nZetsche, Bürkner, Renneberg (2019) può essere facilitata dalla determinazione di\nalcuni valori caratteristici che sintetizzano le informazioni contenute\nnella distribuzione di frequenze. Si dicono quantili (o frattili)\nquei valori caratteristici che hanno le seguenti proprietà. quartili\nsono quei valori che ripartiscono dati \\(x_i\\) quattro parti\nugualmente numerose (pari ciascuna al 25% del totale). Il primo\nquartile, \\(q_1\\), lascia alla sua sinistra il 25% del campione pensato\ncome una fila ordinata (destra quindi il 75%). Il secondo quartile\n\\(q_2\\) lascia sinistra il 50% del campione (destra quindi il 50%).\nEsso viene anche chiamato mediana. Il terzo quartile lascia sinistra\nil 75% del campione (destra quindi il 25%). Secondo lo stesso\ncriterio, si dicono decili quantili di ordine \\(p\\) multiplo di 0.10 e\npercentili quantili di ordine \\(p\\) multiplo di 0.01.Come si calcolano quantili? Consideriamo la definizione di quantile non interpolato di ordine \\(p\\) \\((0 < p < 1)\\). Si procede innanzitutto\nordinando dati ordine crescente, \\(\\{x_1, x_2, \\dots, x_n\\}\\). Ci\nsono poi due possibilità. Se il valore \\(np\\) non è intero, sia \\(k\\)\nl’intero tale che \\(k < np < k + 1\\) – ovvero, la parte intera di \\(np\\).\nAllora \\(q_p = x_{k+1}.\\) Se \\(np = k\\) con \\(k\\) intero, allora\n\\(q_p = \\frac{1}{2}(x_{k} + x_{k+1}).\\) Se vogliamo calcolare il primo\nquartile \\(q_1\\), ad esempio, utilizziamo \\(p = 0.25\\). Dovendo calcolare\ngli altri quantili basta sostituire \\(p\\) il valore appropriato.Gli indici di posizione, tra le altre cose, hanno un ruolo importante,\novvero vengono utilizzati per creare una rappresentazione grafica di una\ndistribuzione di valori che è molto popolare e può essere usata \nalternativa ad un istogramma (realtà vedremo poi come possa essere\ncombinata con un istogramma). Tale rappresentazione va sotto il nome di\nbox-plot.Esercizio 3.6  Per fare un esempio, consideriamo nove soggetti del campione clinico di Zetsche, Bürkner, Renneberg (2019) che hanno riportato un unico episodio di depressione maggiore. Per tali soggetti valori ordinati del BDI-II (per semplicità li chiameremo \\(x\\)) sono seguenti: 19, 26, 27, 28, 28, 33, 33, 41, 43.\nPer il calcolo del secondo quartile (non interpolato), ovvero per il calcolo della mediana, dobbiamo considerare la quantità \\(np = 9 \\cdot 0.5 = 4.5\\), non intero. Quindi, \\(q_1 = x_{4 + 1} = 27\\).\nPer il calcolo del quantile (non interpolato) di ordine \\(p = 2/3\\) dobbiamo considerare la quantità \\(np = 9 \\cdot 2/3 = 6\\), intero. Quindi, \\(q_{\\frac{2}{3}} = \\frac{1}{2} (x_{6} + x_{7}) = \\frac{1}{2} (33 + 33) = 33\\).","code":""},{"path":"ch:freq-distr.html","id":"diagramma-a-scatola","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.9.2 Diagramma a scatola","text":"Il diagramma scatola (o box plot) è uno strumento grafico utile al\nfine di ottenere informazioni circa la dispersione e l’eventuale\nsimmetria o asimmetria di una distribuzione. Per costruire un box-plot\nsi rappresenta sul piano cartesiano un rettangolo (cioè la “scatola”) di\naltezza arbitraria la cui base corrisponde alla dist intanza\ninterquartile (IQR = \\(q_{0.75} - q_{0.25}\\)). La linea interna alla\nscatola rappresenta la mediana \\(q_{0.5}\\). Si tracciano poi ai lati della\nscatola due segmenti di retta cui estremi sono detti “valore\nadiacente” inferiore e superiore. Il valore adiacente inferiore è il\nvalore più piccolo tra le osservazioni che risulta maggiore o uguale al\nprimo quartile meno la distanza corrispondente 1.5 volte la distanza\ninterquartile. Il valore adiacente superiore è il valore più grande tra le osservazioni che risulta minore o uguale \\(Q_3+1.5\\) IQR. valori esterni ai valori adiacenti (chiamati valori anomali) vengono rappresentati individualmente nel box-plot per meglio evidenziarne la presenza e la posizione.\nFIGURA 3.5: Box-plot: \\(M\\) è la mediana, \\(\\bar{x}\\) è la media aritmetica e IQR è la distanza interquartile (\\(Q_3 - Q_1\\)).\nEsercizio 3.7  Per dati di Zetsche, Bürkner, Renneberg (2019), si utilizzi un box-plot per rappresentare graficamente la distribuzione dei punteggi BDI-II nel gruppo dei pazienti e nel gruppo di controllo.Nella figura 3.6 sinistra sono rappresentati dati grezzi. La linea curva che circonda (simmetricamente) le osservazioni è l’istogramma lisciato (kernel density plot) che abbiamo descritto precedenza. Nella figura 3.6 destra sono rappresentanti gli stessi dati: il kernel density plot è lo stesso di prima, ma al suo interno è stato collocato un box-plot. Entrambe le rappresentazioni suggeriscono che la distribuzione dei dati è ’incirca simmetrica nel gruppo clinico. Il gruppo di controllo mostra invece un’asimmetria positiva.\nFIGURA 3.6: Due versioni di un violin plot per valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019).\n","code":"\nbysubj <- df %>%\n  group_by(esm_id, group) %>%\n  summarise(\n    bdi = mean(bdi),\n    nr_of_episodes = mean(nr_of_episodes, na.rm = TRUE)\n  ) %>%\n  na.omit() %>%\n  ungroup()\n\nbysubj$group <- forcats::fct_recode(\n  bysubj$group,\n  \"Controlli\\n sani\" = \"ctl\",\n  \"Depressione\\n maggiore\" = \"mdd\"\n)\n\np1 <- bysubj %>%\n  ggplot(aes(x = group, y = bdi)) +\n  geom_violin(trim = FALSE) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.7) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\"\n  )\np2 <- bysubj %>%\n  ggplot(aes(x = group, y = bdi)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.05) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\"\n  )\np1 + p2"},{"path":"ch:freq-distr.html","id":"sina-plot","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.9.3 Sina plot","text":"Si noti che box plot non sono necessariamente la rappresentazione migliore della distribuzione di una variabile. Infatti, richiedono la comprensione di concetti complessi (quali quantili e la differenza interquantile) che non sono necessari se vogliamo presentare maniera grafica la distribuzione della variabile e, generale, non sono compresi da un pubblico di non specialisti. Inoltre, box plot nascondono informazioni che di solito sono cruciali da vedere. È dunque preferibile presentare direttamente dati.Nella figura 3.7 viene presentato un cosiddetto “sina plot”. tale rappresentazione grafica vengono mostrate le singole osservazioni divise classi. Ai punti viene aggiunto un jitter, così da evitare sovrapposizioni. L’ampiezza del jitter lungo l’asse \\(x\\) è determinata dalla distribuzione della densità dei dati ’interno di ciascuna classe; quindi il grafico mostra lo stesso contorno di un violin plot, ma trasmette informazioni sia sul numero di punti dati, sia sulla distribuzione della densità, sui valori anomali e sulla distribuzione dei dati un formato molto semplice, comprensibile e sintetico.Esercizio 3.8  Si generi un sina plot per dati della figura 3.6. Si aggiunga alla figura una rappresentazione della mediana.\nFIGURA 3.7: Sina plot per valori BDI-II di ciascuno dei due gruppi di soggetti esaminati da Zetsche et al. (2019) con l’indicazione della mediana per ciascun gruppo.\nPer un esempio una recente pubblicazione, possiamo considerare le figure 3 e 6 di Lazic, Semenova, Williams (2020).","code":"\nzetsche_summary <- bysubj %>%\n  group_by(group) %>%\n  summarize(\n    bdi_mean = mean(bdi),\n    bdi_sd = sd(bdi),\n    bdi_median = median(bdi)\n  ) %>%\n  ungroup()\n\nbysubj %>%\n  ggplot(\n    aes(x = group, y = bdi, color = group)\n  ) +\n  ggforce::geom_sina(aes(color = group, size = 3, alpha = .5)) +\n  geom_errorbar(\n    aes(y = bdi_median, ymin = bdi_median, ymax = bdi_median),\n    data = zetsche_summary, width = 0.3, size = 3\n  ) +\n  labs(\n    x = \"\",\n    y = \"BDI-II\",\n    color = \"Gruppo\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_colour_grey(start = 0.7, end = 0)"},{"path":"ch:freq-distr.html","id":"leccellenza-grafica","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"3.9.4 L’eccellenza grafica","text":"Non c’è un unico modo “corretto” per la rappresentazione grafica dei dati. Ciascuno dei grafici che abbiamo discusso precedenza ha suoi pregi e suoi difetti. Un ricercatore che ha molto influenzato il modo cui viene realizzata la visualizzazione dei dati scientifici è Edward Tufte, soprannominato dal New York Times il “Leonardo da Vinci dei dati.” Secondo Tufte, “l’eccellenza nella grafica consiste nel comunicare idee complesse modo chiaro, preciso ed efficiente”. Nella visualizzazione delle informazioni, l’“eccellenza grafica” ha l’obiettivo di comunicare al lettore il maggior numero di idee nella maniera più diretta e semplice possibile. Secondo Tufte (2001), le rappresentazioni grafiche dovrebbero:mostrare dati;indurre l’osservatore riflettere sulla sostanza piuttosto che\nsulla progettazione grafica, o qualcos’altro;evitare di distorcere quanto dati stanno comunicando (“integrità\ngrafica”);presentare molte informazioni forma succinta;rivelare la coerenza tra le molte dimensioni dei dati;incoraggiare l’osservatore confrontare differenti sottoinsiemi di dati;rivelare dati diversi livelli di dettaglio, da una visione ampia\nalla struttura di base;servire ad uno scopo preciso (descrizione, esplorazione, o la\nrisposta qualche domanda);essere fortemente integrate con le descrizioni statistiche e verbali\ndei dati fornite nel testo.base questi principi, figura 3.7 sembra fornire la\nrappresentazione migliore dei dati di Zetsche, Bürkner, Renneberg (2019). Il seguente link fornisce diverse interessanti illustrazioni dei principi elencati sopra.","code":""},{"path":"ch:freq-distr.html","id":"commenti-e-considerazioni-finali-1","chapter":"Capitolo 3 Variabili e distribuzioni di frequenza","heading":"Commenti e considerazioni finali","text":"Una distribuzione è una rappresentazione del modo cui le diverse modalità di una variabile si distribuiscono nelle unità statistiche che compongono il campione o la popolazione oggetto di studio. Il modo più diretto per trasmettere descrivere le proprietà della distribuzione di una variabile discreta è quello di fornire una rappresentazione grafica della distribuzione di frequenza. seguito vedremo la corrispondente rappresentazione che viene usata nel caso delle variabili continue.","code":""},{"path":"ch:loc-scale.html","id":"ch:loc-scale","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"Capitolo 4 Indici di posizione e di scala","text":"L’analisi grafica, esaminata precedenza, costituisce la base di partenza di qualsivoglia analisi quantitativa dei dati. Tramite opportune rappresentazioni grafiche possiamo individuare alcune caratteristiche importanti di una distribuzione: per esempio, è possibile capire se la distribuzione è simmetrica o asimmetrica; oppure se è unimodale o multimodale. Successivamente, possiamo calcolare degli indici numerici che descrivono modo sintetico le caratteristiche di base dei dati esaminati.","code":""},{"path":"ch:loc-scale.html","id":"indici-di-tendenza-centrale","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.1 Indici di tendenza centrale","text":"Tra le misure di tendenza centrale, ovvero tra gli indici che forniscono un’idea dei valori attorno ai quali sono prevalentemente concentrati dati di un campione, quella più comunemente usata è la media.","code":""},{"path":"ch:loc-scale.html","id":"media","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.1.1 Media","text":"Tutti conosciamo la media aritmetica di \\(\\{x_1, x_2, \\dots, x_n\\}\\), ovvero il numero reale \\(\\bar{x}\\) definito da\\[\\begin{equation}\n\\bar{x}=\\frac{1}{n}\\sum_{=1}^n x_i.\n\\tag{4.1}\n\\end{equation}\\]Nella (4.1) abbiamo usato la notazione delle sommatorie per descrivere una somma di valori. Questa notazione è molto usata statistica e viene descritta Appendice.La media gode della seguente importante proprietà: la somma degli scarti tra ciascuna modalità \\(x_i\\) e la media aritmetica \\(\\bar{x}\\) è nulla, cioè\\[\\begin{equation}\n\\sum_{=1}^n (x_i - \\bar{x}) = 0.\\notag\n\\label{eq:diffmeansumzero}\n\\end{equation}\\]Infatti,\\[\n\\begin{aligned}\n\\sum_{=1}^n (x_i - \\bar{x}) &= \\sum_i x_i - \\sum_i \\bar{x}\\notag\\\\\n&= \\sum_i x_i - n \\bar{x}\\notag\\\\\n&= \\sum_i x_i - \\sum_i x_i = 0.\\notag\n\\end{aligned}\n\\]Ciò ci consente di pensare alla media come al baricentro della distribuzione.Un’altra proprietà della media è la seguente. La somma dei quadrati degli scarti tra ciascuna modalità \\(x_i\\) e una costante arbitraria \\(\\), cioè\\[\\begin{equation}\n\\varphi() = \\sum_{=1}^n (x_i - )^2,\\notag\n\\end{equation}\\]è minima per \\(= \\bar{x}\\).Remark. Il concetto statistico di media ha suscitato molte battute. Per esempio,\nil fatto che, media, ciascuno di noi ha un numero di gambe circa pari\n1.9999999. Oppure, il fatto che, media, ciascuno di noi ha un\ntesticolo. Ma la media ha altri problemi, oltre al fatto di ispirare\nbattute simili alle precedenti. particolare, dobbiamo notare che la\nmedia non è sempre l’indice che meglio rappresenta la tendenza centrale\ndi una distribuzione. particolare, ciò non accade quando la\ndistribuzione è asimmetrica, o presenza di valori anomali (outlier)\n– si veda il pannello di destra della figura 3.6. tali circostanze, la tendenza centrale della distribuzione è meglio rappresentata dalla mediana o dalla media spuntata.","code":""},{"path":"ch:loc-scale.html","id":"media-spuntata","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.1.2 Media spuntata","text":"La media spuntata \\(\\bar{x}_t\\) (trimmed mean) non è altro che la\nmedia dei dati calcolata considerando solo il 90% (o altra percentuale)\ndei dati centrali. Per calcolare \\(\\bar{x}_t\\) si ordinando dati secondo\nuna sequenza crescente, \\(x_1 \\leq x_2 \\leq x_3 \\leq \\dots \\leq x_n\\), per\npoi eliminare il primo 5% e l’ultimo 5% dei dati della serie così\nordinata. La media spuntata è data dalla media aritmetica dei dati rimanenti.","code":""},{"path":"ch:loc-scale.html","id":"moda-e-mediana","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.1.3 Moda e mediana","text":"precedenza abbiamo già incontrato altri due popolari indici di\ntendenza centrale: la moda (Mo), ovvero il valore centrale della\nclasse con la frequenza massima (può succedere che una distribuzione\nabbia più mode; tal caso si dice multimodale e questo operatore\nperde il suo significato di indice di tendenza centrale) e la mediana\n\\(\\tilde{x}\\).Remark. Si noti che solitamente software restituiscono un valore interpolato del \\(p\\)-esimo quantile \\(q_p\\) \\((0 < p < 1)\\), il quale viene calcolato mediante specifiche procedure. Il risultato fornito dai software, dunque, non sarà identico quello trovato utilizzando la definizione non interpolata di quantile che abbiamo presentato qui. Se, per qualche ragione, vogliamo conoscere l’algoritmo usato per la determinazione dei quantili interpolati, dobbiamo leggere la documentazione del software.","code":""},{"path":"ch:loc-scale.html","id":"indici-di-dispersione","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2 Indici di dispersione","text":"Le medie e gli indici di posizione descritti precedenza forniscono\ndelle sintesi dei dati che mettono evidenza la tendenza centrale\ndelle osservazioni. Tali indici, tuttavia, non considerano un aspetto\nimportante della distribuzione dei dati, ovvero la variabilità dei\nvalori numerici della variabile statistica. È dunque necessario\nsintetizzare la distribuzione di una variabile statistica oltre che con\nle misure di posizione anche tramite l’utilizzo di indicatori che\nvalutino la dispersione delle unità statistice.Remark. Un’introduzione “soft” al tema degli indici di posizione è fornita nel seguente link.","code":""},{"path":"ch:loc-scale.html","id":"indici-basati-sullordinamento-dei-dati","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.1 Indici basati sull’ordinamento dei dati","text":"È possibile calcolare degli indici di variabilità basati\nsull’ordinamento dei dati. L’indice più ovvio è l’intervallo di\nvariazione, ovvero la distanza tra il valore massimo e il valore minimo\ndi una distribuzione di modalità, mentre precedenza abbiamo già\nincontrato la differenza interquartile. Questi due indici, però, hanno\nil limite di essere calcolati sulla base di due soli valori della\ndistribuzione (\\(x_{\\text{max}}\\) e \\(x_{\\text{mini}}\\), oppure \\(x_{0.25}\\) e\n\\(x_{0.75}\\)). Pertanto non utilizzano tutte le informazioni che sono\ndisponibili. Inoltre, l’intervallo di variazione ha il limite di essere\npesantemente influenzato dalla presenza di valori anomali.","code":""},{"path":"ch:loc-scale.html","id":"varianza","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.2 Varianza","text":"Dati limiti delle statistiche precedenti è più comune misurare la variabilità di una variabile statistica come la dispersione dei dati attorno ad un indice di tendenza centrale. Infatti, la misura di variabilità di gran lunga più usata per valutare la variabilità di una variabile statistica è senza dubbio la varianza. La varianza\\[\\begin{equation}\ns^2 = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\n\\tag{4.2}\n\\end{equation}\\]è la media dei quadrati degli scarti \\(x_i - \\bar{x}\\) tra ogni valore e la media della distribuzione. La varianza è una misura di dispersione più complessa di quelle esaminate precedenza. È appropriata solo nel caso di distribuzioni simmetriche e, anch’essa, è fortemente influenzata dai valori anomali. Inoltre, è espressa un’unità di misura che è il quadrato dell’unità di misura dei dati originari e quindi ad essa non può essere assegnata un’interpretazione intuitiva.Esercizio 4.1  Si calcoli la varianza dei valori BDI-II per dati di Zetsche, Bürkner, Renneberg (2019).Applicando la formula precedente, per tutto il campione abbiamo","code":"\nvar(bysubj$bdi)\n#> [1] 241.9"},{"path":"ch:loc-scale.html","id":"precisione","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.3 Precisione","text":"Si definisce precisione l’inverso della varianza:\\[\\begin{equation}\n\\tau = \\frac{1}{\\sigma^2}.\n\\tag{4.3}\n\\end{equation}\\]Alcuni ritengono che la precisione sia più “intuitiva” della varianza perché dice quanto sono concentrati valori attorno alla media piuttosto che quanto sono dispersi. altri termini, si potrebbe argomentare che siamo più interessati quanto sia precisa una misurazione piuttosto che quanto sia imprecisa. Più sono dispersi valori attorno alla media (alta varianza), meno sono precisi (poca precisione); minore è la varianza, maggiore è la precisione.La precisione è uno dei due parametri naturali della distribuzione gaussiana. Nei termini della (4.3), la distribuzione gaussiana (si veda il Capitolo ??) può essere espressa nel modo seguente\\[\n{\\displaystyle f(y)=\\sqrt{\\frac{\\tau}{2\\pi}} e^{-{\\frac {1}{2}}\\tau\\left({y-\\mu }\\right)^{2}}},\n\\]\nanziché come\\[\n{\\displaystyle f(y)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {y-\\mu }{\\sigma }}\\right)^{2}}}.\n\\]","code":""},{"path":"ch:loc-scale.html","id":"scarto-tipo","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.4 Scarto tipo","text":"Per le ragioni espresse sopra, la misura più usata della dispersione di una distribuzione di dati è lo scarto quadratico medio (o scarto tipo, o deviazione standard), ovvero la radice quadrata della varianza5. differenza della varianza, dunque, lo scarto tipo è espresso nella stessa unità di misura dei dati. Come nel caso della varianza, anche lo scarto tipo \\(s\\) dovrebbe essere usato soltanto quando la media è adeguata per misurare il centro della distribuzione, ovvero, nel caso di distribuzioni simmetriche. Come nel caso della media \\(\\bar{x}\\), anche lo scarto tipo è fortemente influenzato dai dati anomali (outlier), ovvero dalla presenza di uno o di pochi dati che sono molto più distanti dalla media rispetto agli altri valori della distribuzione. Quando tutte le osservazioni sono uguali, \\(s = 0\\), altrimenti \\(s > 0\\).Allo scarto tipo può essere assegnata una semplice interpretazione: lo scarto tipo è simile (ma non identico) allo scarto semplice medio campionario, ovvero alla media aritmetica dei valori assoluti degli scarti dalla media. Lo scarto tipo ci dice, dunque, quanto sono distanti, media, le singole osservazioni dal centro della distribuzione. Un’interpretazione più precisa del significato dello scarto tipo è fornita nel Paragrafo successivo.Esercizio 4.2  Si calcoli lo scarto tipo per valori BDI-II di dati di Zetsche, Bürkner, Renneberg (2019).Applicando la formula precedente, per tutto il campione abbiamo","code":"\nsd(bysubj$bdi)\n#> [1] 15.55"},{"path":"ch:loc-scale.html","id":"deviazione-mediana-assoluta","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.5 Deviazione mediana assoluta","text":"Una misura robusta della dispersione statistica di un campione è la deviazione mediana assoluta (Median Absolute Deviation, MAD) definita come la mediana del valore assoluto delle deviazioni dei dati dalla mediana, ovvero:\\[\n{\\displaystyle \\operatorname {MAD} =\\operatorname {median} \\left(\\ \\left|X_{}-\\operatorname {median} (X)\\right|\\ \\right)}\n\\]\nNel caso di una distribuzione dei dati unimodale simmetrica di forma campanulare (ovvero, normale) si ha che\\[\n{\\displaystyle \\text{deviazione standard} \\approx 1.4826\\ \\operatorname {MAD} .\\,}\n\\]\nPertanto, solitamente software restituiscono il valore MAD moltiplicato per una tale costante.Esercizio 4.3  Si calcoli il valore MAD per valori BDI-II riportati da Zetsche, Bürkner, Renneberg (2019).Applicando la formula precedente, per tutto il campione abbiamo","code":"\n1.4826 * median(abs(bysubj$bdi - median(bysubj$bdi)))\n#> [1] 15.57"},{"path":"ch:loc-scale.html","id":"indici-di-variabilità-relativi","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"4.2.6 Indici di variabilità relativi","text":"volte può essere interessante effettuare un confronto fra due misure di variabilità di grandezze incommensurabili, ovvero di caratteri rilevati mediante differenti unità di misura. questi casi, le misure di variabilità precedentemente descritte si rivelano inadeguate quanto dipendono dall’unità di misura adottata. Diventa dunque necessario ricorrere particolari numeri adimensionali detti indici relativi di variabilità. Il più importante di tali indici è il coefficiente di variazione, ovvero il numero puro\\[\nC_v = \\frac{\\sigma}{\\bar{x}}\n\\]\nottenuto dal rapporto tra la deviazione standard e la media dei dati. Un altro indice relativo di variabilità è la differenza interquartile rapportata al primo quartile oppure al terzo quartile oppure alla mediana, cioè:\\[\n\\frac{x_{0.75} - x_{0.25}}{x_{0.25}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.75}}, \\qquad \\frac{x_{0.75} - x_{0.25}}{x_{0.50}}.\n\\]","code":""},{"path":"ch:loc-scale.html","id":"commenti-e-considerazioni-finali-2","chapter":"Capitolo 4 Indici di posizione e di scala","heading":"Commenti e considerazioni finali","text":"Le statistiche descrittive ci forniscono degli indici sintetici che riassumono dati, ovvero le nostre misurazioni dell’intera popolazione o di un campione estratto da una popolazione. Le statistiche descrittive comprendono gli indici di tendenza centrale e gli indici di dispersione. Gli indici di tendenza centrale includono la media, la mediana e la moda, mentre gli indici di dispersione includono lo scarto tipo, la varianza, la curtosi e l’asimmetria.","code":""},{"path":"ch:correlation.html","id":"ch:correlation","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"Capitolo 5 Le relazioni tra variabili","text":"Nella loro ricerca, Zetsche, Bürkner, Renneberg (2019) hanno misurato il livello di depressione dei soggetti utilizzando due scale psicometriche: il Beck Depression Inventory II (BDI-II) e la Center Epidemiologic Studies Depression Scale (CES-D). Il BDI-II è uno strumento self-report che valutare la presenza e l’intensità di sintomi depressivi pazienti adulti e adolescenti di almeno 13 anni di età con diagnosi psichiatrica\nmentre la CES-D è una scala self-report progettata per misurare \nsintomi depressivi che sono stati vissuti nella settimana precedente\nnella popolazione generale, specialmente quella degli\nadolescenti/giovani adulti. Una domanda ovvia che ci può venire \nmente è: quanto sono simili le misure ottenute mediante queste due\nscale?È chiaro che numeri prodotti dalle scale BDI-II e CES-D non possono\nessere identici, e questo per due motivi: (1) la presenza degli errori\ndi misurazione e (2) l’unità di misura delle due variabili. L’errore di\nmisurazione corrompe sempre, almeno parte, qualunque operazione di\nmisurazione. E questo è vero specialmente psicologia dove\nl’attendibilità degli strumenti di misurazione è minore che altre\ndiscipline (quali la fisica, ad esempio). Il secondo motivo per cui \nvalori delle scale BDI-II e CES-D non possono essere uguali è che\nl’unità di misura delle due scale è arbitraria. Infatti, qual è l’unità\ndi misura della depressione? Chi può dirlo! Ma, al di là delle\ndifferenze derivanti dall’errore di misurazione e dalla differente unità\ndi misura, ci aspettiamo che, se le due scale misurano entrambe lo\nstesso costrutto, allora valori prodotti dalle due scale dovranno\nessere tra loro linearmente associati. Per capire cosa si intende con\n“associazione lineare” iniziamo guardare dati. Per fare questo\nutilizziamo una rappresentazione grafica che va sotto il nome di\ndiagramma dispersione.","code":""},{"path":"ch:correlation.html","id":"diagramma-a-dispersione","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.0.1 Diagramma a dispersione","text":"Il diagramma di dispersione è la rappresentazione grafica delle coppie di punti individuati da due variabili \\(X\\) e \\(Y\\).Per fare un esempio concreto, consideriamo le variabili BDI-II e CES-D di Zetsche, Bürkner, Renneberg (2019). Il diagramma di dispersione per tali variabili si ottiene ponendo, ad esempio, valori BDI-II sull’asse delle ascisse e quelli del CES-D sull’asse delle ordinate. tale grafico, fornito dalla figura 5.1, cascun punto corrisponde ad un individuo del quale, nel caso presente, conosciamo il livello di depressione misurato dalle due scale psicometriche.Dalla figura 5.1 possiamo vedere che dati mostrano una tendenza disporsi attorno ad una retta – nel gergo statistico, questo fatto viene espresso dicendo che punteggi CES-D tendono ad essere linearmente associati ai punteggi BDI-II. È ovvio, tuttavia, che tale relazione lineare è lungi dall’essere perfetta – se fosse perfetta, tutti punti del diagramma dispersione si disporrebbero esattamente lungo una retta.\nFIGURA 5.1: Associazione tra le variabili BDI-II e CES-D nello studio di Zetsche et al. (2019). grigio sono rappresentate le osservazioni del gruppo di controllo; nero quelle dei pazienti.\n","code":"\nbysubj <- df %>%\n  group_by(esm_id, group) %>%\n  summarise(\n    bdi = mean(bdi),\n    cesd = mean(cesd_sum)\n  ) %>%\n  na.omit() %>%\n  ungroup()\nm_cesd <- bysubj %>%\n  dplyr::pull(cesd) %>%\n  mean()\nm_bdi <- bysubj %>%\n  dplyr::pull(bdi) %>%\n  mean()\nFONT_SIZE <- 9\nbysubj %>%\n  ggplot(\n    aes(x = bdi, y = cesd, color = group)\n  ) +\n  geom_point(size = 3, alpha = .9) +\n  geom_hline(\n    yintercept = m_cesd, linetype = \"dashed\",\n    color = \"gray\"\n  ) +\n  geom_vline(\n    xintercept = m_bdi, linetype = \"dashed\",\n    color = \"gray\"\n  ) +\n  geom_text(\n    x = -1, y = 16, label = \"I\", color = \"gray\",\n    size = FONT_SIZE\n  ) +\n  geom_text(\n    x = 0, y = 46, label = \"IV\", color = \"gray\",\n    size = FONT_SIZE\n  ) +\n  geom_text(\n    x = 18, y = 46, label = \"III\", color = \"gray\",\n    size = FONT_SIZE\n  ) +\n  geom_text(\n    x = 18, y = 16, label = \"II\", color = \"gray\",\n    size = FONT_SIZE\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"CESD\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_colour_grey(start = 0.7, end = 0)"},{"path":"ch:correlation.html","id":"covarianza","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.0.2 Covarianza","text":"Il problema che ci poniamo ora è quello di trovare un indice numerico che\ndescrive di quanto la nube di punti si discosta da una perfetta\nrelazione lineare tra le due variabili, ovvero che descrive la direzione e la\nforza della relazione lineare tra le due variabili. Ci sono vari indici\nstatistici che possono essere utilizzati questo scopo.Iniziamo considerare il più importante di tali indici, chiamato\ncovarianza. realtà la definizione di questo indice non ci\nsorprenderà più di tanto quanto, una forma solo apparentemente\ndiversa, l’abbiamo già incontrato precedenza. Ci ricordiamo infatti\nche la varianza di una generica variabile \\(X\\) è definita come la media\ndegli scarti quadratici di ciascuna osservazione dalla media:\\[\\begin{equation}\nS_{XX} = \\frac{1}{n} \\sum_{=1}^n(X_i - \\bar{X}) (X_i - \\bar{X}).\n\\tag{5.1}\n\\end{equation}\\]Infatti, la varianza viene talvolta descritta come la “covarianza di una\nvariabile con sé stessa”.Adesso facciamo un passo ulteriore. Invece di valutare la dispersione di\nuna sola variabile, chiediamoci come due variabili \\(X\\) e \\(Y\\) “variano\ninsieme” (co-variano). È facile capire come una risposta tale domanda\npossa essere fornita da una semplice trasformazione della formula\nprecedente che diventa:\\[\\begin{equation}\nS_{XY} = \\frac{1}{n} \\sum_{=1}^n(X_i - \\bar{X}) (Y_i - \\bar{Y}).\n\\tag{5.2}\n\\end{equation}\\]L’eq. (5.2) ci fornisce dunque la definizione della covarianza.Per capire il significato dell’eq. (5.2), supponiamo di dividere il grafico della figura 5.1 quattro quadranti definiti da una retta verticale passante per la media dei valori BDI-II e da una retta orizzontale passante per la media dei valori CES-D. Numeriamo quadranti partendo da quello basso sinistra e muovendoci senso antiorario.Se prevalgono punti nel e III quadrante, allora la nuvola di punti\navrà un andamento crescente (per cui valori bassi di \\(X\\) tendono ad\nassociarsi valori bassi di \\(Y\\) e valori elevati di \\(X\\) tendono ad\nassociarsi valori elevati di \\(Y\\)) e la covarianza segno positivo. Mentre\nse prevalgono punti nel II e IV quadrante la nuvola di punti avrà un\nandamento decrescente (per cui valori bassi di \\(X\\) tendono ad\nassociarsi valori elevati di \\(Y\\) e valori elevati di \\(X\\) tendono ad\nassociarsi valori bassi di \\(Y\\)) e la covarianza segno negativo. Dunque,\nil segno della covarianza ci informa sulla direzione della relazione\nlineare tra due variabili: l’associazione lineare si dice positiva se la\ncovarianza è positiva, negativa se la covarianza è negativa.Il segno della covarianza ci informa sulla direzione della relazione, ma\ninvece il valore assoluto della covarianza ci dice ben poco. Esso,\ninfatti, dipende dall’unità di misura delle variabili. Nel caso presente\nquesto concetto è difficile da comprendere, dato che le due variabili \nesame non hanno un’unità di misura (ovvero, hanno un’unità di misura\narbitraria e priva di significato). Ma quest’idea diventa chiara se\npensiamo alla relazione lineare tra l’altezza e il peso delle persone,\nad esempio. La covarianza tra queste due quantità è certamente positiva,\nma il valore assoluto della covarianza diventa più grande se l’altezza\nviene misurata millimetri e il peso grammi, e diventa più piccolo\nl’altezza viene misurata metri e il peso chilogrammi. Dunque, il\nvalore della covarianza cambia al mutare dell’unità di misura delle\nvariabili anche se l’associazione tra le variabili resta costante.","code":""},{"path":"ch:correlation.html","id":"correlazione","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.0.3 Correlazione","text":"Dato che il valore assoluto della covarianza è di difficile\ninterpretazione – pratica, non viene mai interpretato – è\nnecessario trasformare la covarianza modo tale da renderla immune\nalle trasformazioni dell’unità di misura delle variabili. Questa\noperazione si dice standardizzazione e corrisponde alla divisione\ndella covarianza per le deviazioni standard (\\(s_X\\), \\(s_Y\\)) delle due\nvariabili:\\[\\begin{equation}\nr_{XY} = \\frac{S_{XY}}{s_X s_Y}.\n\\tag{5.3}\n\\end{equation}\\]La quantià che si ottiene questo modo viene chiamata correlazione di Bravais-Pearson (dal nome degli autori che, indipendentemente l’uno dall’altro, la hanno introdotta).Il coefficiente di correlazione ha le seguenti proprietà:ha lo stesso segno della covarianza, dato che si ottiene dividendo\nla covarianza per due numeri positivi;è un numero puro, cioè non dipende dall’unità di misura delle\nvariabili;assume valori compresi tra -1 e +1.Ad esso possiamo assegnare la seguente interpretazione:\\(r_{XY} = -1\\) \\(\\rightarrow\\) perfetta relazione negativa: tutti \npunti si trovano esattamente su una retta con pendenza negativa (dal\nquadrante alto sinistra al quadrante basso destra);\\(r_{XY} = +1\\) \\(\\rightarrow\\) perfetta relazione positiva: tutti \npunti si trovano esattamente su una retta con pendenza positiva (dal\nquadrante basso sinistra al quadrante alto destra);\\(-1 < r_{XY} < +1\\) \\(\\rightarrow\\) presenza di una relazione lineare\ndi intensità diversa;\\(r_{XY} = 0\\) \\(\\rightarrow\\) assenza di relazione lineare tra \\(X\\) e\n\\(Y\\).Esercizio 5.1  Per dati della figura 5.1, la covarianza è 207.426. Il segno positivo della covarianza ci dice che tra le due variabili c’è un’associazione lineare positiva. Per capire qual è l’intensità della relazione lineare tra le due variabili calcoliamo la correlazione.\nEssendo le deviazioni standard del BDI-II e del CES-D rispettavamente uguali 15.37 e 14.93, la correlazione diventa uguale \\(\\frac{207.426}{15.38 \\cdot 14.93} = 0.904.\\) Tale valore è prossimo 1.0, il che vuol dire che punti del diagramma dispersione non si discostano troppo da una retta con una pendenza positiva.","code":""},{"path":"ch:correlation.html","id":"correlazione-e-causazione","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.1 Correlazione e causazione","text":"Facendo riferimento nuovamente alla figura 5.1, possiamo dire che, molte applicazioni (ma non nel caso presente!) l’asse \\(x\\) rappresenta una quantità nota come variabile indipendente e l’interesse si concentra sulla sua influenza sulla variabile dipendente tracciata sull’asse \\(y\\). Ciò presuppone però che sia nota la direzione cui l’influenza causale potrebbe risiedere. È importante tenere bene mente che la correlazione è soltanto un indice descrittivo della relazione lineare tra due variabili e nessun caso può essere usata per inferire alcunché sulle relazioni causali che legano le variabili. È ben nota l’espressione: “correlazione non significa causazione”.Di opinione diversa era invece Karl Pearson (1911), il quale ha affermato: “Quanto spesso, quando è stato osservato un nuovo fenomeno,\nsentiamo che viene posta la domanda: ‘qual è la sua causa?’. Questa è\nuna domanda cui potrebbe essere assolutamente impossibile rispondere.\nInvece, può essere più facile rispondere alla domanda: ‘che misura\naltri fenomeni sono associati con esso?’. Dalla risposta questa\nseconda domanda possono risultare molte preziose conoscenze.”Che alla seconda domanda posta da Pearson sia facile rispondere è indubbio. Che la nostra comprensione di un fenomeno possa aumentare sulla base delle\ninformazioni fornite unicamente dalle correlazioni, invece, è molto dubbio e quasi certamente falso.","code":""},{"path":"ch:correlation.html","id":"usi-della-correlazione","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.1.1 Usi della correlazione","text":"Anche se non può essere usata per studiare le relazioni causali, la\ncorrelazione viene usata per molti altri scopi tra quali, per esempio,\nquello di misurare la validità concorrente di un test psiologico. Se\nun test psicologico misura effettivamente ciò che ci si aspetta che\nmisuri (nel caso dell’esempio presente, la depressione), allora dovremo\naspettarci che fornisca una correlazione alta con risultati di altri\ntest che misurano lo stesso costrutto – come nel caso dei dati di\n(Zetsche, Bürkner, Renneberg 2019). Un’altra proprietà desiderabile di un test\npsicometrico è la validità divergente: risultati di test\npsicometrici che misurano costrutti diversi dovrebbero essere poco\nassociati tra loro. altre parole, questo secondo caso dovremmo\naspettarci che la correlazione sia bassa.","code":""},{"path":"ch:correlation.html","id":"correlazione-di-spearman","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.1.2 Correlazione di Spearman","text":"Una misura alternativa della relazione lineare tra due variabili è\nfornita dal coefficiente di correlazione di Spearman e dipende soltanto\ndalla relazione d’ordine dei dati, non dagli specifici valori dei dati.\nTale misura di associazione è appropriata quando, del fenomeno esame,\ngli psicologi sono stati grado di misurare soltanto le relazioni\nd’ordine tra le diverse modalità della risposta dei soggetti, non\nl’intensità della risposta. Le variabili psicologiche che hanno questa\nproprietà si dicono ordinali. Nel caso di variabili ordinali, non è\npossibile sintetizzare dati mediante le statistiche descrittive che\nabbiamo introdotto questo capitolo, quali ad esempio la media e la\nvarianza, ma è invece solo possibile riassumere dati mediante una\ndistribuzione di frequenze per le varie modalità della risposta.","code":""},{"path":"ch:correlation.html","id":"correlazione-nulla","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"5.1.3 Correlazione nulla","text":"Un ultimo aspetto da mettere evidenza proposito della correlazione riguarda il fatto che la correlazione descrive la direzione e l’intensità della relazione lineare tra due variabili. Relazioni non lineari tra le variabili, anche se sono molto forti, non vengono catturate dalla correlazione. È importante rendersi conto che una correlazione pari zero non significa che non c’è relazione tra le due variabili, ma solo che tra esse non c’è una relazione lineare.Esercizio 5.2  La figura 5.2 fornisce un esempio di correlazione nulla presenza di una chiara relazione (non lineare) tra due variabili.\nFIGURA 5.2: Due insiemi di dati (fittizi) per quali coefficienti di correlazione di Pearson sono entrambi 0. Ma questo non significa che non vi sia alcuna relazione tra le variabili.\n","code":""},{"path":"ch:correlation.html","id":"commenti-e-considerazioni-finali-3","chapter":"Capitolo 5 Le relazioni tra variabili","heading":"Commenti e considerazioni finali","text":"La prima fase dell’analisi dei dati riassume dati mediante gli strumenti della statistica descrittiva. Le tipiche domande che vengono affrontate questa fase sono: qual è la distribuzione delle variabili di interesse? Quali relazioni coppie si possono osservare nel campione? Ci sono delle osservazioni ‘anomale’, ovvero estremamente discrepanti rispetto alle altre, sia quando si esaminano le statistiche descrittive univariate (ovvero, quelle che riguardano le caratteristiche di una variabile presa singolarmente), sia quando vengono esaminate le statistiche bivariate (ovvero, le statistiche che descrivono l’associazione tra le variabili)? È importante avere ben chiare le idee su questi punti prima di procedere con qualsiasi procedura statistica di tipo inferenziale. Per rispondere alle domande che abbiamo elencato sopra, ed ad altre simili, è molto utile procedere con delle rappresentazioni grafiche dei dati. È chiaro che, quando disponiamo di grandi moli di dati (come è sempre il caso psicologia), l’analisi descrittiva dei dati deve essere svolta mediante un software statistico.","code":""},{"path":"ch:metropolis.html","id":"ch:metropolis","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"Capitolo 6 Approssimazione della distribuzione a posteriori","text":"generale, un problema bayesiano dati \\(y\\) provengono da una densità \\(p(y \\mid \\theta)\\) e al parametro \\(\\theta\\) viene assegnata una densità priori \\(p(\\theta)\\). Dopo avere osservato dati \\(Y = y\\), la funzione di verosimiglianza è uguale \\(\\mathcal{L}(\\theta) = p(y \\mid \\theta)\\) e la densità posteriori diventa\\[\\begin{equation}\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int p(y \\mid \\theta) p(\\theta) \\,\\operatorname {d}\\! \\theta}. \\notag\n\\end{equation}\\]Se vogliamo trovare la distribuzione posteriori con metodi analitici è necessario ricorrere ’impiego di distribuzioni priori coniugate, come nello schema beta-binomiale. Per quanto “semplice” termini formali, la scelta di distribuzioni priori coniugate limita di molto le possibili scelte del ricercatore. Inoltre, non è sempre sensato, dal punto di vista teorico, utilizzare tali distribuzioni per la stima dei parametri di interesse. Il mancato ricorso ’impiego delle distribuzioni priori coniugate richiede necessariamente il computo dell’espressione denominatore della formula di Bayes che solo rare occasioni può essere ottenuta per via analitica. altre parole, è possibile ottenere analiticamenre la distribuzione posteriori solo per alcune specifiche combinazioni di distribuzioni priori e verosimiglianza, il che limita considerevolmente la flessibilità della modellizzazione. Inoltre, sommari della distribuzione posteriori sono espressi come rapporto di integrali. Ad esempio, la media posteriori di \\(\\theta\\) è data da\\[\\begin{equation}\n\\mathbb{E}(\\theta \\mid y) = \\frac{\\int \\theta p(y \\mid \\theta) p(\\theta) \\,\\operatorname {d}\\! \\theta}{\\int p(y \\mid \\theta) p(\\theta) \\,\\operatorname {d}\\! \\theta}.\\notag\n\\end{equation}\\]Il calcolo del valore atteso posteriori richiede dunque il computo di due integrali, quello denominatore e quello numeratore dell’espressione, ciascuno dei quali non esprimibile forma chiusa. Per questa ragione, la strada principale che viene seguita nella modellistica bayesiana è quella che porta determinare la distribuzione posteriori non per via analitica, ma bensì mediante metodi numerici. La simulazione fornisce dunque la strategia generale del calcolo bayesiano. questo fine vengono principalmente usati metodi di campionamento Monte Carlo basati su Catena di Markov (MCMC). Tali metodi costituiscono una potente e praticabile alternativa per la costruzione della distribuzione posteriori per modelli complessi e consentono di decidere quali distribuzioni priori e quali distribuzioni di verosimiglianza usare sulla base di considerazioni teoriche soltanto, senza dovere preoccuparsi di altri vincoli.Dato che è basata su metodi computazionalmente intensivi, la stima numerica della funzione posteriori può essere svolta soltanto mediante software. anni recenti metodi Bayesiani di analisi dei dati sono diventati sempre più popolari proprio perché la potenza di calcolo necessaria per svolgere tali calcoli è ora alla portata di tutti. Questo non era vero solo pochi decenni fa.questo Capitolo verranno presentati due metodi di simulazione iterativa6 che consentono di generare dalle distribuzioni posteriori campioni dei parametri del modello:metodi basati su griglia: dove, sebbene non sia disponibile alcuna formula algebrica forma chiusa, le proprietà della distribuzione posteriori possono essere calcolate con una precisione arbitraria;metodi Monte Carlo: dove, utilizzando appropriate funzioni di numeri casuali, viene generato un ampio campione di casi della variabile casuale per poi stimare empiricamente la proprietà di interesse base al campione così otttenuto.","code":""},{"path":"ch:metropolis.html","id":"metodo-basato-su-griglia","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.1 Metodo basato su griglia","text":"Il metodo basato su griglia (grid-based) è un metodo numerico esatto basato su una griglia di punti uniformemente spaziati. Anche se la maggior parte dei parametri è continua (ovvero, linea di principio ciascun parametro può assumere un numero infinito di valori), possiamo ottenere un’eccellente approssimazione della distribuzione posteriori considerando solo una griglia finita di valori dei parametri. un tale metodo, la densità di probabilità posteriori può dunque essere approssimata tramite le densità di probabilità calcolate ciascuna cella della griglia.Il metodo basato su griglia si sviluppa quattro fasi:fissare una griglia discreta di possibili valori \\(\\theta\\);valutare la distribuzione priori \\(p(\\theta)\\) e la funzione di verosimiglianza \\(p(y \\mid \\theta)\\) corrispondenza di ciascun valore \\(\\theta\\) della griglia;ottenere un’approssimazione discreta della densità posteriori:\nper ciascun valore \\(\\theta\\) della griglia, calcolare il prodotto \\(p(\\theta) p(y \\mid \\theta)\\);\nnormalizzare prodotti così ottenuti modo tale che la loro somma sia 1;\nper ciascun valore \\(\\theta\\) della griglia, calcolare il prodotto \\(p(\\theta) p(y \\mid \\theta)\\);normalizzare prodotti così ottenuti modo tale che la loro somma sia 1;selezionare \\(N\\) valori casuali della griglia modo tale da ottenere un campione casuale delle densità posteriori normalizzate.Possiamo migliorare l’approssimazione aumentando il numero di punti della griglia. Infatti utilizzando un numero infinito di punti si otterrebbe la descrizione esatta della distribuzione posteriori, dovendo però pagare il costo dell’utilizzo di infinite risorse di calcolo. Il limite maggiore dell’approccio basato su griglia è che, al crescere della dimensionalità \\(N\\) dello spazio dei parametri, punti della griglia necessari per avere una buona stima crescerebbero esponenzialmente con \\(N\\), rendendo questo metodo inattuabile.","code":""},{"path":"ch:metropolis.html","id":"modello-beta-binomiale","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.1.1 Modello Beta-Binomiale","text":"Per fare un esempio, consideriamo lo schema beta-binomiale di cui conosciamo la soluzione esatta. Utilizziamo nuovamente dati di Zetsche, Bürkner, Renneberg (2019): 23 “successi” 30 prove Bernoulliane indipendenti.7 Imponiamo alla distribuzione priori su \\(\\theta\\) (probabilità di successo una singola prova, laddove per “successo” si intende una aspettativa distorta negativamente dell’umore futuro) una \\(\\mbox{Beta}(2, 10)\\) per descrivere la nostra incertezza sul parametro prima di avere osservato dati. Dunque, il modello diventa:\\[\\begin{align}\nY \\mid \\theta & \\sim \\mbox{Bin}(n = 30, \\theta), \\notag\\\\\n\\theta & \\sim \\mbox{Beta}(2, 10).\\notag\n\\end{align}\\]queste circostanze, l’aggiornamento bayesiano produce una distribuzione posteriori Beta di parametri 25 (\\(y + \\alpha\\) = 23 + 2) e 17 (\\(n - y + \\beta\\) = 30 - 23 + 10):\\[\\begin{equation}\n\\theta \\mid (y = 23) \\sim \\mbox{Beta}(25, 17).\\notag\n\\end{equation}\\]Per approssimare la distribuzione posteriori, fissiamo una griglia di \\(n = 11\\) valori equispaziati: \\(\\theta \\\\{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\\}\\):corrispondenza di ciascun valore della griglia, valutiamo la distribuzione priori \\(\\mbox{Beta}(2, 10)\\) e la verosimiglianza \\(\\mbox{Bin}(y = 23, n = 30)\\).ciascuna cella della griglia calcoliamo poi il prodotto della verosimiglianza e della distribuzione priori. Troviamo così un’approssimazione discreta e non normalizzata della distribuzione posteriori (unnormalized). Normalizziamo questa approssimazione dividendo ciascun valore unnormalized per la somma di tutti valori del vettore:Verifichiamo:Abbiamo dunque ottenuto la seguente distribuzione posteriori discretizzata \\(p(\\theta \\mid y)\\):La figura 6.1 mostra un grafico della distribuzione posteriori discretizzata così ottenuta:\nFIGURA 6.1: Distribuzione posteriori discretizzata ottenuta con il metodo grid-based per \\(y\\) = 23 successi 30 prove Bernoulliane, con distribuzione priori \\(\\mbox{Beta}(2, 10)\\). È stata utilizzata una griglia di solo \\(n\\) = 11 punti.\nL’ultimo passo della simulazione è il campionamento dalla distribuzione posteriori discretizzata:La figura 6.2 mostra che, con una griglia così sparsa abbiamo ottenuto una versione approssimata della vera distribuzione posteriori (’istogramma è stata sovrapposta l’esatta distribuzione posteriori \\(\\mbox{Beta}(25, 17)\\)).\nFIGURA 6.2: Campionamento dalla distribuzione posteriori discretizzata ottenuta con il metodo grid-based per \\(y\\) = 23 successi 30 prove Bernoulliane, con distribuzione priori \\(\\mbox{Beta}(2, 10)\\). È stata utilizzata una griglia di solo \\(n\\) = 11 punti.\nPossiamo ottenere un risultato migliore con una griglia più fine, come indicato nella figura 6.3:\nFIGURA 6.3: Distribuzione posteriori discretizzata ottenuta con il metodo grid-based per \\(y\\) = 23 successi 30 prove Bernoulliane, con distribuzione priori \\(\\mbox{Beta}(2, 10)\\). È stata utilizzata una griglia di \\(n\\) = 100 punti.\nCampioniamo ora 10000 punti:Con il campionamento dalla distribuzione posteriori discretizzata costruita mediante una griglia più densa (\\(n = 100\\)) otteniamo un risultato soddisfacente (figura 6.4): ora la distribuzione dei valori prodotti dalla simulazione approssima molto bene la corretta distribuzione posteriori \\(p(\\theta \\mid y) = \\mbox{Beta}(25, 17)\\).\nFIGURA 6.4: Campionamento dalla distribuzione posteriori discretizzata ottenuta con il metodo grid-based per \\(y\\) = 23 successi 30 prove Bernoulliane, con distribuzione priori \\(\\mbox{Beta}(2, 10)\\). È stata utilizzata una griglia di \\(n\\) = 100 punti. ’istogramma è stata sovrapposta la corretta distribuzione posteriori, ovvero la densità \\(\\mbox{Beta}(25, 17)\\).\nconclusione, il metodo basato su griglia è molto intuitivo e non richiede particolari competenze di programmazione per essere implementato. Inoltre, fornisce un risultato che, per tutti gli scopi pratici, può essere considerato come un campione casuale estratto da \\(p(\\theta \\mid y)\\). Tuttavia, anche se tale metodo fornisce risultati accuratissimi, esso ha un uso limitato. causa della maledizione della dimensionalità8, tale metodo può solo essere solo nel caso di semplici modelli statistici, con non più di due parametri. Nella pratica concreta tale metodo viene dunque sostituito da altre tecniche più efficienti quanto, anche nei più comuni modelli utilizzati psicologia, vengono solitamente stimati centinaia se non migliaia di parametri.","code":"\ngrid_data <- tibble(\n  theta_grid = seq(from = 0, to = 1, length.out = 11)\n)\ngrid_data\n#> # A tibble: 11 × 1\n#>   theta_grid\n#>        <dbl>\n#> 1        0  \n#> 2        0.1\n#> 3        0.2\n#> 4        0.3\n#> 5        0.4\n#> 6        0.5\n#> 7        0.6\n#> 8        0.7\n#> # … with 3 more rows\ngrid_data <- grid_data %>%\n  mutate(\n    prior = dbeta(theta_grid, 2, 10),\n    likelihood = dbinom(23, 30, theta_grid)\n  )\ngrid_data <- grid_data %>%\n  mutate(\n    unnormalized = likelihood * prior,\n    posterior = unnormalized / sum(unnormalized)\n  )\ngrid_data %>%\n  summarize(\n    sum(unnormalized),\n    sum(posterior)\n  )\n#> # A tibble: 1 × 2\n#>   `sum(unnormalized)` `sum(posterior)`\n#>                 <dbl>            <dbl>\n#> 1            0.000869                1\nround(grid_data, 2)\n#> # A tibble: 11 × 5\n#>   theta_grid prior likelihood unnormalized posterior\n#>        <dbl> <dbl>      <dbl>        <dbl>     <dbl>\n#> 1        0    0          0               0      0   \n#> 2        0.1  4.26       0               0      0   \n#> 3        0.2  2.95       0               0      0   \n#> 4        0.3  1.33       0               0      0   \n#> 5        0.4  0.44       0               0      0.02\n#> 6        0.5  0.11       0               0      0.23\n#> 7        0.6  0.02       0.03            0      0.52\n#> 8        0.7  0          0.12            0      0.21\n#> # … with 3 more rows\ngrid_data %>%\n  ggplot(\n    aes(x = theta_grid, y = posterior)\n  ) +\n  geom_point() +\n  geom_segment(\n    aes(\n      x = theta_grid,\n      xend = theta_grid,\n      y = 0,\n      yend = posterior\n    )\n  )\nset.seed(84735)\npost_sample <- sample_n(\n  grid_data,\n  size = 1e5,\n  weight = posterior,\n  replace = TRUE\n)\nggplot(post_sample, aes(x = theta_grid)) +\n  geom_histogram(aes(y = ..density..), color = \"white\") +\n  stat_function(fun = dbeta, args = list(25, 17)) +\n  lims(x = c(0, 1))\ngrid_data <- tibble(\n  theta_grid = seq(from = 0, to = 1, length.out = 100)\n)\ngrid_data <- grid_data %>%\n  mutate(\n    prior = dbeta(theta_grid, 2, 10),\n    likelihood = dbinom(23, 30, theta_grid)\n  )\ngrid_data <- grid_data %>%\n  mutate(\n    unnormalized = likelihood * prior,\n    posterior = unnormalized / sum(unnormalized)\n  )\ngrid_data %>%\n  ggplot(\n    aes(x = theta_grid, y = posterior)\n  ) +\n  geom_point() +\n  geom_segment(\n    aes(\n      x = theta_grid,\n      xend = theta_grid,\n      y = 0,\n      yend = posterior\n    )\n  )\n# Set the seed\nset.seed(84735)\npost_sample <- sample_n(\n  grid_data,\n  size = 1e4,\n  weight = posterior,\n  replace = TRUE\n)\npost_sample %>%\n  ggplot(aes(x = theta_grid)) +\n  geom_histogram(\n    aes(y = ..density..),\n    color = \"white\",\n    bins = 50\n  ) +\n  stat_function(fun = dbeta, args = list(25, 17)) +\n  lims(x = c(0, 1))"},{"path":"ch:metropolis.html","id":"chapter-simulazioneMC","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2 Metodo Monte Carlo","text":"metodi più ampiamente adottati nell’analisi bayesiana per la costruzione della distribuzione posteriori per modelli complessi sono metodi di campionamento MCMC. Tali metodi consentono al ricercatore di decidere quali distribuzioni priori e quali distribuzioni di verosimiglianza usare sulla base di considerazioni teoriche soltanto, senza doversi preoccupare di altri vincoli. Dato che è basata su metodi computazionalmente intensivi, la stima numerica MCMC della funzione posteriori può essere svolta soltanto mediante software. anni recenti metodi Bayesiani di analisi dei dati sono diventati sempre più popolari proprio perché la potenza di calcolo necessaria per svolgere tali calcoli è ora alla portata di tutti. Questo non era vero solo pochi decenni fa.","code":""},{"path":"ch:metropolis.html","id":"integration-mc","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.1 Integrazione di Monte Carlo","text":"Il termine Monte Carlo si riferisce al fatto che la computazione fa ricorso ad un ripetuto campionamento casuale attraverso la generazione di sequenze di numeri casuali. Una delle sue applicazioni più potenti è il calcolo degli integrali mediante simulazione numerica. Supponiamo di essere grado di estrarre campioni casuali dalla distribuzione continua \\(p(\\theta \\mid y)\\) di media \\(\\mu\\). Se possiamo ottenere una sequenza di realizzazioni indipendenti\\[\n\\theta^{(1)}, \\theta^{(2)},\\dots, \\theta^{(T)} \\overset{\\text{iid}}{\\sim} p(\\theta \\mid y)\n\\]allora diventa possibile calcolare\\[\n\\mathbb{E}(\\theta \\mid y) = \\int \\theta p(\\theta \\mid y) \\,\\operatorname {d}\\!\\theta \\approx \\frac{1}{T} \\sum_{=1}^T \\theta^{(t)}.\n\\]altre parole, l’aspettazione teorica di \\(\\theta\\) può essere approssimata dalla media campionaria di un insieme di realizzazioni indipendenti ricavate da \\(p(\\theta \\mid y)\\). Per la Legge Forte dei Grandi Numeri, l’approssimazione diventa arbitrariamente esatta per \\(T \\rightarrow \\infty\\).9Quello che è stato detto sopra non è altro che un modo sofisticato per dire che, se vogliamo calcolare un’approssimazione del valore atteso di una variabile casuale, non dobbiamo fare altro che la media aritmetica di un grande numero di realizzazioni indipendenti della variabile casuale. Come è facile intuire, l’approssimazione migliora al crescere del numero di dati che abbiamo disposizione.Un’altra importante funzione di \\(\\theta\\) è la funzione indicatore, \\((l < \\theta < u)\\), che assume valore 1 se \\(\\theta\\) giace nell’intervallo \\((l, u)\\) e 0 altrimenti. Il valore di aspettazione di \\((l < \\theta < u)\\) rispetto \\(p(\\theta)\\) dà la probabilità che \\(\\theta\\) rientri nell’intervallo specificato, \\(Pr(l < \\theta < u)\\), e può essere approssimato usando l’integrazione Monte Carlo, ovvero prendendo la media campionaria del valore della funzione indicatore per ogni realizzazione \\(\\theta^{(t)}\\). È semplice vedere come\\[\nPr(l < \\theta < u) \\approx \\frac{\\text{numero di realizzazioni } \\theta^{(t)} \\(l, u)}{T}.\n\\]Presentiamo qui l’integrazione di Monte Carlo perché, nell’analisi bayesiana, il metodo Monte Carlo viene usato per ottenere un’approssimazione della distribuzione posteriori, quando tale distribuzione non può essere calcolata con metodi analitici. altre parole, il metodo Monte Carlo consente di ottenere un gran numero di valori \\(\\theta\\) che, nelle circostanze ideali, avrà una distribuzione identica alla distribuzione posteriori \\(p(\\theta \\mid y)\\).","code":""},{"path":"ch:metropolis.html","id":"descrizione-intuitiva","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.2 Descrizione intuitiva","text":"Se la funzione di densità \\(p(\\theta \\mid y)\\) è conosciuta, è facile ottenere una sequenza di realizzazioni iid della variabile casuale, per esempio, usando \\(\\textsf{R}\\). Ma ora supponiamo di non conoscere \\(p(\\theta \\mid y)\\). Quello che vogliamo fare è ottenere comunque una sequenza di valori \\(\\theta\\). Anche se tali valori non saranno iid, per qualunque coppia di valori \\(\\theta_a\\) e \\(\\theta_b\\) nella sequenza vogliamo che sia soddisfatto il seguente vincolo:\\[\n\\frac{\\#\\theta'\\text{ nella sequenza} = \\theta_a}{\\#\\theta'\\text{ nella sequenza} = \\theta_b} \\approx \\frac{p(\\theta_a \\mid y)}{p(\\theta_b \\mid y)}.\n\\]L’algoritmo di Metropolis ci consente di ottenere una tale sequenza di valori, la cui distribuzione sarà dunque uguale \\(p(\\theta \\mid y)\\). forma intuitiva, l’algoritmo di Metropolis può essere descritto come indicato di seguito.Data una sequenza di valori \\(\\{\\theta^{(1)}, \\theta^{(2)},\\dots, \\theta^{(t)}\\}\\), ci poniamo il problema di aggiungere un nuovo valore \\(\\theta^{t+1}\\) alla sequenza.Data una sequenza di valori \\(\\{\\theta^{(1)}, \\theta^{(2)},\\dots, \\theta^{(t)}\\}\\), ci poniamo il problema di aggiungere un nuovo valore \\(\\theta^{t+1}\\) alla sequenza.Consideriamo un valore \\(\\theta^*\\) simile \\(\\theta^{(t)}\\); ci chiediamo se dobbiamo inserire un tale valore nella sequenza oppure .Consideriamo un valore \\(\\theta^*\\) simile \\(\\theta^{(t)}\\); ci chiediamo se dobbiamo inserire un tale valore nella sequenza oppure .Se \\(p(\\theta^* \\mid y) > p(\\theta^{(t)} \\mid y)\\), allora sicuramente lo dobbiamo aggiungere alla sequenza perché, nella sequenza, il numero di valori \\(\\theta^*\\) deve essere maggiore del numero dei valori \\(\\theta^{(t)}\\).Se \\(p(\\theta^* \\mid y) > p(\\theta^{(t)} \\mid y)\\), allora sicuramente lo dobbiamo aggiungere alla sequenza perché, nella sequenza, il numero di valori \\(\\theta^*\\) deve essere maggiore del numero dei valori \\(\\theta^{(t)}\\).Se invece \\(p(\\theta^* \\mid y) < p(\\theta^{(t)} \\mid y)\\), allora non dobbiamo necessariamente aggiungere \\(\\theta^*\\) alla sequenza.Se invece \\(p(\\theta^* \\mid y) < p(\\theta^{(t)} \\mid y)\\), allora non dobbiamo necessariamente aggiungere \\(\\theta^*\\) alla sequenza.La decisione di aggiungere o \\(\\theta^*\\) alla sequenza dipenderà dal confronto tra \\(p(\\theta^* \\mid y)\\) e \\(p(\\theta^{(t)} \\mid y)\\).La decisione di aggiungere o \\(\\theta^*\\) alla sequenza dipenderà dal confronto tra \\(p(\\theta^* \\mid y)\\) e \\(p(\\theta^{(t)} \\mid y)\\).Calcoliamo il rapportoCalcoliamo il rapporto\\[\n  r = \\frac{p(\\theta^* \\mid y)}{p(\\theta^{(t)} \\mid y)} = \\frac{p(y \\mid \\theta^*) p(\\theta^*)}{p(y \\mid p(\\theta^{(t)}) p(p(\\theta^{(t)})}.\n\\]Se \\(r > 1\\), accettiamo \\(\\theta^*\\) e lo aggiungiamo alla sequenza: \\(\\theta^{(t+1)} = \\theta^*\\), quanto \\(\\theta^{(t)}\\) è già presente nella sequenza e \\(\\theta^*\\) ha una probabilità maggiore di \\(\\theta^{(t)}\\).Se \\(r < 1\\), per ciasuna istanza di \\(\\theta^{(t)}\\), accettiamo \\(\\theta^*\\) solo una frazione di volte uguale \\[\n\\frac{p(\\theta^* \\mid y)}{p(\\theta^{(t)} \\mid y)}\n\\]quanto la frequenza relativa dei valori \\(\\theta^{(t)}\\) e \\(\\theta^*\\) nella sequenza deve essere uguale al rapporto precedente. Per ottenere questo risultato, poniamo \\(\\theta^{(t+1)}\\) uguale \\(\\theta^*\\) o \\(\\theta^{(t)}\\) con probabilità rispettivamente uguali \\(r\\) o \\(1 - r\\).Questa è l’intuizione che sta alla base dell’algoritmo di Metropolis et al. (1953).","code":""},{"path":"ch:metropolis.html","id":"unapplicazione-empirica","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.3 Un’applicazione empirica","text":"Poniamoci ora il problema di usare l’algoritmo di Metropolis per calcolare la distribuzione posteriori di una proporzione \\(\\theta\\). Usiamo nuovamente dati di Zetsche, Bürkner, Renneberg (2019) (ovvero, 23 “successi” 30 prove Bernoulliane) e, per rendere il problema più interessante, assumiamo per \\(\\theta\\) una distribuzione priori \\(\\mbox{Beta}(2, 10)\\). Sappiamo che, tali circostanze, la distribuzione posteriori può essere ottenuta analiticamente tramite lo schema beta-binomiale ed è una \\(\\mbox{Beta}(25, 17)\\). Se vogliamo il valore della media posteriori di \\(\\theta\\), il risultato esatto è dunque\\[\n\\bar{\\theta}_{post} = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{25}{25 + 17} \\approx 0.5952.\n\\]È anche possibile ottenere il valore della media posteriori per via numerica. Sapendo che la distribuzione posteriori è una \\(\\mbox{Beta}(25, 17)\\), possiamo estrarre un campione di osservazioni da una tale distribuzione e calcolare la media. Con poche osservazioni (diciamo 10) otteniamo un risultato molto approssimatoma, per la legge dei grandi numeri, l’approssimazione migliora ’aumentare del numero di osservazioni:Lo stesso si può dire delle altre statistiche descrittive: moda, varianza, eccetera. Nel presente esempio, la simulazione Monte Carlo produce il risultato desiderato perchésappiamo che la distribuzione posteriori è una \\(\\mbox{Beta}(25, 17)\\),è possibile usare le funzioni \\(\\textsf{R}\\) per estrarre campioni casuali da una tale distribuzione.Tuttavia, capita raramente di usare una distribuzione priori coniugata alla verosimiglianza. Quindi, generale, le due condizioni descritte sopra non si applicano. Ad esempio, nel caso di una verosimiglianza binomiale e di una distribuzione priori gaussiana, la distribuzione posteriori di \\(\\theta\\) è\\[\np(\\theta \\mid y) = \\frac{\\mathrm{e}^{-(\\theta - 1 / 2)^2} \\theta^y (1 - \\theta)^{n - y}} {\\int_0^1 \\mathrm{e}^{-(t - 1 / 2)^2} t^y (1 - t)^{n - y} dt}.\n\\]Una tale distribuzione non è implementata \\(\\textsf{R}\\) e dunque non possiamo ottenere dei campioni casuali da una tale distribuzione.tali circostanze, però, è ancora possibile ottenere ottenere un campione causale dalla distribuzione posteriori un altro modo. Questo risultato si ottiene utilizzando metodi Monte Carlo basati su Catena di Markov (MCMC). metodi MCMC, di cui l’algoritmo di Metropolis è un caso particolare e ne rappresenta il primo esempio, sono una classe di algoritmi che consentono di ottenere campioni casuali da una distribuzione posteriori senza dovere conoscere la rappresentazione analitica di una tale distribuzione.10 Le tecniche MCMC sono il metodo computazionale maggiormente usato per risolvere problemi dell’inferenza bayesiana.","code":"\nset.seed(84735)\nprint(mean(rbeta(1e2, shape1 = 25, shape2 = 17)), 6)\n#> [1] 0.584251\nprint(mean(rbeta(1e4, shape1 = 25, shape2 = 17)), 6)\n#> [1] 0.595492\nprint(mean(rbeta(1e6, shape1 = 25, shape2 = 17)), 6)\n#> [1] 0.595192"},{"path":"ch:metropolis.html","id":"una-passeggiata-casuale-sui-numeri-naturali","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.4 Una passeggiata casuale sui numeri naturali","text":"Prima di applicare l’algoritmo di Metropolis ai dati di Zetsche, Bürkner, Renneberg (2019), consideriamo un caso più semplice. questo esempio preliminare useremo l’algoritmo di Metropolis per ottenere un campione casuale da una distribuzione di massa di probabilità; esamineremo il caso continuo seguito.11Definiamo la distribuzione di probabilità discreta della variabile casuale \\(X\\) che assume valori nell’insieme dei numeri naturali \\(1, 2, \\dots, K\\). Scriviamo \\(\\textsf{R}\\) la funzione pd() che assegna \\(X = \\{1, 2, \\dots, 8\\}\\) valori di probabilità proporzionali agli interi 5, 10, 4, 4, 20, 20, 12 e 5:La figura 6.5 illustra la distribuzione di massa di probabilità che è stata generata questo modo.\nFIGURA 6.5: Distribuzione di massa di probabilità della variabile casuale discreta \\(X\\) avente supporto \\(\\{1, 2, ..., 8\\}\\).\nPer dati di questo esempio, l’algoritmo di Metropolis corrisponde alla seguente passeggiata aleatoria.12L’algoritmo inizia con un valore iniziale qualsiasi da 1 \\(K=8\\) della variabile casuale.Per simulare il valore successivo della sequenza, lanciamo una moneta equilibrata. Se esce testa, consideriamo come valore candidato il valore immediatamente precedente al valore corrente nella sequenza; se esce croce, il candidato è il valore nella sequenza immediatamente successivo quello corrente.Si calcola il rapporto \\(r\\) tra la probabilità del valore candidato e la probabilità del valore corrente:\\[\nr = \\frac{pd(\\text{valore candidato})}{pd(\\text{valore corrente})}.\n\\]Si estrae un numero caso \\(\\[0, 1]\\). Se tale valore è minore di \\(r\\) si accetta il valore candidato come valore successivo della catena markoviana; altrimenti il valore successivo della catena rimane il valore corrente.termini tecnici (si veda l’Appendice ??), passi da 1 4 definiscono una catena di Markov irriducibile e aperiodica sui valori di stato \\(\\{1, 2,\\dots, 8\\}\\), dove il passo 1 fornisce il valore iniziale della catena e passi da 2 4 definiscono la matrice di transizione \\(P\\). Il campionamento dalla distribuzione di massa pd corrisponde ad una passeggiata aleatoria che inizia da una posizione qualsiasi e che ripete le fasi 2, 3 e 4 dell’algoritmo di Metropolis. Dopo un gran numero di passi, la distribuzione dei valori della catena markoviana approssimerà la distribuzione di probabilità pd.La funzione random_walk() implementa l’algoritmo di Metropolis. Tale funzione prende input la distribuzione di probabilità pd, la posizione di partenza start e il numero di passi dell’algoritmo num_steps.Implementiamo ora l’algoritmo di Metropolis utilizzando, quale valore iniziale, \\(X=4\\). Ripetiamo la simulazione 10,000 volte.\nFIGURA 6.6: L’istogramma confronta valori prodotti dall’algoritmo di Metropolis con valori corretti della distribuzione di massa di probabilità.\nLa figura 6.6 confronta l’istogramma dei valori simulati dalla passeggiata aleatoria con l’effettiva distribuzione di probabilità pd. Si noti che le due distribuzioni sono molto simili.","code":"\npd <- function(x) {\n  values <- c(5, 10, 4, 4, 20, 20, 12, 5)\n  ifelse(\n    x %in% 1:length(values),\n    values[x] / sum(values),\n    0\n  )\n}\nprob_dist <- tibble(\n  x = 1:8,\n  prob = pd(1:8)\n)\nx <- 1:8\nprob_dist %>%\n  ggplot(aes(x = x, y = prob)) +\n  geom_bar(stat = \"identity\", width = 0.06) +\n  scale_x_continuous(\"x\", labels = as.character(x), breaks = x) +\n  labs(\n    y = \"Probabilità\",\n    x = \"X\"\n  )\nrandom_walk <- function(pd, start, num_steps) {\n  y <- rep(0, num_steps)\n  current <- start\n  for (j in 1:num_steps) {\n    candidate <- current + sample(c(-1, 1), 1)\n    prob <- pd(candidate) / pd(current)\n    if (runif(1) < prob) {\n      current <- candidate\n    }\n    y[j] <- current\n  }\n  return(y)\n}\nout <- random_walk(pd, 4, 1e4)\n\nS <- tibble(out) %>%\n  group_by(out) %>%\n  summarize(\n    N = n(),\n    Prob = N / 10000\n  )\n\nprob_dist2 <- rbind(\n  prob_dist,\n  tibble(\n    x = S$out,\n    prob = S$Prob\n  )\n)\nprob_dist2$Type <- rep(\n  c(\"Prob. corrette\", \"Prob. simulate\"),\n  each = 8\n)\nx <- 1:8\nprob_dist2 %>%\n  ggplot(aes(x = x, y = prob, fill = Type)) +\n  geom_bar(\n    stat = \"identity\",\n    width = 0.1,\n    position = position_dodge(0.3)\n  ) +\n  scale_x_continuous(\n    \"x\",\n    labels = as.character(x),\n    breaks = x\n  ) +\n  scale_fill_manual(values = c(\"black\", \"gray80\")) +\n  theme(legend.title = element_blank()) +\n  labs(\n    y = \"Probabilità\",\n    x = \"X\"\n  )"},{"path":"ch:metropolis.html","id":"lalgoritmo-di-metropolis","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.5 L’algoritmo di Metropolis","text":"Dopo avere introdotto l’algoritmo di Metropolis con l’esempio proposto da Albert Hu (2019), consideriamo ora l’algoritmo nella sua forma più generale.13 Nelle iterazioni dell’algoritmo di Metropolis possiamo distinguere le seguenti fasi.Si inizia con un punto arbitrario \\(\\theta^{(1)}\\); quindi il primo valore della catena di Markov \\(\\theta^{(1)}\\) può corrispondere semplicemente ad un valore caso tra valori possibili del parametro.Per ogni passo successivo della catena, \\(m + 1\\), si estrae un valore candidato \\(\\theta'\\) da una distribuzione proposta: \\(\\theta' \\sim \\Pi(\\theta)\\). La distribuzione proposta può essere qualunque distribuzione, anche se, idealmente, è meglio che sia simile alla distribuzione posteriori. pratica, però, la distribuzione posteriori è sconosciuta e quindi il valore \\(\\theta'\\) viene estratto caso da una qualche distribuzione simmetrica centrata sul valore corrente \\(\\theta^{(m)}\\) del parametro. Nell’esempio presente useremo la gaussiana quale distribuzione proposta. La distribuzione proposta (gaussiana) sarà centrata sul valore corrente della catena e avrà una deviazione standard appropriata: \\(\\theta' \\sim \\mathcal{N}(\\theta^{(m)}, \\sigma)\\). pratica, questo significa che, se \\(\\sigma\\) è piccola, il valore candidato \\(\\theta'\\) sarà simile al valore corrente \\(\\theta^{(m)}\\).Per ogni passo successivo della catena, \\(m + 1\\), si estrae un valore candidato \\(\\theta'\\) da una distribuzione proposta: \\(\\theta' \\sim \\Pi(\\theta)\\). La distribuzione proposta può essere qualunque distribuzione, anche se, idealmente, è meglio che sia simile alla distribuzione posteriori. pratica, però, la distribuzione posteriori è sconosciuta e quindi il valore \\(\\theta'\\) viene estratto caso da una qualche distribuzione simmetrica centrata sul valore corrente \\(\\theta^{(m)}\\) del parametro. Nell’esempio presente useremo la gaussiana quale distribuzione proposta. La distribuzione proposta (gaussiana) sarà centrata sul valore corrente della catena e avrà una deviazione standard appropriata: \\(\\theta' \\sim \\mathcal{N}(\\theta^{(m)}, \\sigma)\\). pratica, questo significa che, se \\(\\sigma\\) è piccola, il valore candidato \\(\\theta'\\) sarà simile al valore corrente \\(\\theta^{(m)}\\).Si calcola il rapporto \\(r\\) tra la densità della distribuzione posteriori non normalizzata calcolata nel punto \\(\\theta'\\) e la densità nel punto \\(\\theta^{(m)}\\):Si calcola il rapporto \\(r\\) tra la densità della distribuzione posteriori non normalizzata calcolata nel punto \\(\\theta'\\) e la densità nel punto \\(\\theta^{(m)}\\):\\[\\begin{equation}\nr = \\frac{p(y \\mid \\theta') p(\\theta')}{p(y \\mid \\theta^{(m)}) p(\\theta^{(m)})}.\n\\tag{6.1}\n\\end{equation}\\]Il numeratore della (6.1) contiene il prodotto tra la verosimiglianza \\(p(y \\mid \\theta')\\) e la densità priori di \\(\\theta\\), entrambe calcolate nel punto \\(\\theta'\\). Il denominatore contiene il prodotto tra la verosimiglianza \\(p(y \\mid \\theta^{(m)})\\) e la densità priori di \\(\\theta\\), entrambe calcolate nel punto \\(\\theta^{(m)}\\). Si noti che, essendo un rapporto, la (6.1) cancella la costante di normalizzazione.Si decide se accettare il candidato \\(\\theta'\\) oppure se rigettarlo e estrarre un nuovo valore dalla distribuzione proposta. Possiamo pensare al rapporto \\(r\\) come alla risposta alla seguente domanda: alla luce dei dati, quale stima di \\(\\theta\\) è più plausibile il valore candidato o il valore corrente? Se \\(r\\) è maggiore di 1, ciò significa che il candidato è più plausibile del valore corrente; dunque il candidato viene sempre accettato. Altrimenti, si decide di accettare il candidato con una probabilità minore di 1, ovvero non sempre, ma soltanto con una probabilità uguale ad \\(r\\). Se \\(r\\) è uguale 0.10, ad esempio, questo significa che la credibilità posteriori del valore candidato è 10 volte più piccola della credibilità posteriori del valore corrente. Dunque, il valore candidato verrà accettato solo nel 10% dei casi. Come conseguenza di questa strategia di scelta, l’algoritmo di Metropolis ottiene un campione casuale dalla distribuzione posteriori, dato che la probabilità di accettare il valore candidato è proporzionale alla densità del candidato nella distribuzione posteriori. Dal punto di vista algoritmico, la procedura descritta sopra viene implementata confrontando il rapporto \\(r\\) con un valore estratto caso da una distribuzione uniforme \\(\\mbox{Unif}(0, 1)\\). Se \\(r > u \\sim \\mbox{Unif}(0, 1)\\), allora il candidato \\(\\theta'\\) viene accettato e la catena si muove quella nuova posizione, ovvero \\(\\theta^{(m+1)} = \\theta'\\). Altrimenti \\(\\theta^{(m+1)} = \\theta^{(m)}\\) e si estrae un nuovo candidato dalla distribuzione proposta.Si decide se accettare il candidato \\(\\theta'\\) oppure se rigettarlo e estrarre un nuovo valore dalla distribuzione proposta. Possiamo pensare al rapporto \\(r\\) come alla risposta alla seguente domanda: alla luce dei dati, quale stima di \\(\\theta\\) è più plausibile il valore candidato o il valore corrente? Se \\(r\\) è maggiore di 1, ciò significa che il candidato è più plausibile del valore corrente; dunque il candidato viene sempre accettato. Altrimenti, si decide di accettare il candidato con una probabilità minore di 1, ovvero non sempre, ma soltanto con una probabilità uguale ad \\(r\\). Se \\(r\\) è uguale 0.10, ad esempio, questo significa che la credibilità posteriori del valore candidato è 10 volte più piccola della credibilità posteriori del valore corrente. Dunque, il valore candidato verrà accettato solo nel 10% dei casi. Come conseguenza di questa strategia di scelta, l’algoritmo di Metropolis ottiene un campione casuale dalla distribuzione posteriori, dato che la probabilità di accettare il valore candidato è proporzionale alla densità del candidato nella distribuzione posteriori. Dal punto di vista algoritmico, la procedura descritta sopra viene implementata confrontando il rapporto \\(r\\) con un valore estratto caso da una distribuzione uniforme \\(\\mbox{Unif}(0, 1)\\). Se \\(r > u \\sim \\mbox{Unif}(0, 1)\\), allora il candidato \\(\\theta'\\) viene accettato e la catena si muove quella nuova posizione, ovvero \\(\\theta^{(m+1)} = \\theta'\\). Altrimenti \\(\\theta^{(m+1)} = \\theta^{(m)}\\) e si estrae un nuovo candidato dalla distribuzione proposta.Il passaggio finale dell’algoritmo calcola l’accettanza una specifica esecuzione dell’algoritmo, ovvero la proporzione di candidati \\(\\theta'\\) che sono stati accettati quali valori successivi della catena.Il passaggio finale dell’algoritmo calcola l’accettanza una specifica esecuzione dell’algoritmo, ovvero la proporzione di candidati \\(\\theta'\\) che sono stati accettati quali valori successivi della catena.L’algoritmo di Metropolis prende come input il numero \\(T\\) di passi da simulare, la deviazione standard \\(\\sigma\\) della distribuzione proposta e la densità priori, e ritorna come output la sequenza \\(\\theta^{(1)}, \\theta^{(2)}, \\dots, \\theta^{(T)}\\). La chiave del successo dell’algoritmo di Metropolis è il numero di passi fino che la catena approssima la stazionarietà. Tipicamente primi da 1000 5000 elementi sono scartati. Dopo un certo periodo \\(k\\) (detto di burn-), la catena di Markov converge ad una variabile casuale che è distribuita secondo la distribuzione posteriori. altre parole, campioni del vettore \\(\\left(\\theta^{(k+1)}, \\theta^{(k+2)}, \\dots, \\theta^{(T)}\\right)\\) diventano campioni di \\(p(\\theta \\mid y)\\).","code":""},{"path":"ch:metropolis.html","id":"unapplicazione-empirica-1","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.6 Un’applicazione empirica","text":"Possiamo ora utilizzare l’algoritmo di Metropolis per trovare, nel caso dei pazienti clinici depressi di Zetsche, Bürkner, Renneberg (2019), la distribuzione posteriori di \\(\\theta\\), ovvero la probabilità che l’umore futuro atteso sia negativo. dati di Zetsche, Bürkner, Renneberg (2019) ci dicono che, nel caso dei 30 pazienti che sono stati esaminati, 23 hanno manifestato aspettative distorte negativamente circa il loro stato d’animo futuro. priori, abbiamo deciso di imporre su \\(\\theta\\) una \\(\\mbox{Beta}(2, 10)\\).14","code":""},{"path":"ch:metropolis.html","id":"funzioni","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.6.1 Funzioni","text":"Definiamo la funzione likelihood(), considerati come fissi dati di Zetsche, Bürkner, Renneberg (2019), ritorna l’ordinata della verosimiglianza binomiale per ciascun valore param input:La funzione prior() ritorna l’ordinata della distribuzione priori \\(\\mbox{Beta}(2, 10)\\) per ciascun valore param input:La funzione posterior() ritorna, per ciascun valore param input, il prodotto della densità priori e della verosimiglianza:","code":"\nlikelihood <- function(param, x = 23, N = 30) {\n  dbinom(x, N, param)\n}\nprior <- function(param, alpha = 2, beta = 10) {\n  dbeta(param, alpha, beta)\n}\nposterior <- function(param) {\n  likelihood(param) * prior(param)\n}"},{"path":"ch:metropolis.html","id":"implementazione","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.6.2 Implementazione","text":"Per implementare l’algoritmo di Metropolis utilizzeremo una distribuzione proposta gaussiana. Il valore candidato sarà dunque un valore selezionato caso da una gaussiana di parametri \\(\\mu\\) uguale al valore corrente nella catena e \\(\\sigma = 0.9\\). questo esempio, la deviazione standard \\(\\sigma\\) è stata scelta empiricamente modo tale da ottenere una accettanza adeguata. L’accettanza ottimale èpari circa 0.20/0.30 — se l’accettanza è troppo grande, l’algoritmo esplora uno spazio troppo ristretto della distribuzione posteriori.15Ho inserito un controllo che impone al valore candidato di essere incluso nell’intervallo [0, 1], com’è necessario per il valore di una proporzione.16L’algoritmo di Metropolis viene implementato nella seguente funzione:Mediante la funzione precedente, generiamo una catena di valori \\(\\theta\\):questo modo, abbiamo ottenuto una catena di Markov costituita da 10,001 valori. Escludiamo primi 5,000 valori considerati come burn-. Consideriamo restanti 5,001 valori come un campione casuale estratto dalla distribuzione posteriori \\(p(\\theta \\mid y)\\).L’accettanza è pari \nil che conferma la bontà della deviazione standard (\\(\\sigma\\) = 0.9) scelta per la distribuzione proposta.Mediante valori della catena così ottenuta è facile trovare una stima posteriori del parametro \\(\\theta\\). Per esempio, la stima della media posteriori è:Una figura che mostra l’approssimazione di \\(p(\\theta \\mid y)\\) ottenuta con l’algoritmo di Metropolis, insieme ad un trace plot dei valori della catena di Markov, è prodotta nel modo seguente:\nFIGURA 6.7: Sinistra. Stima della distribuzione posteriori della probabilità di una aspettativa futura distorta negativamente per dati di Zetsche et al. (2019). Destra. Trace plot dei valori della catena di Markov escludendo il periodo di burn-.\n","code":"\nproposal_distribution <- function(param) {\n  while (1) {\n    res <- rnorm(1, mean = param, sd = 0.9)\n    if (res > 0 & res < 1) {\n      break\n    }\n  }\n  res\n}\nmetropolis <- function(startvalue, iterations) {\n  chain <- vector(length = iterations + 1)\n  chain[1] <- startvalue\n  for (i in 1:iterations) {\n    proposal <- proposal_distribution(chain[i])\n    r <- posterior(proposal) / posterior(chain[i])\n    if (runif(1) < r) {\n      chain[i + 1] <- proposal\n    } else {\n      chain[i + 1] <- chain[i]\n    }\n  }\n  chain\n}\nset.seed(84735)\nstartvalue <- runif(1, 0, 1)\nniter <- 1e4\nchain <- metropolis(startvalue, niter)\nburnin <- niter / 2\nacceptance <- 1 - mean(duplicated(chain[-(1:burnin)]))\nacceptance\n#> [1] 0.2585\nmean(chain[-(1:burnin)])\n#> [1] 0.5957\np1 <- tibble(\n  x = chain[-(1:burnin)]\n) %>%\n  ggplot(aes(x)) +\n  geom_histogram(fill = \"darkgray\") +\n  labs(\n    x = expression(theta),\n    y = \"Frequenza\",\n    title = \"Distribuzione a posteriori\"\n  ) +\n  geom_vline(\n    xintercept = mean(chain[-(1:burnin)])\n  ) +\n  xlim(c(0.3, 0.85)) +\n  coord_flip()\n\np2 <- tibble(\n  x = 1:length(chain[-(1:burnin)]),\n  y = chain[-(1:burnin)]\n) %>%\n  ggplot(aes(x, y)) +\n  geom_line(color = \"darkgray\") +\n  labs(\n    x = \"Numero di passi\",\n    y = expression(theta),\n    title = \"Valori della catena\"\n  ) +\n  geom_hline(\n    yintercept = mean(chain[-(1:burnin)]),\n    colour = \"black\"\n  ) +\n  ylim(c(0.3, 0.85))\n\np1 + p2"},{"path":"ch:metropolis.html","id":"funzione-metropolis","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.6.3 Funzione metropolis()","text":"calcoli precedenti possono essere svolti anche mediante la funzione metropolis() del pacchetto ProbBayes. Per fare un esempio, consideriamo dati corrispondenti 23 successi 30 prove, con una distribuzione priori \\(\\mbox{Beta}(2, 10)\\). Le istruzioni sono le seguenti:Utilizzando un burn-piuttosto lungo, la stima posteriori di \\(\\theta\\) diventa:L’accettanza però non è ottimale:","code":"\nset.seed(123)\nlpost <- function(theta, s) {\n  dbeta(theta, 2, 10, log = TRUE) +\n    dbinom(23, 30, theta, log = TRUE)\n}\npost <- ProbBayes::metropolis(lpost, 0.5, 0.2, 8000)\n\ntibble(\n  x = post$S[4001:8000]\n) %>%\n  ggplot(aes(x)) +\n  geom_histogram(fill = \"darkgray\") +\n  labs(\n    x = expression(theta),\n    y = \"Frequenza\",\n    title = \"Distribuzione a posteriori\"\n  )\nmean(post$S[4001:8000])\n#> [1] 0.5948\npost$accept_rate\n#> [1] 0.5464"},{"path":"ch:metropolis.html","id":"gibbs-sampling","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.7 Gibb’s sampling","text":"Il campionamento di Gibbs è un algoritmo MCMC per ottenere una sequenza di campioni casuali da una distribuzione di probabilità congiunta di due o più variabili casuali quando il campionamento diretto si dimostra difficoltoso. Questa sequenza può essere usata per approssimare la distribuzione congiunta.Consideriamo, quale esempio, la seguente distribuzione di probabilità congiunta \\(p(x, y)\\) delle due variabili casuali \\(X\\) e \\(Y\\) (lo script \\(\\textsf{R}\\) è ricavato da Albert Hu 2019):Supponiamo che sia difficile campionare direttamente da \\(p(x, y)\\); supponiamo, tuttavia, di poter facilmente campionare dalle distribuzioni condizionate \\(p(x \\mid y)\\) e \\(p(y \\mid x)\\). Poniamo che l’algoritmo inizi con il valore \\(X = 1\\).Passo 1. Si simula \\(Y\\) dalla distribuzione condizionata \\(f(y \\mid X = 1)\\). Questa distribuzione condizionata è contenuta nella prima colonna della matrice \\(p(x, y)\\), ovveroSupponiamo di avere ottenuto il valore \\(Y = 2\\).Passo 2. Ora estraiamo un valore caso \\(X\\) dalla distribuzione condizionata \\(f(x \\mid Y = 2)\\). Questa distribuzione condizionata è contenuta nella seconda riga della matrice \\(p(x, y)\\), ovveroSupponiamo di avere ottenuto il valore \\(X = 3\\).Implementando passi 1 e 2, si ottiene un’iterazione del campionamento di Gibbs, ovvero la coppia simulata \\((X, Y) = (3, 2)\\). Il campionamento di Gibbs si realizza ripetendo passaggi 1 e 2 molte volte, laddove ogni passo condizioniamo valori simulati ai valori \\(X\\) o \\(Y\\) più recenti.Le seguenti istruzioni generano 100,000 iterazioni dell’algorimo di Gibbs per l’esempio considerazione:Esaminando risultati ottenuti, notiamo che il campionamento di Gibbs produce una buona approssimazione della distribuzione di probabilità congiunta di partenza:","code":"\np <- matrix(c(\n  4, 3, 2, 1,\n  3, 4, 3, 2,\n  2, 3, 4, 3,\n  1, 2, 3, 4\n) / 40, 4, 4, byrow = TRUE)\ndimnames(p)[[1]] <- 1:4\ndimnames(p)[[2]] <- 1:4\np\n#>       1     2     3     4\n#> 1 0.100 0.075 0.050 0.025\n#> 2 0.075 0.100 0.075 0.050\n#> 3 0.050 0.075 0.100 0.075\n#> 4 0.025 0.050 0.075 0.100\np[, 1]\n#>     1     2     3     4 \n#> 0.100 0.075 0.050 0.025\np[2, ]\n#>     1     2     3     4 \n#> 0.075 0.100 0.075 0.050\nn_iter <- 1e5\ngibbs_discrete <- function(p, i = 1, iter = n_iter) {\n  x <- matrix(0, iter, 2)\n  n_x <- dim(p)[1]\n  n_y <- dim(p)[2]\n  for (k in 1:n_iter) {\n    j <- sample(1:n_y, 1, prob = p[i, ])\n    i <- sample(1:n_x, 1, prob = p[, j])\n    x[k, ] <- c(i, j)\n  }\n  x\n}\nsp <- data.frame(gibbs_discrete(p))\nnames(sp) <- c(\"X\", \"Y\")\nround(table(sp) / n_iter, 3)\n#>    Y\n#> X       1     2     3     4\n#>   1 0.090 0.068 0.046 0.023\n#>   2 0.067 0.090 0.068 0.046\n#>   3 0.045 0.069 0.091 0.069\n#>   4 0.023 0.045 0.068 0.093"},{"path":"ch:metropolis.html","id":"campionamento-beta-binomiale","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.7.1 Campionamento beta-binomiale","text":"L’esempio precedente ha dimostrato il campionamento di Gibbs per una distribuzione discreta due parametri, ma il campionamento di Gibbs funziona per qualsiasi distribuzione due parametri. Per illustrare questo, consideriamo nuovamente lo schema beta-binomiale.\\[\n\\begin{split}\nY \\mid p &\\sim \\mbox{Binom}(n, p),\\\\\np &\\sim \\mbox{Beta}(, b).\n\\end{split}\n\\]Per implementare il campionamento Gibbs per questa situazione, è necessario identificare le due distribuzioni condizionate \\(Y \\mid P\\) e \\(p \\mid Y\\). questo fine, calcoliamo prima la distribuzione congiunta \\(f(Y, p)\\):\\[\n\\begin{split}\nf(Y, p) &= f(p) f(y \\mid p)\\\\\n&= \\left[ \\frac{1}{B(, b)}p^{-1} (1-p)^{b-1} \\right] \\left[ \\binom{n}{y}p^y (1-p)^{n-y}\\right].\n\\end{split}\n\\]Per trovare le distribuzioni marginali, sopprimiamo la dipendenza di \\(f(Y, p)\\) dalle costanti irrilevanti. Se teniamo fisso \\(p\\), l’unica variabile casuale è \\(y\\). Dunque, tutto il contenuto della prima parentesi quadra diventa una costante che può essere ignorata. Per cui \\(f(y \\mid p)\\) è \\(\\mbox{Binom}(n, p)\\). D’altra parte, se teniamo fisso \\(y\\), ignorando le costanti, la densità congiunta risulta proporzionale \\[\np^{y+-1} (1-p)^{n-y+b-1}\n\\]che è il kernel della distribuzione Beta con parametri di forma \\(y+\\) e \\(n-y+b\\). Quindi abbiamo \\(f(p \\mid y) = \\mbox{Beta}(y+, n-y+b)\\).Una volta trovate le due distribuzioni condizionate è facile modificare l’algoritmo di Gibbs visto sopra per adattarlo alla situazione presente:Di seguito eseguiamo il campionamento Gibbs per questo modello Beta-Binomiale con \\(n=20\\), \\(= 5\\) e \\(b=5\\). Dopo aver eseguito 100,000 iterazioni, possiamo considerare la matrice \\(sp\\) come un campione casuale tratto dalla distribuzione congiunta \\(f(Y, p)\\).Se dalla sequenza di coppie \\((Y, p)\\) generate dall’algoritmo di Gibbs consideriamo solo valori \\(Y\\), possiamo ottenere un’approssimazione della distribuzione marginale \\(f(y)\\) di \\(Y\\).La figura seguente mostra invece l’approssimazione della distribuzione congiunta \\(f(Y, p)\\) che è stata ottenuta:","code":"\ngibbs_betabin <- function(n, a, b, p = 0.5, iter = n_iter) {\n  x <- matrix(0, iter, 2)\n  for (k in 1:iter) {\n    y <- rbinom(1, size = n, prob = p)\n    p <- rbeta(1, y + a, n - y + b)\n    x[k, ] <- c(y, p)\n  }\n  x\n}\nset.seed(123)\nsp <- data.frame(gibbs_betabin(20, 5, 5))\nggplot(data.frame(Y = sp$X1), aes(Y)) +\n  geom_bar(width = 0.5, fill = \"darkgray\") +\n  ylab(\"Frequency\")\nggplot(sp, aes(X1, X2)) +\n  geom_point(size = 0.5, color = \"black\", alpha = 0.05) +\n  xlab(\"Y\") +\n  ylab(\"p\")"},{"path":"ch:metropolis.html","id":"input","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.8 Input","text":"Negli esempi discussi questo Capitolo abbiamo illustrato l’esecuzione di una singola catena cui si parte un unico valore iniziale e si raccolgono valori simulati da molte iterazioni. È possibile però che valori di una catena siano influenzati dalla scelta del valore iniziale. Quindi una raccomandazione generale è di eseguire l’algoritmo di Metropolis più volte utilizzando diversi valori di partenza. questo caso, si avranno più catene di Markov. Confrontando le proprietà delle diverse catene si esplora la sensibilità dell’inferenza alla scelta del valore di partenza. software MCMC consentono sempre ’utente di specificare diversi valori di partenza e di generare molteplici catene di Markov.","code":""},{"path":"ch:metropolis.html","id":"stazionarietà","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.9 Stazionarietà","text":"Un punto importante da verificare è se il campionatore ha raggiunto la sua distribuzione stazionaria. La convergenza di una catena di Markov alla distribuzione stazionaria viene detta “mixing”.","code":""},{"path":"ch:metropolis.html","id":"approx-post-autocor","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.9.1 Autocorrelazione","text":"Informazioni sul “mixing” della catena di Markov sono fornite dall’autocorrelazione. L’autocorrelazione misura la correlazione tra valori successivi di una catena di Markov. Il valore \\(m\\)-esimo della serie ordinata viene confrontato con un altro valore ritardato di una quantità \\(k\\) (dove \\(k\\) è l’entità del ritardo) per verificare quanto si correli al variare di \\(k\\). L’autocorrelazione di ordine 1 (lag 1) misura la correlazione tra valori successivi della catena di Markow (cioè, la correlazione tra \\(\\theta^{(m)}\\) e \\(\\theta^{(m-1)}\\)); l’autocorrelazione di ordine 2 (lag 2) misura la correlazione tra valori della catena di Markow separati da due “passi” (cioè, la correlazione tra \\(\\theta^{(m)}\\) e \\(\\theta^{(m-2)}\\)); e così via.L’autocorrelazione di ordine \\(k\\) è data da \\(\\rho_k\\) e può essere stimata come:\\[\\begin{align}\n\\rho_k &= \\frac{\\mbox{Cov}(\\theta_m, \\theta_{m+k})}{\\mbox{Var}(\\theta_m)}\\notag\\\\\n&= \\frac{\\sum_{m=1}^{n-k}(\\theta_m - \\bar{\\theta})(\\theta_{m-k} - \\bar{\\theta})}{\\sum_{m=1}^{n-k}(\\theta_m - \\bar{\\theta})^2} \\qquad\\text{con }\\quad \\bar{\\theta} = \\frac{1}{n}\\sum_{m=1}^{n}\\theta_m.\n\\tag{6.2}\n\\end{align}\\]Per fare un esempio pratico, simuliamo dei dati autocorrelati con la funzione \\(\\textsf{R}\\) colorednoise::colored_noise():L’autocorrelazione di ordine 1 è semplicemente la correlazione tra ciascun elemento e quello successivo nella sequenza. Nell’esempio, il vettore rednoise è una sequenza temporale di 30 elementi. Il vettore rednoise[-length(rednoise)] include gli elementi con gli indici da 1 29 nella sequenza originaria, mentre il vettore rednoise[-1] include gli elementi 2:30. Gli elementi delle coppie ordinate dei due vettori avranno dunque gli indici \\((1, 2), (2, 3), \\dots (29, 30)\\) degli elementi della sequenza originaria. La correlazione di Pearson tra vettori rednoise[-length(rednoise)] e rednoise[-1] corrisponde ’autocorrelazione di ordine 1 della serie temporale.Il Correlogramma è uno strumento grafico usato per la valutazione della tendenza di una catena di Markov nel tempo. Il correlogramma si costruisce partire dall’autocorrelazione \\(\\rho_k\\) di una catena di Markov funzione del ritardo (lag) \\(k\\) con cui l’autocorrelazione è calcolata: nel grafico ogni barretta verticale riporta il valore dell’autocorrelazione (sull’asse delle ordinate) funzione del ritardo (sull’asse delle ascisse). \\(\\textsf{R}\\), il correlogramma può essere prodotto con una chiamata acf():Nel correlogramma precedente vediamo che l’autocorrelazione di ordine 1 è circa pari 0.4 e diminuisce per lag maggiori; per lag di 4, l’autocorrelazione diventa negativa e aumenta progressivamente fino ad un lag di 8; eccetera.situazioni ottimali l’autocorrelazione diminuisce rapidamente ed è effettivamente pari 0 per piccoli lag. Ciò indica che valori della catena di Markov che si trovano più di soli pochi passi di distanza gli uni dagli altri non risultano associati tra loro, il che fornisce una conferma del “mixing” della catena di Markov, ossia della convergenza alla distribuzione stazionaria. Nelle analisi bayesiane, una delle strategie che consentono di ridurre l’autocorrelazione è quella di assottigliare l’output immagazzinando solo ogni \\(m\\)-esimo punto dopo il periodo di burn-. Una tale strategia va sotto il nome di thinning.","code":"\nsuppressPackageStartupMessages(library(\"colorednoise\"))\nset.seed(34783859)\nrednoise <- colored_noise(\n  timesteps = 30, mean = 0.5, sd = 0.05, phi = 0.3\n)\ncor(rednoise[-length(rednoise)], rednoise[-1])\n#> [1] 0.3967\nacf(rednoise)"},{"path":"ch:metropolis.html","id":"test-di-convergenza","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"6.2.10 Test di convergenza","text":"Un test di convergenza può essere svolto maniera grafica mediante le tracce delle serie temporali (trace plot), cioè il grafico dei valori simulati rispetto al numero di iterazioni. Se la catena è uno stato stazionario le tracce mostrano assenza di periodicità nel tempo e ampiezza costante, senza tendenze visibili o andamenti degni di nota. Un esempio di trace plot è fornito nella figura 6.7 (destra).Ci sono inoltre alcuni test che permettono di verificare la stazionarietà del campionatore dopo un dato punto. Uno è il test di Geweke che suddivide il campione, dopo aver rimosso un periodo di burn , due parti. Se la catena è uno stato stazionario, le medie dei due campioni dovrebbero essere uguali. Un test modificato, chiamato Geweke z-score, utilizza un test \\(z\\) per confrontare due subcampioni ed il risultante test statistico, se ad esempio è più alto di 2, indica che la media della serie sta ancora muovendosi da un punto ad un altro e quindi è necessario un periodo di burn-più lungo.","code":""},{"path":"ch:metropolis.html","id":"commenti-e-considerazioni-finali-4","chapter":"Capitolo 6 Approssimazione della distribuzione a posteriori","heading":"Commenti e considerazioni finali","text":"generale, la distribuzione posteriori dei parametri di un modello statistico non può essere determinata per via analitica. Tale problema viene invece affrontato facendo ricorso ad una classe di algoritmi per il campionamento da distribuzioni di probabilità che sono estremamente onerosi dal punto di vista computazionale e che possono essere utilizzati nelle applicazioni pratiche solo grazie alla potenza di calcolo dei moderni computer. Lo sviluppo di software che rendono sempre più semplice l’uso dei metodi MCMC, insieme ’incremento della potenza di calcolo dei computer, ha contribuito rendere sempre più popolare il metodo dell’inferenza bayesiana che, questo modo, può essere estesa problemi di qualunque grado di complessità.","code":""}]
