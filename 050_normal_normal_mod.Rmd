# Modello Normale-Normale {#normal-normal-mod-stan}

```{r, echo=FALSE}
source("_common.R")
source("_stan_options.R")
```

Estendiamo ora la discussione precedente considerano un altro caso comune: quello in cui la verosimiglianza è Normale e la distribuzione a priori è Normale. Questa situazione definisce lo schema Normale-Normale. In queste circostanze, l'uso di famiglie di distribuzioni coniugate consente la derivazione analitica della distribuzione a posteriori. La trattazione matematica di tale derivazione, però, è piuttosto complessa e qui verrà solo accennata. Mostreremo invece come svolgere l'inferenza sul caso Normale-Normale mediante i metodi MCMC.

## Distribuzione Normale-Normale con varianza nota

Per $\sigma^2$ nota, la famiglia della distribuzione gaussiana è coniugata a sé stessa:  se la funzione di verosimiglianza è gaussiana, la scelta per la media di una distribuzione a priori gaussiana assicurera che anche la distribuzione a posteriori (della media) sarà ancora gaussiana. Siano $Y_1, \dots, Y_n$ $n$ variabili casuali i.i.d. che seguono la distribuzione gaussiana:

$$
Y_1, \dots, Y_n  \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma).
$$

Ci poniamo il problema di stimare $\mu$ sulla base di $n$ osservazioni $y_1, \dots, y_n$. Consideriamo qui solamente il caso in cui $\sigma^2$ sia supposta perfettamente nota. Ricordiamo che la densità gaussiana è

$$
p(y_i \mid \mu, \sigma) = \frac{1}{{\sigma \sqrt {2\pi}}}\exp\left\{{-\frac{(y_i - \mu)^2}{2\sigma^2}}\right\}.
$$

Essendo le variabili i.i.d., possiamo scrivere la densità congiunta come il prodotto delle singole densità e quindi si ottiene

$$
p(y \mid \mu) = \, \prod_{i=1}^n p(y_i \mid \mu).
$$

Una volta osservati i dati $y$, la verosimiglianza diventa 

\begin{align}
\mathcal{L}(\mu \mid y) =& \, \prod_{i=1}^n p(y_i \mid \mu) = \notag\\
& \frac{1}{{\sigma \sqrt {2\pi}}}\exp\left\{{-\frac{(y_1 - \mu)^2}{2\sigma^2}}\right\} \times \notag\\
 & \frac{1}{{\sigma \sqrt {2\pi}}}\exp\left\{{-\frac{(y_2 - \mu)^2}{2\sigma^2}}\right\} \times  \notag\\
& \vdots \notag\\
 & \frac{1}{{\sigma \sqrt {2\pi}}}\exp\left\{{-\frac{(y_n - \mu)^2}{2\sigma^2}}\right\}.
\end{align}

Se viene scelta una densità a priori gaussiana, ciò fa sì che anche la densità a posteriori sia gaussiana. Supponiamo che

\begin{equation}
p(\mu) = \frac{1}{{\tau_0 \sqrt {2\pi}}}\exp\left\{{-\frac{(\mu - \mu_0)^2}{2\tau_0^2}}\right\},
(\#eq:prior-mu-norm-norm)
\end{equation}

ovvero che la distribuzione a priori di $\mu$ sia gaussiana con media $\mu_0$ e varianza $\tau_0^2$. Possiamo dire che $\mu_0$ rappresenta il valore ritenuto più probabile per $\mu$ e $\tau_0^2$ il grado di incertezza che abbiamo rispetto a tale valore. 

Svolgendo una serie di passaggi algebrici, si arriva a

\begin{equation}
p(\mu \mid y) = \frac{1}{{\tau_p \sqrt {2\pi}}}\exp\left\{{-\frac{(\mu - \mu_p)^2}{2\tau_p^2}}\right\},
(\#eq:post-norm-norm)
\end{equation}

dove

\begin{equation}
\mu_p = \frac{\frac{1}{\tau_0^2}\mu_0+ \frac{n}{\sigma^2}\bar{y}}{\frac {1}{\tau_0^2} + \frac{n}{\sigma^2}} 
(\#eq:post-norm-mup)
\end{equation}

e 

\begin{equation}
\tau_p^2 = \frac{1}{\frac {1}{\tau_0^2}+ \frac{n}{\sigma^2}}.
(\#eq:post-norm-taup2)
\end{equation}

In altri termini, se la distribuzione a priori per $\mu$ è gaussiana, la distribuzione a posteriori è anch'essa gaussiana con valore atteso (a posteriori) $\mu_p$ e varianza (a posteriori) $\tau_p^2$ date dalle espressioni precedenti.

In conclusione, il risultato trovato indica che:

- il valore atteso a posteriori è una media pesata fra il valore atteso a priori $\mu_0$ e la media campionaria $\bar{y}$; il peso della media campionaria è tanto maggiore tanto più è grande $n$ (il numero di osservazioni) e $\tau_0^2$ (l'incertezza iniziale); 
- l'incertezza (varianza) a posteriori $\tau_p^2$ è sempre più piccola dell'incertezza a priori $\tau_0^2$ e diminuisce al crescere di $n$.

## Il modello Normale con Stan

Per esaminare un esempio pratico, consideriamo i 30 valori BDI-II dei soggetti clinici di @zetschefuture2019:

```{r}
df <- data.frame(
  y = c(
    26.0, 35.0, 30, 25, 44, 30, 33, 43, 22, 43,
    24, 19, 39, 31, 25, 28, 35, 30, 26, 31, 41,
    36, 26, 35, 33, 28, 27, 34, 27, 22
  )
)
```

Calcoliamo le statistiche descrittive del campione di dati:

```{r}
df %>% 
  summarise(
    sample_mean = mean(y),
    sample_sd = sd(y)
  )
```

Nella discussione seguente assumeremo che $\mu$ e $\sigma$ siano indipendenti. Assegneremo a $\mu$ una distribuzione a priori $\mathcal{N}(25, 2)$ e a $\sigma$ una distribuzione a priori $\mathcal{N}(5, 2)$. Il modello statistico diventa:

\begin{align}
Y_i &\sim \mathcal{N}(\mu, \sigma) \notag\\
\mu &\sim \mathcal{N}(25, 2) \notag\\
\sigma &\sim \mathcal{N}(5, 2.5) \notag
\end{align}

In base al modello definito, la variabile casuale $Y$ segue la distribuzione Normale di parametri $\mu$ e $\sigma$. Il parametro $\mu$ è sconosciuto e abbiamo deciso di descrivere la nostra incertezza relativa ad esso mediante una distribuzione a priori Normale con media uguale a 25 e deviazione standard pari a 2. L'incertezza relativa a $\sigma$ è quantificata da una distribuzione a priori Normale di parametri $\mu = 5$ e $\sigma = 2.5$.

La procedura MCMC utilizzata da Stan è basata su un campionamento Monte Carlo Hamiltoniano che non richiede l'uso di distribuzioni a priori coniugate; pertanto per i parametri è possibile scegliere una qualunque distribuzione a priori arbitraria. Svolgiamo però qui l'inferenza implementando il modello descritto sopra. In Stan, il modello statistico può essere scritto com indicato di seguito: 

```{r}
modelString = "
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  mu ~ normal(25, 2);
  sigma ~ normal(5, 2.5);
  y ~ normal(mu, sigma);
}
"
writeLines(modelString, con = "code/normalmodel.stan")
```

Si noti che, nel presente modello, il parametro $\sigma$ è considerato incognito.

Sistemiamo i dati nel formato appropriato per potere essere letti da Stan:

```{r}
data_list <- list(
  N = length(df$y),
  y = df$y
)
```

Leggiamo il file in cui abbiamo salvato il codice Stan

```{r}
file <- file.path("code", "normalmodel.stan")
```

compiliamo il modello
```{r}
mod <- cmdstan_model(file)
```

ed eseguiamo il campionamento MCMC:

```{r, message = FALSE, warning=FALSE, results='hide'}
fit <- mod$sample(
  data = data_list,
  iter_sampling = 4000L,
  iter_warmup = 2000L,
  seed = SEED,
  chains = 4L,
  parallel_chains = 2L,
  refresh = 0,
  thin = 1
)
```

Le stime a posteriori dei parametri si ottengono con:

```{r}
fit$summary(c("mu", "sigma"))
```

oppure, dopo avere trasformato l'oggetto `fit` nel formato `stanfit`, 

```{r}
stanfit <- rstan::read_stan_csv(fit$output_files())
```

con 

```{r}
out <- rstantools::posterior_interval(
  as.matrix(stanfit), 
  prob = 0.95
)
out
```

Possiamo dunque concludere, con un grado di certezza soggettiva del 95%, che la media della popolazione da cui abbiamo tratto i dati è compresa nell'intervallo [`r round(out[1, 1], 2)`, `r round(out[1, 2], 2)`]. 

<!-- ## Il modello normale con `brms::brm()` -->

<!-- Ripetiamo qui l'analisi precedente usando la funzione `brms::brm()`. In questo caso non è necessario scrivere il modello in forma esplicita, come abbiamo fatto in precedenza. La sintassi seguente viene trasformata in maniera automatica in linguaggio Stan prima di adattare il modello ai dati: -->

<!-- ```{r} -->
<!-- fit <- brm( -->
<!--   data = df, -->
<!--   family = gaussian(), -->
<!--   y ~ 1, -->
<!--   prior = c( -->
<!--     prior(normal(25, 2), class = Intercept), -->
<!--     prior(normal(5, 2.5), class = sigma) -->
<!--   ), -->
<!--   iter = 4000, -->
<!--   refresh = 0, -->
<!--   chains = 4, -->
<!--   backend = "cmdstanr" -->
<!-- ) -->
<!-- ``` -->

<!-- I trace-plot si ottengono nel modo seguente: -->

<!-- ```{r} -->
<!-- plot(fit) -->
<!-- ``` -->

<!-- La stima a posteriori di $\mu$ è fornita dalla funzione `fixef()`: -->

<!-- ```{r} -->
<!-- fixef(fit) -->
<!-- ``` -->

<!-- In conclusione, la funzione `brms::brm()` produce lo stesso risultato (considerate le fluttuazioni nella stima numerica MCMC) trovato in precedenza usando un modello scritto in Stan. -->

## Commenti e considerazioni finali {-}

Questo esempio ci mostra come calcolare l'intervallo di credibilità per la media di una v.c. gaussiana. La domanda più ovvia di analisi dei dati, dopo  avere visto come creare l'intervallo di credibilità per la media di un gruppo, riguarda il confronto tra le medie di due gruppi. Il confronto tra le medie di due gruppi può essere considerato come un caso speciale di un metodo di analisi dei dati più generale, chiamato analisi di regressione lineare. Prima di discutere il problema del confronto tra le medie di due gruppi è dunque necessario esaminare il modello statistico della regressione lineare.


